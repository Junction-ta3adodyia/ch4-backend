Here is a coding project I am working on.
It starts with the full structure of the project, then you will have each file title and file content.

Respond with 'OK' and for now, just understand the project completely.
I will ask for help in the next prompt so you can assist me with this project.



--- PROJECT STRUCTURE ---


{
    "aquaculture-backend/": {
        ".env": {},
        ".gitignore": {},
        "alembic.ini": {},
        "create_admin.py": {},
        "docker-compose.yml": {},
        "Dockerfile": {},
        "fake_data.py": {},
        "nginx.conf": {},
        "README.md": {},
        "requirements.txt": {},
        ".git/": {},
        "alembic/": {},
        "app/": {
            "config.py": {},
            "database.py": {},
            "main.py": {},
            "__init__.py": {},
            "api/": {
                "deps.py": {},
                "__init__.py": {},
                "endpoints/": {
                    "alerts.py": {},
                    "analytics.py": {},
                    "auth.py": {},
                    "ponds.py": {},
                    "sensors.py": {},
                    "users.py": {},
                    "__init__.py": {}
                }
            },
            "core/": {
                "alert_engine.py": {},
                "health_calculator.py": {},
                "security.py": {},
                "__init__.py": {}
            },
            "models/": {
                "alert.py": {},
                "pond.py": {},
                "sensor.py": {},
                "__init__.py": {}
            },
            "schemas/": {
                "alert.py": {},
                "auth.py": {},
                "pond.py": {},
                "sensor.py": {},
                "__init__.py": {}
            },
            "services/": {
                "alert_service.py": {},
                "data_processor.py": {},
                "notification.py": {},
                "page_hinkley.py": {},
                "__init__.py": {}
            },
            "tasks/": {
                "data_aggregation.py": {},
                "__init__.py": {}
            },
            "utils/": {
                "helpers.py": {},
                "__init__.py": {}
            }
        },
        "__pycache__/": {},
        "init-db/": {},
        "logs/": {},
        "migrations/": {},
        "nginx/": {
            "nginx.conf": {}
        },
        "tests/": {}
    }
}


--- FILE : .env ---


--- FILE TOO LARGE / NO NEED TO READ ---


--- FILE : alembic.ini ---


--- FILE TOO LARGE / NO NEED TO READ ---


--- FILE : create_admin.py ---


import os
import sys
import argparse
from getpass import getpass

# Add the project root to the Python path to allow imports from 'app'
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

from app.database import SessionLocal
from app.models.pond import User, UserRole, Pond
from app.models.sensor import SensorData
from app.models.alert import Alert
from app.models.alert import PondHealth
from app.core.security import get_password_hash


def create_admin_user(db_session, username, email, password, recreate=False):
    """Creates an admin user, optionally deleting an existing one first."""
    
    # Check if user already exists
    existing_user = db_session.query(User).filter(
        (User.username == username) | (User.email == email)
    ).first()
    
    if existing_user:
        if recreate:
            print(f"Found existing user '{existing_user.username}'. Deleting before recreation.")
            db_session.delete(existing_user)
            db_session.commit()
        else:
            print(f"Error: User with username '{username}' or email '{email}' already exists.")
            print("Use the --recreate flag to delete the existing user first.")
            return

    # Hash the password
    hashed_password = get_password_hash(password)
    
    # Create the new admin user
    admin_user = User(
        username=username,
        email=email,
        hashed_password=hashed_password,
        role=UserRole.ADMIN,
        is_active=True,
        is_verified=True  # Admins should be verified by default
    )
    
    db_session.add(admin_user)
    db_session.commit()
    
    print(f"Successfully created admin user '{username}' with email '{email}'.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create or recreate an admin user.")
    parser.add_argument("--username", required=True, help="Username for the new admin.")
    parser.add_argument("--email", required=True, help="Email for the new admin.")
    parser.add_argument(
        "--recreate",
        action="store_true",
        help="If the user already exists, delete them before creating the new admin."
    )
    
    args = parser.parse_args()
    
    password = getpass("Enter password for the new admin: ")
    password_confirm = getpass("Confirm password: ")
    
    if password != password_confirm:
        print("Error: Passwords do not match.")
        sys.exit(1)
        
    db = SessionLocal()
    try:
        create_admin_user(db, args.username, args.email, password, recreate=args.recreate)
    finally:
        db.close()


--- FILE : docker-compose.yml ---


version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15
    container_name: aquaculture_db
    environment:
      POSTGRES_DB: ${DATABASE_NAME:-aquaculture}
      POSTGRES_USER: ${DATABASE_USER:-postgres}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD:-postgres}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped

  # Redis for caching and background tasks
  redis:
    image: redis:7-alpine
    container_name: aquaculture_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # FastAPI Application
  api:
    build: .
    container_name: aquaculture_api
    env_file:
      - .env  # Add this line to load your .env file
    environment:
      DATABASE_URL: postgresql://${DATABASE_USER:-postgres}:${DATABASE_PASSWORD:-postgres}@postgres:5432/${DATABASE_NAME:-aquaculture}
      REDIS_URL: redis://redis:6379/0
    volumes:
      - ./app:/app/app
      - ./logs:/app/logs
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Nginx reverse proxy (optional)
  nginx:
    image: nginx:alpine
    container_name: aquaculture_nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - api
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:


--- FILE : fake_data.py ---


--- ERROR READING FILE ---


--- FILE : nginx.conf ---


events {
    worker_connections 1024;
}

http {
    upstream api {
        server aquaculture_api:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /health {
            proxy_pass http://api/health;
        }
    }
}


--- FILE : README.md ---


# 🐟 Aquaculture Management System - Backend

A comprehensive IoT-based aquaculture monitoring and management system designed for fish farms in Algeria. This FastAPI-powered backend provides real-time water quality monitoring, anomaly detection, automated alerting, and comprehensive analytics for optimizing aquaculture operations.

## 🌟 Features

### 🔍 Real-time Monitoring
- **Multi-sensor Support**: Temperature, pH, dissolved oxygen, turbidity, and ammonia monitoring
- **Live Data Processing**: Real-time sensor data ingestion and validation
- **Data Quality Assurance**: Automated data cleaning and outlier detection
- **Historical Analytics**: Comprehensive trend analysis and reporting

### 🚨 Intelligent Alerting System
- **Page-Hinkley Change Detection**: Advanced anomaly detection using statistical change point detection
- **Multi-level Alerts**: Critical, warning, and informational alert classifications
- **Smart Notifications**: Email alerts with multilingual support (Arabic, French, English)
- **Alert Management**: Configurable thresholds and notification preferences

### 📊 Analytics & Insights
- **Health Score Calculation**: Automated pond health assessment based on multiple parameters
- **Trend Analysis**: Historical data visualization and pattern recognition
- **Performance Metrics**: KPI tracking and optimization recommendations
- **Data Aggregation**: Hourly and daily data summaries for efficient querying

### 🏢 Multi-tenancy & Security
- **Organization Management**: Multi-tenant architecture with role-based access
- **JWT Authentication**: Secure API access with token-based authentication
- **User Roles**: Admin, manager, and operator permission levels
- **Data Isolation**: Secure data separation between organizations

### 🌐 API & Integration
- **RESTful API**: Comprehensive REST endpoints for all operations
- **OpenAPI Documentation**: Auto-generated API documentation with Swagger UI
- **Data Export**: CSV and JSON export capabilities
- **IoT Integration**: MQTT and HTTP endpoints for sensor data ingestion

## 🏗️ Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   IoT Sensors   │───▶│   FastAPI App   │───▶│   PostgreSQL    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                               │
                               ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │  Alert Engine   │───▶│   Email Service │
                       └─────────────────┘    └─────────────────┘
                               │
                               ▼
                       ┌─────────────────┐
                       │     Redis       │
                       │   (Caching)     │
                       └─────────────────┘
```

### Technology Stack
- **Backend Framework**: FastAPI with Python 3.11+
- **Database**: PostgreSQL 15 with SQLAlchemy ORM
- **Caching**: Redis for session management and caching
- **Task Scheduling**: APScheduler for background jobs
- **Authentication**: JWT with bcrypt password hashing
- **Deployment**: Docker with docker-compose orchestration
- **Monitoring**: Built-in health checks and logging

## 🚀 Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.11+ (for local development)
- PostgreSQL 15+ (if running without Docker)

### 1. Clone and Setup
```bash
git clone <repository-url>
cd aquaculture-backend
cp .env.example .env
```

### 2. Configure Environment
Edit `.env` file with your settings:
```bash
# Database Configuration
DATABASE_URL=postgresql://postgres:your_password@localhost:5432/aquaculture
DATABASE_PASSWORD=your_secure_password

# Security
SECRET_KEY=your-super-secret-key-here
JWT_SECRET_KEY=your-jwt-secret-key

# Email Configuration (for alerts)
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# Application Settings
ENVIRONMENT=production
DEBUG=false
ANOMALY_DETECTION_THRESHOLD=0.1
```

### 3. Start with Docker (Recommended)
```bash
# Build and start all services
docker-compose up -d

# View logs
docker-compose logs -f api

# Run database migrations
docker-compose exec api alembic upgrade head
```

### 4. Access the Application
- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **API Base URL**: http://localhost:8000/api/v1

## 🛠️ Development Setup

### Local Development
```bash
# Install dependencies
pip install -r requirements.txt

# Set up pre-commit hooks
pip install pre-commit
pre-commit install

# Run database migrations
alembic upgrade head

# Start development server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Database Migrations
```bash
# Create new migration
alembic revision --autogenerate -m "Description of changes"

# Apply migrations
alembic upgrade head

# Rollback migration
alembic downgrade -1
```

### Testing
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=app --cov-report=html

# Run specific test file
pytest tests/test_sensors.py -v
```

## 📡 API Usage

### Authentication
```bash
# Login to get access token
curl -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=your_password"

# Use token in subsequent requests
curl -H "Authorization: Bearer YOUR_TOKEN" \
  "http://localhost:8000/api/v1/ponds/"
```

### Sensor Data Submission
```bash
# Submit sensor readings
curl -X POST "http://localhost:8000/api/v1/sensors/data" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "pond_id": 1,
    "sensor_type": "temperature",
    "value": 25.5,
    "unit": "celsius",
    "timestamp": "2024-01-15T10:30:00Z"
  }'
```

### Pond Management
```bash
# Create new pond
curl -X POST "http://localhost:8000/api/v1/ponds/" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Pond Alpha",
    "location": "Farm Section A",
    "capacity": 5000,
    "species": "Tilapia"
  }'

# Get pond health status
curl -H "Authorization: Bearer YOUR_TOKEN" \
  "http://localhost:8000/api/v1/ponds/1/health"
```

## 🔍 Anomaly Detection

The system uses the **Page-Hinkley algorithm** for real-time change point detection:

### How it Works
1. **Continuous Monitoring**: Each sensor reading is processed through the Page-Hinkley detector
2. **Statistical Analysis**: Detects significant changes in data distribution
3. **Threshold-based Alerts**: Configurable sensitivity for different sensor types
4. **Contextual Awareness**: Considers historical patterns and seasonal variations

### Configuration
```python
# In your .env file
ANOMALY_DETECTION_THRESHOLD=0.1  # Lower = more sensitive
PAGE_HINKLEY_DELTA=0.01          # Detection sensitivity
PAGE_HINKLEY_LAMBDA=50           # Forgetting factor
```

### Alert Types
- **Critical**: Immediate action required (e.g., oxygen depletion)
- **Warning**: Attention needed (e.g., temperature drift)
- **Info**: Informational updates (e.g., feeding reminders)

## 📧 Email Notifications

### Multilingual Support
The system supports alerts in multiple languages:
- **Arabic**: للتنبيهات الحرجة
- **French**: Pour les alertes critiques
- **English**: For critical alerts

### Email Templates
- **Critical Alerts**: Immediate notification with action items
- **Daily Summaries**: Comprehensive daily reports
- **System Status**: Health checks and maintenance notifications

### Configuration
```bash
# SMTP Configuration
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USE_TLS=true
SMTP_USERNAME=your-email@domain.com
SMTP_PASSWORD=your-app-password

# Alert Settings
ALERT_EMAIL_FROM=alerts@yourfarm.com
ALERT_EMAIL_REPLY_TO=support@yourfarm.com
```

## 🐳 Deployment

### Production Deployment
```bash
# Clone repository on server
git clone <repository-url>
cd aquaculture-backend

# Configure production environment
cp .env.example .env
# Edit .env with production values

# Deploy with Docker Compose
docker-compose -f docker-compose.prod.yml up -d

# Set up SSL with Let's Encrypt (optional)
docker-compose exec nginx certbot --nginx -d yourdomain.com
```

### Environment Variables
| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | Required |
| `SECRET_KEY` | Application secret key | Required |
| `SMTP_SERVER` | Email server hostname | localhost |
| `SMTP_PORT` | Email server port | 587 |
| `ANOMALY_DETECTION_THRESHOLD` | Sensitivity for anomaly detection | 0.1 |
| `DATA_RETENTION_DAYS` | Days to keep raw sensor data | 90 |

### Health Monitoring
```bash
# Check application health
curl http://localhost:8000/health

# Monitor logs
docker-compose logs -f api

# Database health
docker-compose exec postgres pg_isready
```

## 🔧 Maintenance

### Data Management
```bash
# Backup database
docker-compose exec postgres pg_dump -U postgres aquaculture > backup.sql

# Restore database
docker-compose exec postgres psql -U postgres aquaculture < backup.sql

# Clean old data (automated via cron)
docker-compose exec api python -m app.tasks.data_aggregation cleanup_old_data
```

### Performance Optimization
- **Database Indexing**: Automatic indexing on frequently queried columns
- **Query Optimization**: Efficient SQL queries with proper joins
- **Caching**: Redis caching for frequently accessed data
- **Connection Pooling**: SQLAlchemy connection pooling for database efficiency

## 🛡️ Security

### Best Practices
- **JWT Tokens**: Secure authentication with expirable tokens
- **Password Hashing**: bcrypt for secure password storage
- **CORS Configuration**: Proper cross-origin request handling
- **Input Validation**: Pydantic models for request validation
- **SQL Injection Protection**: SQLAlchemy ORM prevents SQL injection

### Security Headers
```python
# Automatically configured security headers
- X-Content-Type-Options: nosniff
- X-Frame-Options: DENY
- X-XSS-Protection: 1; mode=block
```

## 📝 API Documentation

### Interactive Documentation
- **Swagger UI**: Available at `/docs`
- **ReDoc**: Available at `/redoc`
- **OpenAPI Schema**: Available at `/openapi.json`

### Main Endpoints
- `POST /api/v1/auth/login` - User authentication
- `GET /api/v1/ponds/` - List all ponds
- `POST /api/v1/sensors/data` - Submit sensor readings
- `GET /api/v1/alerts/` - Retrieve alerts
- `GET /api/v1/analytics/health/{pond_id}` - Pond health metrics

## 🤝 Contributing

### Development Workflow
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Run the test suite: `pytest`
5. Commit your changes: `git commit -m 'Add amazing feature'`
6. Push to your fork: `git push origin feature/amazing-feature`
7. Create a Pull Request

### Code Standards
- **Black**: Code formatting with `black app/`
- **Flake8**: Linting with `flake8 app/`
- **Type Hints**: Full type annotation coverage
- **Docstrings**: Comprehensive documentation for all functions

## 📊 Monitoring & Analytics

### Built-in Metrics
- **System Health**: API response times, database connections
- **Data Quality**: Sensor reading validation and completeness
- **Alert Performance**: Alert response times and accuracy
- **User Activity**: API usage patterns and authentication metrics

### Integration Options
- **Prometheus**: Metrics collection and monitoring
- **Grafana**: Visualization and dashboarding
- **ELK Stack**: Log aggregation and analysis

## 🐛 Troubleshooting

### Common Issues

#### Database Connection Issues
```bash
# Check database connectivity
docker-compose exec api python -c "from app.database import engine; print(engine.execute('SELECT 1').scalar())"

# Reset database
docker-compose down
docker volume rm aquaculture-backend_postgres_data
docker-compose up -d
```

#### Email Notifications Not Working
```bash
# Test SMTP configuration
docker-compose exec api python -c "
from app.services.notification import send_test_email
send_test_email('test@example.com')
"
```

#### High Memory Usage
```bash
# Monitor resource usage
docker stats aquaculture_api

# Adjust memory limits in docker-compose.yml
mem_limit: 512m
```

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 📞 Support

For support and questions:
- **Documentation**: Check the `/docs` endpoint for API documentation
- **Issues**: Create an issue on the repository for bug reports
- **Email**: Contact the development team for enterprise support

---

## 🎯 Roadmap

### Upcoming Features
- [ ] **Mobile App Integration**: React Native companion app
- [ ] **Machine Learning**: Predictive analytics for water quality
- [ ] **IoT Expansion**: Support for additional sensor types
- [ ] **Blockchain Integration**: Supply chain traceability
- [ ] **Advanced Analytics**: AI-powered insights and recommendations

### Recent Updates
- ✅ **Page-Hinkley Anomaly Detection**: Advanced change point detection
- ✅ **Multilingual Email Alerts**: Arabic, French, and English support
- ✅ **Docker Deployment**: Complete containerization with docker-compose
- ✅ **Real-time Monitoring**: Live sensor data processing and alerts

---

*Built with ❤️ for the aquaculture industry in Algeria*



--- FILE : requirements.txt ---


aiohappyeyeballs==2.6.1
aiohttp==3.12.14
aiohttp-retry==2.9.1
aiosignal==1.4.0
aiosmtplib==3.0.1
alembic==1.12.1
amqp==5.3.1
annotated-types==0.7.0
anyio==3.7.1
APScheduler==3.10.4
attrs==25.3.0
bcrypt==4.3.0
billiard==4.2.1
black==23.11.0
celery==5.3.4
certifi==2025.7.14
cffi==1.17.1
charset-normalizer==3.4.2
click==8.2.1
click-didyoumean==0.3.1
click-plugins==1.1.1.2
click-repl==0.3.0
colorama==0.4.6
coverage==7.9.2
cryptography==45.0.5
dnspython==2.7.0
ecdsa==0.19.1
email_validator==2.2.0
fastapi==0.104.1
flake8==6.1.0
frozenlist==1.7.0
greenlet==3.2.3
h11==0.16.0
httpcore==1.0.9
httptools==0.6.4
httpx==0.25.2
idna==3.10
iniconfig==2.1.0
itsdangerous==2.1.2
Jinja2==3.1.6
joblib==1.5.1
kombu==5.5.4
Mako==1.3.10
MarkupSafe==3.0.2
mccabe==0.7.0
multidict==6.6.3
mypy_extensions==1.1.0
numpy==1.25.2
packaging==25.0
pandas==2.1.3
passlib==1.7.4
pathspec==0.12.1
platformdirs==4.3.8
pluggy==1.6.0
prometheus-client==0.19.0
prompt_toolkit==3.0.51
propcache==0.3.2
psycopg2-binary==2.9.9
pyasn1==0.6.1
pycodestyle==2.11.1
pycparser==2.22
pydantic==2.5.0
pydantic-settings==2.1.0
pydantic_core==2.14.1
pyfcm==1.5.4
pyflakes==3.1.0
PyJWT==2.10.1
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
python-dateutil==2.8.2
python-dotenv==1.0.0
python-jose==3.3.0
python-multipart==0.0.6
pytz==2023.3
PyYAML==6.0.2
redis==5.0.1
requests==2.32.4
rsa==4.9.1
scikit-learn==1.3.2
scipy==1.11.4
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.23
starlette==0.27.0
structlog==23.2.0
threadpoolctl==3.6.0
twilio==8.10.0
typing_extensions==4.14.1
tzdata==2025.2
tzlocal==5.3.1
urllib3==2.5.0
uvicorn==0.24.0
vine==5.1.0
watchfiles==1.1.0
wcwidth==0.2.13
websockets==12.0
yarl==1.20.1



--- FILE : app/config.py ---


"""
Application configuration
Manages environment variables and application settings using Pydantic
"""

from typing import List, Dict, Any, Optional
from pydantic import validator
from pydantic_settings import BaseSettings
import json
import os


class Settings(BaseSettings):
    """
    Application settings loaded from environment variables
    """
    
    # Database Configuration
    DATABASE_URL: str
    DATABASE_NAME: str = "aquaculture"
    DATABASE_USER: str = "postgres"
    DATABASE_PASSWORD: str
    DATABASE_HOST: str = "localhost"
    DATABASE_PORT: int = 5432
    
    # Security Settings
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 1440
    ALGORITHM: str = "HS256"
    
    # Application Settings
    ENVIRONMENT: str = "development"
    DEBUG: bool = True
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    ALLOWED_HOSTS: List[str] = ["*"]
    
    # Data Processing Settings
    ANOMALY_DETECTION_THRESHOLD: float = 0.1
    DATA_QUALITY_THRESHOLD: float = 0.7
    DATA_RETENTION_DAYS: int = 90
    
    # Alert System Configuration
    ALERT_CHECK_INTERVAL_MINUTES: int = 5
    MAX_ALERTS_PER_HOUR: int = 10
    
    # File Upload Settings
    MAX_FILE_SIZE_MB: int = 10
    ALLOWED_FILE_TYPES: List[str] = ["csv", "xlsx", "json"]
    
    # Email Configuration - Now reading from .env
    SMTP_SERVER: str = "smtp.gmail.com"
    SMTP_PORT: int = 587
    SMTP_USERNAME: str  # Required from .env
    SMTP_PASSWORD: str  # Required from .env
    FROM_EMAIL: str = "noreply@aquaculture.dz"
    ENABLE_EMAIL_ALERTS: bool = True
    
    # SMS Configuration - Now reading from .env
    TWILIO_ACCOUNT_SID: Optional[str] = None
    TWILIO_AUTH_TOKEN: Optional[str] = None
    TWILIO_PHONE_NUMBER: Optional[str] = None
    
    # Push Notification Configuration (Optional)
    FIREBASE_SERVER_KEY: Optional[str] = None
    
    # Redis Configuration
    REDIS_URL: str = "redis://localhost:6379/0"
    
    # Multilingual Support
    DEFAULT_LANGUAGE: str = "fr"
    SUPPORTED_LANGUAGES: List[str] = ["fr", "ar", "en"]
    
    # Alert Configuration
    ALERT_COOLDOWN_MINUTES: int = 30
    
    @validator('ALLOWED_HOSTS', pre=True)
    def parse_allowed_hosts(cls, v):
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError:
                return [v]
        return v
    
    @validator('ALLOWED_FILE_TYPES', pre=True)
    def parse_allowed_file_types(cls, v):
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError:
                return v.split(',')
        return v
    
    @validator('SUPPORTED_LANGUAGES', pre=True)
    def parse_supported_languages(cls, v):
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError:
                return v.split(',')
        return v
    
    @validator('DEBUG', pre=True)
    def parse_debug(cls, v):
        if isinstance(v, str):
            return v.lower() in ('true', '1', 'yes', 'on')
        return v
    
    @validator('ENABLE_EMAIL_ALERTS', pre=True)
    def parse_enable_email_alerts(cls, v):
        if isinstance(v, str):
            return v.lower() in ('true', '1', 'yes', 'on')
        return v
    
    class Config:
        env_file = ".env"
        case_sensitive = True
        extra = "ignore"  # This allows extra fields in .env without validation errors


# Alert thresholds configuration (as constants)
ALERT_THRESHOLDS = {
    "temperature": {
        "unit": "Â°C",
        "optimal_min": 20.0,
        "optimal_max": 28.0,
        "warning_min": 18.0,
        "warning_max": 30.0,
        "critical_min": 15.0,
        "critical_max": 35.0
    },
    "ph": {
        "unit": "pH",
        "optimal_min": 6.5,
        "optimal_max": 8.5,
        "warning_min": 6.0,
        "warning_max": 9.0,
        "critical_min": 5.5,
        "critical_max": 9.5
    },
    "dissolved_oxygen": {
        "unit": "mg/L",
        "optimal_min": 5.0,
        "optimal_max": 12.0,
        "warning_min": 3.0,
        "warning_max": 15.0,
        "critical_min": 2.0,
        "critical_max": 20.0
    },
    "turbidity": {
        "unit": "NTU",
        "optimal_min": 0.0,
        "optimal_max": 10.0,
        "warning_min": 0.0,
        "warning_max": 25.0,
        "critical_min": 0.0,
        "critical_max": 50.0
    },
    "ammonia": {
        "unit": "mg/L",
        "optimal_min": 0.0,
        "optimal_max": 0.25,
        "warning_min": 0.0,
        "warning_max": 0.5,
        "critical_min": 0.0,
        "critical_max": 1.0
    },
    "nitrate": {
        "unit": "mg/L",
        "optimal_min": 0.0,
        "optimal_max": 20.0,
        "warning_min": 0.0,
        "warning_max": 40.0,
        "critical_min": 0.0,
        "critical_max": 80.0
    }
}

# Health scoring weights
HEALTH_WEIGHTS = {
    "temperature": 0.25,
    "ph": 0.20,
    "dissolved_oxygen": 0.30,
    "turbidity": 0.10,
    "ammonia": 0.10,
    "nitrate": 0.05
}

# Multilingual alert messages
ALERT_MESSAGES = {
    "fr": {
        "critical_temp_high": "TempÃ©rature critique Ã©levÃ©e: {value}{unit} dans {pond_name}",
        "critical_temp_low": "TempÃ©rature critique basse: {value}{unit} dans {pond_name}",
        "critical_oxygen_low": "OxygÃ¨ne dissous critique: {value}{unit} dans {pond_name}",
        "critical_ph_high": "pH critique Ã©levÃ©: {value}{unit} dans {pond_name}",
        "critical_ph_low": "pH critique bas: {value}{unit} dans {pond_name}",
        "warning_temperature": "Alerte tempÃ©rature: {value}{unit} dans {pond_name}",
        "warning_ph": "Alerte pH: {value}{unit} dans {pond_name}",
        "warning_dissolved_oxygen": "Alerte oxygÃ¨ne: {value}{unit} dans {pond_name}",
        "warning_turbidity": "Alerte turbiditÃ©: {value}{unit} dans {pond_name}",
        "warning_ammonia": "Alerte ammoniac: {value}{unit} dans {pond_name}",
        "warning_nitrate": "Alerte nitrate: {value}{unit} dans {pond_name}",
        "anomaly_detected": "Anomalie dÃ©tectÃ©e - ParamÃ¨tres: {parameters}. Score: {score}",
        "system_error": "Erreur systÃ¨me dans {pond_name}: {error_message}"
    },
    "ar": {
        "critical_temp_high": "Ø¯Ø±Ø¬Ø© Ø­Ø±Ø§Ø±Ø© Ø­Ø±Ø¬Ø© Ø¹Ø§ÙÙØ©: {value}{unit} ÙÙ {pond_name}",
        "critical_temp_low": "Ø¯Ø±Ø¬Ø© Ø­Ø±Ø§Ø±Ø© Ø­Ø±Ø¬Ø© ÙÙØ®ÙØ¶Ø©: {value}{unit} ÙÙ {pond_name}",
        "critical_oxygen_low": "Ø£ÙØ³Ø¬ÙÙ ÙÙØ­Ù Ø­Ø±Ø¬: {value}{unit} ÙÙ {pond_name}",
        "critical_ph_high": "Ø±ÙÙ ÙÙØ¯Ø±ÙØ¬ÙÙÙ Ø­Ø±Ø¬ Ø¹Ø§ÙÙ: {value}{unit} ÙÙ {pond_name}",
        "critical_ph_low": "Ø±ÙÙ ÙÙØ¯Ø±ÙØ¬ÙÙÙ Ø­Ø±Ø¬ ÙÙØ®ÙØ¶: {value}{unit} ÙÙ {pond_name}",
        "warning_temperature": "ØªØ­Ø°ÙØ± Ø¯Ø±Ø¬Ø© Ø§ÙØ­Ø±Ø§Ø±Ø©: {value}{unit} ÙÙ {pond_name}",
        "warning_ph": "ØªØ­Ø°ÙØ± Ø§ÙØ±ÙÙ Ø§ÙÙÙØ¯Ø±ÙØ¬ÙÙÙ: {value}{unit} ÙÙ {pond_name}",
        "warning_dissolved_oxygen": "ØªØ­Ø°ÙØ± Ø§ÙØ£ÙØ³Ø¬ÙÙ: {value}{unit} ÙÙ {pond_name}",
        "warning_turbidity": "ØªØ­Ø°ÙØ± Ø§ÙØ¹ÙØ§Ø±Ø©: {value}{unit} ÙÙ {pond_name}",
        "warning_ammonia": "ØªØ­Ø°ÙØ± Ø§ÙØ£ÙÙÙÙØ§: {value}{unit} ÙÙ {pond_name}",
        "warning_nitrate": "ØªØ­Ø°ÙØ± Ø§ÙÙØªØ±Ø§Øª: {value}{unit} ÙÙ {pond_name}",
        "anomaly_detected": "ØªÙ Ø§ÙØªØ´Ø§Ù Ø´Ø°ÙØ° - Ø§ÙÙØ¹Ø§ÙÙØ±: {parameters}. Ø§ÙÙØªÙØ¬Ø©: {score}",
        "system_error": "Ø®Ø·Ø£ ÙÙ Ø§ÙÙØ¸Ø§Ù ÙÙ {pond_name}: {error_message}"
    },
    "en": {
        "critical_temp_high": "Critical high temperature: {value}{unit} in {pond_name}",
        "critical_temp_low": "Critical low temperature: {value}{unit} in {pond_name}",
        "critical_oxygen_low": "Critical low dissolved oxygen: {value}{unit} in {pond_name}",
        "critical_ph_high": "Critical high pH: {value}{unit} in {pond_name}",
        "critical_ph_low": "Critical low pH: {value}{unit} in {pond_name}",
        "warning_temperature": "Temperature warning: {value}{unit} in {pond_name}",
        "warning_ph": "pH warning: {value}{unit} in {pond_name}",
        "warning_dissolved_oxygen": "Oxygen warning: {value}{unit} in {pond_name}",
        "warning_turbidity": "Turbidity warning: {value}{unit} in {pond_name}",
        "warning_ammonia": "Ammonia warning: {value}{unit} in {pond_name}",
        "warning_nitrate": "Nitrate warning: {value}{unit} in {pond_name}",
        "anomaly_detected": "Anomaly detected - Parameters: {parameters}. Score: {score}",
        "system_error": "System error in {pond_name}: {error_message}"
    }
}

# Water quality grade mappings
HEALTH_GRADE_THRESHOLDS = {
    90: {"grade": "A+", "status": "Excellent", "color": "#00C851"},
    80: {"grade": "A", "status": "Very Good", "color": "#2E7D32"},
    70: {"grade": "B", "status": "Good", "color": "#4CAF50"},
    60: {"grade": "C", "status": "Fair", "color": "#FF9800"},
    50: {"grade": "D", "status": "Poor", "color": "#FF5722"},
    0: {"grade": "F", "status": "Critical", "color": "#F44336"}
}

# Create settings instance
settings = Settings()


--- FILE : app/database.py ---


"""
Database connection and session management
Handles SQLAlchemy setup and provides database session dependency
"""

from sqlalchemy import create_engine, MetaData
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import StaticPool
import logging

from app.config import settings

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create SQLAlchemy engine
# For PostgreSQL with connection pooling
engine = create_engine(
    settings.DATABASE_URL,
    pool_pre_ping=True,  # Verify connections before use
    pool_recycle=300,    # Recycle connections every 5 minutes
    pool_size=20,        # Connection pool size
    max_overflow=30,     # Maximum overflow connections
    echo=False  # Log SQL queries in debug mode
)

# Create session maker
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create declarative base for models
Base = declarative_base()

# Metadata for migrations
metadata = MetaData()


def get_db() -> Session: # type: ignore
    """
    Dependency function to get database session
    Automatically handles session lifecycle
    """
    db = SessionLocal()
    try:
        yield db
    except Exception as e:
        logger.error(f"Database session error: {e}")
        db.rollback()
        raise
    finally:
        db.close()


def init_db():
    """
    Initialize database tables
    Creates all tables defined in models
    """
    try:
        Base.metadata.create_all(bind=engine)
        logger.info("Database tables created successfully")
    except Exception as e:
        logger.error(f"Error creating database tables: {e}")
        raise


def check_db_connection():
    """
    Check database connection health
    Returns True if connection is healthy
    """
    try:
        with engine.connect() as connection:
            connection.execute("SELECT 1")
        return True
    except Exception as e:
        logger.error(f"Database connection failed: {e}")
        return False


--- FILE : app/main.py ---


"""
Main FastAPI application
Aquaculture Management System for Algeria
"""

import asyncio
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from fastapi import Depends, FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy import and_
from starlette.middleware.sessions import SessionMiddleware
import uvicorn
import logging
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from app.models.pond import Pond, User, UserRole
from app.models.sensor import SensorData
from sqlalchemy.orm import Session
from app.api.deps import get_current_active_user
import time


from app.config import settings
from app.database import engine, Base, get_db
from app.api.endpoints import auth, ponds, sensors, alerts, users
from app.tasks.data_aggregation import (
    aggregate_hourly_data,
    aggregate_daily_data,
    cleanup_old_data,
    system_health_check
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Global scheduler
scheduler = AsyncIOScheduler()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan manager
    Handles startup and shutdown events
    """
    # Startup
    logger.info("Starting Aquaculture Management System")
    
    # Create database tables
    Base.metadata.create_all(bind=engine)
    logger.info("Database tables created/verified")
    
    # Start scheduler for background tasks
    scheduler.start()
    logger.info("Background task scheduler started")
    
    # Schedule background tasks
    _schedule_background_tasks()
    
    logger.info("Application startup complete")
    
    yield
    
    # Shutdown
    logger.info("Shutting down application")
    scheduler.shutdown()
    logger.info("Background task scheduler stopped")


def _schedule_background_tasks():
    """
    Schedule all background tasks
    """
    # Hourly data aggregation (every hour at minute 5)
    scheduler.add_job(
        aggregate_hourly_data,
        CronTrigger(minute=5),
        id="hourly_aggregation",
        name="Hourly Data Aggregation",
        replace_existing=True
    )
    
    # Daily data aggregation (every day at 00:10)
    scheduler.add_job(
        aggregate_daily_data,
        CronTrigger(hour=0, minute=10),
        id="daily_aggregation",
        name="Daily Data Aggregation",
        replace_existing=True
    )
    
    # Weekly data cleanup (every Sunday at 02:00)
    scheduler.add_job(
        cleanup_old_data,
        CronTrigger(day_of_week=6, hour=2, minute=0),
        id="data_cleanup",
        name="Data Cleanup",
        replace_existing=True
    )
    
    # System health check (every 15 minutes)
    scheduler.add_job(
        system_health_check,
        CronTrigger(minute="*/15"),
        id="health_check",
        name="System Health Check",
        replace_existing=True
    )
    
    logger.info("Background tasks scheduled")


# Create FastAPI application
app = FastAPI(
    title="Aquaculture Management System",
    description="""
    Comprehensive aquaculture pond management system for Algeria
    
    Features:
    - Real-time water quality monitoring
    - Intelligent health assessment
    - Multi-language support (French/Arabic)
    - Advanced alerting system
    - IoT sensor integration
    - Data analytics and reporting
    """,
    version="1.0.0",
    contact={
        "name": "Aquaculture System Support",
        "email": "support@aquaculture-algeria.com"
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT"
    },
    lifespan=lifespan
)

# Add middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_HOSTS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(SessionMiddleware, secret_key=settings.SECRET_KEY)

# Custom middleware for request logging and monitoring
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """
    Log all requests for monitoring
    """
    start_time = time.time()
    
    # Process request
    response = await call_next(request)
    
    # Log request details
    process_time = time.time() - start_time
    logger.info(
        f"{request.method} {request.url} - "
        f"Status: {response.status_code} - "
        f"Time: {process_time:.3f}s"
    )
    
    # Add custom headers
    response.headers["X-Process-Time"] = str(process_time)
    response.headers["X-API-Version"] = "1.0.0"
    
    return response


# Exception handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """
    Custom HTTP exception handler
    """
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": True,
            "message": exc.detail,
            "status_code": exc.status_code,
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )


@app.exception_handler(ValueError)
async def value_error_handler(request: Request, exc: ValueError):
    """
    Handle validation errors
    """
    return JSONResponse(
        status_code=400,
        content={
            "error": True,
            "message": f"Validation error: {str(exc)}",
            "status_code": 400,
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """
    Handle unexpected errors
    """
    logger.error(f"Unexpected error: {str(exc)}", exc_info=True)
    
    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "message": "Internal server error. Please try again later.",
            "status_code": 500,
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )


# Include routers
app.include_router(auth.router, prefix="/api/v1")
app.include_router(users.router, prefix="/api/v1")
app.include_router(ponds.router, prefix="/api/v1")
app.include_router(sensors.router, prefix="/api/v1")
app.include_router(alerts.router, prefix="/api/v1")


# Health check endpoints
@app.get("/health")
async def health_check():
    """
    Simple health check endpoint
    """
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "environment": settings.ENVIRONMENT
    }


@app.get("/health/detailed")
async def detailed_health_check():
    """Detailed health check with system information"""
    from app.database import get_db
    from sqlalchemy import text
    
    try:
        # Test database connection with proper SQLAlchemy syntax
        db = next(get_db())
        result = db.execute(text("SELECT 1"))  # Use text() wrapper
        result.fetchone()  # Actually fetch the result
        db.close()
        db_status = "healthy"
    except Exception as e:
        db_status = f"unhealthy: {str(e)}"
    
    return {
        "status": "healthy" if db_status == "healthy" else "degraded",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "environment": settings.ENVIRONMENT,
        "components": {
            "database": db_status,
            "scheduler": "running" if scheduler.running else "stopped",
            "scheduled_jobs": len(scheduler.get_jobs())
        }
    }


# Root endpoint
@app.get("/")
async def root():
    """
    API root endpoint
    """
    return {
        "message": "Aquaculture Management System API",
        "version": "1.0.0",
        "docs_url": "/docs",
        "health_check": "/health",
        "environment": settings.ENVIRONMENT
    }


# Dashboard summary endpoint
@app.get("/api/v1/dashboard")
async def get_dashboard_summary(
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db)
):
    """
    Get dashboard summary data
    """
    # Get user's ponds
    user_ponds = db.query(Pond).filter(Pond.assigned_users.any(id=current_user.id)).all()


    # Calculate summary statistics
    total_ponds = len(user_ponds)
    active_ponds = len([p for p in user_ponds if p.is_active])
    
    # Get active alerts
    from app.models.alert import Alert, AlertStatus, AlertSeverity
    if current_user.role != UserRole.ADMIN:
        # Non-admin users can only see their own ponds' alerts
        active_alerts = db.query(Alert).join(Pond).filter(
            and_(
                Pond.assigned_users.any(id=current_user.id),
                Alert.status == AlertStatus.ACTIVE
            )
        ).count()
    else:
        # Admins can see all active alerts
        active_alerts = db.query(Alert).filter(Alert.status == AlertStatus.ACTIVE).count()
    
    if current_user.role == UserRole.ADMIN:
        # Admins can see all ponds' critical alerts
        critical_alerts = db.query(Alert).filter(
            and_(
                Alert.status == AlertStatus.ACTIVE,
                Alert.severity == AlertSeverity.CRITICAL
            )
        ).count()
    else:
        # Non-admin users only see critical alerts for their ponds

        critical_alerts = db.query(Alert).join(Pond).filter(
            and_(
                Pond.assigned_users.any(id=current_user.id),
                Alert.status == AlertStatus.ACTIVE,
                Alert.severity == AlertSeverity.CRITICAL
            )
        ).count()
    
    # Get recent readings count
    recent_threshold = datetime.utcnow() - timedelta(hours=24)
    if current_user.role != UserRole.ADMIN:        
        recent_readings = db.query(SensorData).join(Pond).filter(
            and_(
                Pond.assigned_users.any(id=current_user.id),
                SensorData.timestamp >= recent_threshold
            )
        ).count()
    
    # Get health distribution (simplified)
    health_distribution = {
        "excellent": 0,
        "good": 0,
        "fair": 0,
        "poor": 0
    }
    
    # This would be calculated from actual health assessments
    # For now, just placeholder values
    for pond in user_ponds:
        # This would call the health calculator
        health_distribution["good"] += 1
    
    return {
        "total_ponds": total_ponds,
        "active_ponds": active_ponds,
        "total_alerts": active_alerts,
        "critical_alerts": critical_alerts,
        "warning_alerts": active_alerts - critical_alerts,
        "recent_readings_count": recent_readings,
        "health_distribution": health_distribution,
        "last_updated": datetime.utcnow().isoformat()
    }


if __name__ == "__main__":
    import time
    
    # Run the application
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info"
    )


--- FILE : app/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/api/deps.py ---


"""
Dependency injection utilities
Common dependencies for API endpoints
"""

from typing import Optional, Tuple
from fastapi import Depends, HTTPException, status, Query
from fastapi.security import OAuth2PasswordBearer
from sqlalchemy.orm import Session
from jose import JWTError

from app.database import get_db
from app.models.pond import User, Pond, UserRole
from app.core.security import verify_token  # Now this import should work
from app.config import settings

# OAuth2 scheme for token authentication
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="api/v1/auth/login")


async def get_current_user(
    token: str = Depends(oauth2_scheme),
    db: Session = Depends(get_db)
) -> User:
    """
    Get current user from JWT token
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    # Verify token
    payload = verify_token(token)
    if payload is None:
        raise credentials_exception
    
    # Extract user ID
    user_id: str = payload.get("sub")
    if user_id is None:
        raise credentials_exception
    
    try:
        user_id = int(user_id)
    except ValueError:
        raise credentials_exception
    
    # Get user from database
    user = db.query(User).filter(User.id == user_id).first()
    if user is None:
        raise credentials_exception
    
    return user


async def get_current_active_user(
    current_user: User = Depends(get_current_user)
) -> User:
    """
    Get current active user (must be active and verified)
    """
    if not current_user.is_active:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Inactive user"
        )
    
    return current_user


async def get_current_admin_user(
    current_user: User = Depends(get_current_active_user)
) -> User:
    """
    Get current admin user (must be active, verified, and admin)
    """
    if not current_user.role == UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )
    
    return current_user


def check_pond_ownership(
    pond_id: int,
    current_user: User,
    db: Session
) -> Pond:
    """
    Check if current user owns the specified pond
    """
    pond = db.query(Pond).filter(Pond.id == pond_id).first()
    
    if not pond:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Pond not found"
        )
    
    if current_user.id not in pond.assigned_users and not current_user.role == UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions to access this pond"
        )
    
    return pond


# Fix the get_pagination_params function to return individual values

def get_pagination_params(
    skip: int = Query(0, ge=0, description="Number of records to skip"),
    limit: int = Query(100, ge=1, le=1000, description="Number of records to return")
) -> tuple[int, int]:  # This returns a tuple, causing the issue
    """Get pagination parameters with validation"""
    return skip, limit



def get_date_range_params(
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)")
) -> Tuple[Optional[str], Optional[str]]:
    """
    Get date range parameters for filtering
    """
    return start_date, end_date


def get_sensor_type_filter(
    sensor_type: Optional[str] = Query(None, description="Filter by sensor type")
) -> Optional[str]:
    """
    Get sensor type filter parameter
    """
    if sensor_type:
        valid_types = ["temperature", "ph", "dissolved_oxygen", "turbidity", "ammonia", "nitrate"]
        if sensor_type not in valid_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid sensor type. Must be one of: {', '.join(valid_types)}"
            )
    
    return sensor_type


--- FILE : app/api/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/api/endpoints/alerts.py ---


"""
Alert management API endpoints
Handles alert rules, active alerts, and alert acknowledgment
"""

from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc, func, or_
from datetime import datetime, timedelta

from app.database import get_db
from app.models.alert import Alert, AlertRule, AlertStatus, AlertSeverity
from app.models.pond import Pond, User, UserRole
from app.schemas import alert as alert_schemas
from app.api.deps import get_current_active_user, check_pond_ownership, get_pagination_params
from app.services.notification import NotificationService

router = APIRouter(prefix="/alerts", tags=["alerts"])


# Alert Rules Management
@router.get("/rules", response_model=List[alert_schemas.AlertRuleResponse])
async def get_alert_rules(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
    pond_id: Optional[int] = Query(None),
    active_only: bool = Query(True)
):
    """
    Get alert rules for user's ponds
    """
    if current_user.role != UserRole.ADMIN:
        query = db.query(AlertRule).join(Pond).filter(Pond.assigned_users.any(id=current_user.id))
    else:
        query = db.query(AlertRule)

    if pond_id and current_user != UserRole.ADMIN:
        check_pond_ownership(pond_id, current_user, db)
        query = query.filter(AlertRule.pond_id == pond_id)
    
    if active_only:
        query = query.filter(AlertRule.is_active == True)
    
    rules = query.all()
    return rules


@router.post("/rules", response_model=alert_schemas.AlertRuleResponse, status_code=status.HTTP_201_CREATED)
async def create_alert_rule(
    rule_data: alert_schemas.AlertRuleCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Create a new alert rule
    """
    check_pond_ownership(rule_data.pond_id, current_user, db)
    
    # Create alert rule
    alert_rule = AlertRule(
        **rule_data.dict(),
        created_by=current_user.id
    )
    
    db.add(alert_rule)
    db.commit()
    db.refresh(alert_rule)
    
    return alert_rule


@router.put("/rules/{rule_id}", response_model=alert_schemas.AlertRuleResponse)
async def update_alert_rule(
    rule_id: int,
    rule_update: alert_schemas.AlertRuleUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Update an alert rule
    """
    if current_user.role != UserRole.ADMIN:
        # Get rule and check ownership
        rule = db.query(AlertRule).join(Pond).filter(
            and_(
                AlertRule.id == rule_id,
                Pond.owner_id == current_user.id
            )
        ).first()
    else:
        rule = db.query(AlertRule).filter(AlertRule.id == rule_id).first()
    
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Alert rule not found"
        )
    
    # Update fields
    update_data = rule_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(rule, field, value)
    
    db.commit()
    db.refresh(rule)
    
    return rule


@router.delete("/rules/{rule_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_alert_rule(
    rule_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Delete an alert rule
    """
    # Get rule and check ownership
    if current_user.role != UserRole.ADMIN:
    
        rule = db.query(AlertRule).join(Pond).filter(
            and_(
                AlertRule.id == rule_id,
                Pond.owner_id == current_user.id
            )
        ).first()
    else:
        rule = db.query(AlertRule).filter(AlertRule.id == rule_id).first()

    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Alert rule not found"
        )
    
    # Soft delete - just deactivate
    rule.is_active = False
    db.commit()


# Active Alerts Management
@router.get("/", response_model=List[alert_schemas.AlertResponse])
async def get_alerts(
    query_params: alert_schemas.AlertQuery = Depends(),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Get alerts with filtering options
    """
    if current_user.role != UserRole.ADMIN:
        # Non-admin users can only see their own ponds' alerts
        query_params.pond_id = current_user.id
        query = db.query(Alert).join(Pond).filter(Pond.assigned_users.any(id=current_user.id))
    else:
        # Admins can see all alerts
        query = db.query(Alert)
    
    # Apply filters
    if query_params.pond_id:
        check_pond_ownership(query_params.pond_id, current_user, db)
        query = query.filter(Alert.pond_id == query_params.pond_id)
    
    if query_params.severity:
        query = query.filter(Alert.severity == query_params.severity)
    
    if query_params.status:
        query = query.filter(Alert.status == query_params.status)
    
    if query_params.parameter:
        query = query.filter(Alert.parameter == query_params.parameter)
    
    if query_params.start_date:
        query = query.filter(Alert.triggered_at >= query_params.start_date)
    
    if query_params.end_date:
        query = query.filter(Alert.triggered_at <= query_params.end_date)
    
    # Ordering
    if query_params.order_direction == "desc":
        query = query.order_by(desc(getattr(Alert, query_params.order_by)))
    else:
        query = query.order_by(getattr(Alert, query_params.order_by))
    
    # Pagination
    alerts = query.offset(query_params.offset).limit(query_params.limit).all()
    
    # Enhance with pond names
    alert_responses = []
    for alert in alerts:
        pond = db.query(Pond).filter(Pond.id == alert.pond_id).first()
        alert_response = alert_schemas.AlertResponse(
            **alert.__dict__,
            pond_name=pond.name if pond else "Unknown"
        )
        alert_responses.append(alert_response)
    
    return alert_responses


@router.get("/active", response_model=List[alert_schemas.AlertResponse])
async def get_active_alerts(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
    severity: Optional[alert_schemas.AlertSeverity] = Query(None)
):
    """
    Get all active alerts for user's ponds
    """
    if current_user.role != UserRole.ADMIN:
        # Non-admin users can only see their own ponds' active alerts
        query = db.query(Alert).join(Pond).filter(
            and_(
                Pond.assigned_users.any(id=current_user.id),
                Alert.status == AlertStatus.ACTIVE
            )
        )
    else:
        # Admins can see all active alerts
        query = db.query(Alert).filter(Alert.status == AlertStatus.ACTIVE)
    
    if severity:
        query = query.filter(Alert.severity == severity)
    
    alerts = query.order_by(desc(Alert.triggered_at)).all()
    
    # Enhance with pond names
    alert_responses = []
    for alert in alerts:
        pond = db.query(Pond).filter(Pond.id == alert.pond_id).first()
        alert_response = alert_schemas.AlertResponse(
            **alert.__dict__,
            pond_name=pond.name if pond else "Unknown"
        )
        alert_responses.append(alert_response)
    
    return alert_responses


@router.post("/acknowledge", status_code=status.HTTP_200_OK)
async def acknowledge_alerts(
    acknowledge_data: alert_schemas.AlertAcknowledge,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Acknowledge multiple alerts
    """
    # Get alerts and verify ownership
    if current_user.role != UserRole.ADMIN:
        alerts = db.query(Alert).join(Pond).filter(
            and_(
                Alert.id.in_(acknowledge_data.alert_ids),
                Pond.assigned_users.any(id=current_user.id),
                Alert.status == AlertStatus.ACTIVE
            )
        ).all()

    else:
        # Admins can acknowledge any active alerts
        alerts = db.query(Alert).filter(
            and_(
                Alert.id.in_(acknowledge_data.alert_ids),
                Alert.status == AlertStatus.ACTIVE
            )
        ).all()
        
    if len(alerts) != len(acknowledge_data.alert_ids):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Some alerts not found or not accessible"
        )
    
    # Acknowledge alerts
    acknowledged_count = 0
    for alert in alerts:
        alert.status = AlertStatus.ACKNOWLEDGED
        alert.acknowledged_at = datetime.utcnow()
        alert.acknowledged_by = current_user.id
        acknowledged_count += 1
    
    db.commit()
    
    # Send notification about acknowledgment
    background_tasks.add_task(
        send_acknowledgment_notification,
        current_user.id,
        acknowledged_count,
        acknowledge_data.note
    )
    
    return {"message": f"Acknowledged {acknowledged_count} alerts"}


@router.post("/resolve", status_code=status.HTTP_200_OK)
async def resolve_alerts(
    resolve_data: alert_schemas.AlertResolve,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Resolve multiple alerts
    """
    # Get alerts and verify ownership
    alerts = db.query(Alert).join(Pond).filter(
        and_(
            Alert.id.in_(resolve_data.alert_ids),
            Pond.assigned_users.any(id = current_user.id),
            Alert.status.in_([AlertStatus.ACTIVE, AlertStatus.ACKNOWLEDGED])
        )
    ).all()
    
    if len(alerts) != len(resolve_data.alert_ids):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Some alerts not found or not accessible"
        )
    
    # Resolve alerts
    resolved_count = 0
    for alert in alerts:
        alert.status = AlertStatus.RESOLVED
        alert.resolved_at = datetime.utcnow()
        alert.resolved_by = current_user.id
        resolved_count += 1
    
    db.commit()
    
    return {"message": f"Resolved {resolved_count} alerts"}


@router.get("/statistics")
async def get_alert_statistics(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
    days: int = Query(30, ge=1, le=365)
):
    """
    Get alert statistics for dashboard
    """
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # Get user's pond IDs
    if current_user.role != UserRole.ADMIN:
        user_pond_ids = db.query(Pond.id).filter(Pond.assigned_users.any(id=current_user.id)).subquery()
    else:
        # Admins can see all ponds
        user_pond_ids = db.query(Pond.id).subquery()
    
    # Total alerts in period
    total_alerts = db.query(Alert).filter(
        and_(
            Alert.pond_id.in_(user_pond_ids),
            Alert.triggered_at >= start_date
        )
    ).count()
    
    # Active alerts
    active_alerts = db.query(Alert).filter(
        and_(
            Alert.pond_id.in_(user_pond_ids),
            Alert.status == AlertStatus.ACTIVE
        )
    ).count()
    
    # Critical alerts
    critical_alerts = db.query(Alert).filter(
        and_(
            Alert.pond_id.in_(user_pond_ids),
            Alert.severity == AlertSeverity.CRITICAL,
            Alert.triggered_at >= start_date
        )
    ).count()
    
    # Alerts by severity
    severity_counts = {}
    for severity in AlertSeverity:
        count = db.query(Alert).filter(
            and_(
                Alert.pond_id.in_(user_pond_ids),
                Alert.severity == severity,
                Alert.triggered_at >= start_date
            )
        ).count()
        severity_counts[severity.value] = count
    
    # Alerts by parameter
    parameter_counts = db.query(
        Alert.parameter,
        func.count(Alert.id).label('count')
    ).filter(
        and_(
            Alert.pond_id.in_(user_pond_ids),
            Alert.triggered_at >= start_date
        )
    ).group_by(Alert.parameter).all()
    
    # Recent alert trend (last 7 days)
    recent_trend = []
    for i in range(7):
        day_start = datetime.utcnow() - timedelta(days=i+1)
        day_end = datetime.utcnow() - timedelta(days=i)
        
        day_count = db.query(Alert).filter(
            and_(
                Alert.pond_id.in_(user_pond_ids),
                Alert.triggered_at >= day_start,
                Alert.triggered_at < day_end
            )
        ).count()
        
        recent_trend.append({
            "date": day_start.strftime("%Y-%m-%d"),
            "count": day_count
        })
    
    return {
        "total_alerts": total_alerts,
        "active_alerts": active_alerts,
        "critical_alerts": critical_alerts,
        "severity_breakdown": severity_counts,
        "parameter_breakdown": dict(parameter_counts),
        "recent_trend": list(reversed(recent_trend))
    }


async def send_acknowledgment_notification(user_id: int, count: int, note: Optional[str]):
    """
    Send notification about alert acknowledgment
    Background task function
    """
    # Implementation would send email/push notification
    # This is a placeholder for the notification service
    pass


--- FILE : app/api/endpoints/analytics.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/api/endpoints/auth.py ---


"""
Authentication endpoints
Handles user registration, login, and token management
"""

from datetime import datetime, timedelta
from typing import Any
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session

from app.database import get_db
from app.models.pond import User
from app.schemas.auth import Token, UserCreate, UserLogin, UserResponse  # Fixed import
from app.core.security import create_access_token, verify_password, get_password_hash
from app.config import settings
from app.api.deps import get_current_active_user

router = APIRouter(prefix="/auth", tags=["authentication"])


@router.post("/register", response_model=UserResponse, status_code=status.HTTP_201_CREATED)
async def register_user(
    user_data: UserCreate,
    db: Session = Depends(get_db)
):
    """
    Register a new user
    """
    # Check if user already exists
    existing_user = db.query(User).filter(
        (User.email == user_data.email) | (User.username == user_data.username)
    ).first()
    
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="User with this email or username already exists"
        )
    
    # Create new user
    hashed_password = get_password_hash(user_data.password)
    
    user = User(
        username=user_data.username,
        email=user_data.email,
        hashed_password=hashed_password,
        first_name=user_data.first_name,
        last_name=user_data.last_name,
        phone_number=user_data.phone_number,
        organization=user_data.organization,
        language=user_data.language or "fr",
        is_active=True,
        is_verified=False  # Email verification required
    )
    
    db.add(user)
    db.commit()
    db.refresh(user)
    
    return user


@router.post("/login", response_model=Token)
async def login_user(
    form_data: OAuth2PasswordRequestForm = Depends(),
    db: Session = Depends(get_db)
):
    """
    Login user and return access token
    """
    # Find user by username or email
    user = db.query(User).filter(
        (User.username == form_data.username) | (User.email == form_data.username)
    ).first()
    
    if not user or not verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username/email or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    if not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Inactive user"
        )
    
    # Update last login
    user.last_login = datetime.utcnow()
    db.commit()
    
    # Create access token
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": str(user.id)}, expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60,
        "user": user
    }


@router.post("/refresh", response_model=Token)
async def refresh_token(
    current_user: User = Depends(get_current_active_user),  # Fixed function name
    db: Session = Depends(get_db)
):
    """
    Refresh access token
    """
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": str(current_user.id)}, expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60,
        "user": current_user
    }


--- FILE : app/api/endpoints/ponds.py ---


"""
Pond management API endpoints
Handles CRUD operations for ponds and basic pond information
"""

from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_

from app.database import get_db
from app.models.pond import Pond, User
from app.schemas import pond as pond_schemas
from app.api.deps import get_current_active_user, check_pond_ownership, get_pagination_params
from app.core.health_calculator import calculate_pond_health
from app.services.data_processor import get_pond_latest_data, get_pond_statistics as pond_stats_service

router = APIRouter(prefix="/ponds", tags=["ponds"])


@router.get("/", response_model=List[pond_schemas.PondSummary])
async def get_ponds(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
    skip: int = Query(0, ge=0, description="Number of records to skip"),
    limit: int = Query(100, ge=1, le=1000, description="Number of records to return"),
    active_only: bool = Query(True, description="Show only active ponds"),
    search: Optional[str] = Query(None, description="Search pond names")
):
    """Get list of user's ponds with summary information"""
    # Show ponds the user owns OR is assigned to
    query = db.query(Pond).filter(
        or_(
            Pond.owner_id == current_user.id,
            Pond.assigned_users.any(id=current_user.id)
        )
    )    
    # Apply filters
    if active_only:
        query = query.filter(Pond.is_active == True)
    
    if search:
        query = query.filter(Pond.name.ilike(f"%{search}%"))
    
    # Apply pagination - FIXED
    ponds = query.offset(skip).limit(limit).all()
    
    # Rest of the function remains the same...
    pond_summaries = []
    for pond in ponds:
        health_data = await calculate_pond_health(pond.id, db)
        
        from app.models.alert import Alert, AlertStatus
        active_alerts = db.query(Alert).filter(
            and_(
                Alert.pond_id == pond.id,
                Alert.status == AlertStatus.ACTIVE
            )
        ).count()
        
        summary = pond_schemas.PondSummary(
            id=pond.id,
            name=pond.name,
            health_score=health_data.get("overall_score") if health_data else None,
            health_grade=health_data.get("grade") if health_data else None,
            status="Active" if pond.is_active else "Inactive",
            active_alerts_count=active_alerts,
            last_updated=pond.updated_at
        )
        pond_summaries.append(summary)
    
    return pond_summaries

@router.post("/", response_model=pond_schemas.PondResponse, status_code=status.HTTP_201_CREATED)
async def create_pond(
    pond_data: pond_schemas.PondCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    Create a new pond
    """
    # Create pond object
    pond = Pond(
        **pond_data.dict(),
        owner_id=current_user.id
    )
    
    db.add(pond)
    db.commit()
    db.refresh(pond)
    
    # Create default alert rules in background
    background_tasks.add_task(create_default_alert_rules, pond.id, db)
    
    return pond


@router.get("/{pond_id}", response_model=pond_schemas.PondWithStats)
async def get_pond(
    pond_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Get detailed pond information with statistics"""
    
    # No await needed - fixed
    pond = check_pond_ownership(pond_id, current_user, db)
    
    # Get additional statistics
    latest_reading = await get_pond_latest_data(pond_id, db)
    health_data = await calculate_pond_health(pond_id, db)
    
    # Get active alerts count
    from app.models.alert import Alert, AlertStatus
    active_alerts = db.query(Alert).filter(
        and_(
            Alert.pond_id == pond_id,
            Alert.status == AlertStatus.ACTIVE
        )
    ).count()
    
    # Create response with statistics
    pond_with_stats = pond_schemas.PondWithStats(
        **pond.__dict__,
        latest_reading=latest_reading,
        health_score=health_data.get("overall_score") if health_data else None,
        health_grade=health_data.get("grade") if health_data else None,
        active_alerts_count=active_alerts,
        last_data_timestamp=latest_reading.get("timestamp") if latest_reading else None
    )
    
    return pond_with_stats


@router.put("/{pond_id}", response_model=pond_schemas.PondResponse)
async def update_pond(
    pond_id: int,
    pond_update: pond_schemas.PondUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Update pond information"""
    
    # No await needed - fixed
    pond = check_pond_ownership(pond_id, current_user, db)
    
    # Update fields
    update_data = pond_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(pond, field, value)
    
    db.commit()
    db.refresh(pond)
    
    return pond


@router.delete("/{pond_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_pond(
    pond_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
    permanent: bool = Query(False, description="Permanently delete (vs soft delete)")
):
    """Delete pond (soft delete by default)"""
    
    # No await needed - fixed
    pond = check_pond_ownership(pond_id, current_user, db)
    
    if permanent:
        db.delete(pond)
    else:
        pond.is_active = False
    
    db.commit()


@router.get("/{pond_id}/health", response_model=pond_schemas.HealthAssessment)
async def get_pond_health(
    pond_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user),
    days: int = Query(7, ge=1, le=90, description="Days to analyze")
):
    """
    Get comprehensive pond health assessment
    """
    # Check ownership
    check_pond_ownership(pond_id, current_user, db)
    
    # Calculate health assessment
    health_data = await calculate_pond_health(pond_id, db, days=days)
    
    if not health_data:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Insufficient data for health assessment"
        )
    
    return health_data


@router.get("/{pond_id}/statistics")
async def get_pond_statistics(
    pond_id: int,
    days: int = Query(30, ge=1, le=365, description="Number of days for statistics"),
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db)
):
    """Get comprehensive statistics for a specific pond"""
    
    try:
        # Check pond ownership - no await needed now
        pond = check_pond_ownership(pond_id, current_user, db)
        
        # Get statistics
        stats = await pond_stats_service(pond_id, db, days)
        
        return {
            "pond_id": pond_id,
            "pond_name": pond.name,
            "owner_id": pond.owner_id,
            "statistics": stats,
            "success": True
        }
        
    except HTTPException:
        # Re-raise HTTP exceptions (like 404, 403)
        raise
    except Exception as e:
        # Handle any other errors
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving pond statistics: {str(e)}"
        )


def create_default_alert_rules(pond_id: int, db: Session):
    """
    Create default alert rules for a new pond
    Background task function
    """
    from app.models.alert import AlertRule, AlertSeverity
    from app.config import settings
    
    default_rules = []
    
    # Create rules based on your threshold analysis
    for parameter, thresholds in settings.ALERT_THRESHOLDS.items():
        # Critical low threshold
        if "critical_low" in thresholds:
            rule = AlertRule(
                pond_id=pond_id,
                parameter=parameter,
                rule_name=f"{parameter} Critical Low",
                description=f"Alert when {parameter} falls below critical threshold",
                min_threshold=thresholds["critical_low"],
                severity=AlertSeverity.CRITICAL,
                send_sms=True,
                cooldown_minutes=15
            )
            default_rules.append(rule)
        
        # Critical high threshold
        if "critical_high" in thresholds:
            rule = AlertRule(
                pond_id=pond_id,
                parameter=parameter,
                rule_name=f"{parameter} Critical High",
                description=f"Alert when {parameter} exceeds critical threshold",
                max_threshold=thresholds["critical_high"],
                severity=AlertSeverity.CRITICAL,
                send_sms=True,
                cooldown_minutes=15
            )
            default_rules.append(rule)
        
        # Warning thresholds
        if "warning_low" in thresholds:
            rule = AlertRule(
                pond_id=pond_id,
                parameter=parameter,
                rule_name=f"{parameter} Warning Low",
                description=f"Alert when {parameter} falls below warning threshold",
                min_threshold=thresholds["warning_low"],
                severity=AlertSeverity.WARNING,
                send_sms=False,
                cooldown_minutes=60
            )
            default_rules.append(rule)
        
        if "warning_high" in thresholds:
            rule = AlertRule(
                pond_id=pond_id,
                parameter=parameter,
                rule_name=f"{parameter} Warning High",
                description=f"Alert when {parameter} exceeds warning threshold",
                max_threshold=thresholds["warning_high"],
                severity=AlertSeverity.WARNING,
                send_sms=False,
                cooldown_minutes=60
            )
            default_rules.append(rule)
    
    # Add rules to database
    for rule in default_rules:
        db.add(rule)
    
    try:
        db.commit()
    except Exception as e:
        db.rollback()
        print(f"Error creating default alert rules: {e}")


--- FILE : app/api/endpoints/sensors.py ---


"""
Sensor data endpoints
Handle sensor data collection, validation, and storage
"""

from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks, Query
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError, SQLAlchemyError
from sqlalchemy import and_, desc, func

from app.api.deps import get_db, get_current_active_user
from app.models.pond import User, UserRole # Import UserRole
from app.models.pond import Pond
from app.models.sensor import SensorData
from app.schemas.sensor import (
    SensorDataCreate, 
    SensorDataResponse, 
    SensorDataBulkCreate,
    SensorDataQuery,
    SensorDataUpdate
)
from app.services.data_processor import (
    validate_sensor_data, 
    detect_anomalies,
    process_sensor_data_batch
)
from app.services.data_processor import process_sensor_alerts
import uuid
from app.services.alert_service import send_anomaly_alert_notification
from app.models.alert import Alert
from app.database import SessionLocal

router = APIRouter()


# Update the main sensor endpoint with better error tracking
@router.post("/data", response_model=SensorDataResponse, status_code=status.HTTP_201_CREATED)
async def add_sensor_data(
    sensor_data: SensorDataCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Create new sensor data reading with anomaly detection and alerts"""
    
    try:
        print(f"üîç Processing sensor data for pond {sensor_data.pond_id}")
        if current_user.role == UserRole.ADMIN:
            print(f"üë§ User {current_user.username} is an admin, proceeding with data submission")
            # Verify pond access
            pond = db.query(Pond).filter(
                Pond.id == sensor_data.pond_id,
            ).first()
        else:
            # Verify pond access
            pond = db.query(Pond).filter(
                Pond.id == sensor_data.pond_id,
                Pond.assigned_users.any(id=current_user.id)
            ).first()
        
        if not pond:
            print(f"‚ö†Ô∏è  Pond {sensor_data.pond_id} not found or no permission for user {current_user.username}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Pond not found or you don't have permission to add data to this pond"
            )
        
        print(f"‚úÖ Pond access verified for user '{current_user.username}' on pond '{pond.name}'")
        
        # Validate sensor data quality
        quality_score = validate_sensor_data(sensor_data)
        print(f"üìä Data quality score: {quality_score}")
        
        # Detect anomalies with Page-Hinkley method
        is_anomaly = False
        anomaly_alert_id = None
        
        try:
            print("üîç Running Page-Hinkley anomaly detection...")
            from app.services.page_hinkley import page_hinkley_service
            
            # Run anomaly detection with alert creation
            anomaly_results = await page_hinkley_service.detect_anomaly_with_alerts(
                sensor_data.pond_id, sensor_data, db
            )
            
            is_anomaly = anomaly_results['is_anomaly']
            anomaly_alert_id = anomaly_results.get('alert_id')
            
            if is_anomaly:
                print(f"üö® ANOMALY DETECTED in Pond {sensor_data.pond_id}")
                print(f"   Anomaly Score: {anomaly_results['anomaly_score']:.3f}")
                print(f"   Change Points: {anomaly_results['change_points_detected']}")
                if anomaly_alert_id:
                    print(f"   Alert Created: ID {anomaly_alert_id}")
            else:
                print("‚úÖ No anomaly detected")
            
        except Exception as anomaly_error:
            print(f"‚ùå Anomaly detection failed: {anomaly_error}")
            import traceback
            traceback.print_exc()
        
        # Create database record
        print("üíæ Creating sensor data record...")
        db_sensor_data = SensorData(
            pond_id=sensor_data.pond_id,
            timestamp=sensor_data.timestamp,
            temperature=sensor_data.temperature,
            ph=sensor_data.ph,
            dissolved_oxygen=sensor_data.dissolved_oxygen,
            turbidity=sensor_data.turbidity,
            ammonia=sensor_data.ammonia,
            nitrate=sensor_data.nitrate,
            nitrite=sensor_data.nitrite,
            salinity=sensor_data.salinity,
            fish_count=sensor_data.fish_count,
            fish_length=sensor_data.fish_length,
            fish_weight=sensor_data.fish_weight,
            water_level=sensor_data.water_level,
            flow_rate=sensor_data.flow_rate,
            data_source=sensor_data.data_source,
            quality_score=quality_score,
            is_anomaly=is_anomaly,
            entry_id=str(uuid.uuid4()),
            notes=sensor_data.notes
        )
        
        db.add(db_sensor_data)
        db.commit()
        db.refresh(db_sensor_data)
        print(f"‚úÖ Sensor data saved with ID: {db_sensor_data.id}")
        
        # If anomaly was detected and alert created, send email notification
        if is_anomaly and anomaly_alert_id:
            print(f"üìß Scheduling email notification for alert {anomaly_alert_id}")
            background_tasks.add_task(
                send_anomaly_email_notification,
                anomaly_alert_id,
                db_session_factory=SessionLocal
            )
        
        # Process regular sensor alerts in background
        print("üîî Scheduling alert processing...")
        background_tasks.add_task(
            process_sensor_alerts, 
            sensor_data.pond_id, 
            db_sensor_data.id
        )
        
        print("‚úÖ Request completed successfully")
        return db_sensor_data
        
    except Exception as e:
        print(f"‚ùå Unexpected error in add_sensor_data: {str(e)}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Unexpected error: {str(e)}"
        )


async def send_anomaly_email_notification(alert_id: int, db_session_factory):
    """Background task to send anomaly email notification"""
    db = db_session_factory()
    try:
        alert = db.query(Alert).filter(Alert.id == alert_id).first()
        if alert:
            print(f"üìß Sending email notification for alert {alert_id}")
            from app.services.alert_service import send_anomaly_alert_notification
            success = await send_anomaly_alert_notification(alert, db)
            if success:
                print(f"‚úÖ Email notification sent successfully for alert {alert_id}")
            else:
                print(f"‚ùå Failed to send email notification for alert {alert_id}")
        else:
            print(f"‚ö†Ô∏è  Alert {alert_id} not found for email notification")
    except Exception as e:
        print(f"‚ùå Error in background email task for alert {alert_id}: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


@router.post("/data/batch", status_code=status.HTTP_201_CREATED)
async def add_sensor_data_batch(
    batch_data: SensorDataBulkCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Create multiple sensor data readings in batch"""
    
    created_records = []
    errors = []
    
    try:
        # Process batch validation first
        batch_results = await process_sensor_data_batch(batch_data.readings, db)
        
        # Verify pond access for all ponds in batch
        pond_ids = list(set(reading.pond_id for reading in batch_data.readings))
        accessible_ponds = db.query(Pond.id).filter(
            Pond.id.in_(pond_ids),
            Pond.assigned_users.any(id=current_user.id)
        ).all()
        
        accessible_pond_ids = {pond.id for pond in accessible_ponds}
        
        for i, sensor_data in enumerate(batch_data.readings):
            try:
                # Check pond access
                if sensor_data.pond_id not in accessible_pond_ids:
                    errors.append(f"Reading {i}: Pond {sensor_data.pond_id} not found or no permission")
                    continue
                
                # Get quality score from batch processing
                quality_score = batch_results["quality_scores"][i] if i < len(batch_results["quality_scores"]) else 0.8
                
                # Create database record
                db_sensor_data = SensorData(
                    pond_id=sensor_data.pond_id,
                    timestamp=sensor_data.timestamp,
                    temperature=sensor_data.temperature,
                    ph=sensor_data.ph,
                    dissolved_oxygen=sensor_data.dissolved_oxygen,
                    turbidity=sensor_data.turbidity,
                    ammonia=sensor_data.ammonia,
                    nitrate=sensor_data.nitrate,
                    nitrite=sensor_data.nitrite,
                    salinity=sensor_data.salinity,
                    fish_count=sensor_data.fish_count,
                    fish_length=sensor_data.fish_length,
                    fish_weight=sensor_data.fish_weight,
                    water_level=sensor_data.water_level,
                    flow_rate=sensor_data.flow_rate,
                    data_source=sensor_data.data_source,
                    quality_score=quality_score,
                    is_anomaly=False,  # Set to False for batch, process later
                    entry_id=str(uuid.uuid4()),
                    notes=sensor_data.notes
                )
                
                db.add(db_sensor_data)
                created_records.append(db_sensor_data)
                
            except Exception as e:
                errors.append(f"Reading {i}: {str(e)}")
        
        # Commit all successful records
        if created_records:
            db.commit()
            
            # Refresh all created records
            for record in created_records:
                db.refresh(record)
            
            # Process alerts for all ponds in background
            for pond_id in accessible_pond_ids:
                background_tasks.add_task(
                    process_sensor_alerts, 
                    pond_id, 
                    None  # Process all recent data for the pond
                )
        
        return {
            "created": len(created_records),
            "errors": errors,
            "batch_analysis": batch_results,
            "success": len(created_records) > 0
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Batch processing error: {str(e)}"
        )

@router.get("/data", response_model=List[SensorDataResponse])
async def get_sensor_data(
    query: SensorDataQuery = Depends(),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Get sensor data with filtering and pagination"""
    
    try:
        # Base query for ponds the user is assigned to
        base_query = db.query(SensorData).join(Pond).filter(
            Pond.assigned_users.any(id=current_user.id)
        )
        
        # Apply filters
        if query.pond_id:
            base_query = base_query.filter(SensorData.pond_id == query.pond_id)
        
        if query.start_date:
            base_query = base_query.filter(SensorData.timestamp >= query.start_date)
        
        if query.end_date:
            base_query = base_query.filter(SensorData.timestamp <= query.end_date)
        
        if not query.include_anomalies:
            base_query = base_query.filter(SensorData.is_anomaly == False)
        
        # Apply ordering
        if query.order_direction == "desc":
            base_query = base_query.order_by(desc(getattr(SensorData, query.order_by)))
        else:
            base_query = base_query.order_by(getattr(SensorData, query.order_by))
        
        # Apply pagination
        sensor_data = base_query.offset(query.offset).limit(query.limit).all()
        
        return sensor_data
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving sensor data: {str(e)}"
        )


@router.get("/data/{sensor_id}", response_model=SensorDataResponse)
async def get_sensor_data_by_id(
    sensor_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Get specific sensor data by ID"""
    
    sensor_data = db.query(SensorData).join(Pond).filter(
        SensorData.id == sensor_id,
        Pond.assigned_users.any(id=current_user.id)
    ).first()
    
    if not sensor_data:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Sensor data not found or no permission"
        )
    
    return sensor_data


@router.put("/data/{sensor_id}", response_model=SensorDataResponse)
async def update_sensor_data(
    sensor_id: int,
    sensor_update: SensorDataUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Update sensor data"""
    
    try:
        # Get sensor data with access check
        sensor_data = db.query(SensorData).join(Pond).filter(
            SensorData.id == sensor_id,
            Pond.assigned_users.any(id=current_user.id)
        ).first()
        
        if not sensor_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Sensor data not found or no permission"
            )
        
        # Update fields
        update_data = sensor_update.dict(exclude_unset=True)
        for field, value in update_data.items():
            setattr(sensor_data, field, value)
        
        # Recalculate quality score if data parameters changed
        parameter_fields = {'temperature', 'ph', 'dissolved_oxygen', 'turbidity', 'ammonia', 'nitrate'}
        if any(field in update_data for field in parameter_fields):
            # Create a temporary schema object for validation
            from app.schemas.sensor import SensorDataCreate
            temp_data = SensorDataCreate(
                pond_id=sensor_data.pond_id,
                timestamp=sensor_data.timestamp,
                temperature=sensor_data.temperature,
                ph=sensor_data.ph,
                dissolved_oxygen=sensor_data.dissolved_oxygen,
                turbidity=sensor_data.turbidity,
                ammonia=sensor_data.ammonia,
                nitrate=sensor_data.nitrate,
                data_source=sensor_data.data_source
            )
            sensor_data.quality_score = validate_sensor_data(temp_data)
        
        db.commit()
        db.refresh(sensor_data)
        
        return sensor_data
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error updating sensor data: {str(e)}"
        )


@router.delete("/data/{sensor_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_sensor_data(
    sensor_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Delete sensor data"""
    
    try:
        # Check access before deleting
        sensor_data = db.query(SensorData).join(Pond).filter(
            SensorData.id == sensor_id,
            Pond.assigned_users.any(id=current_user.id)
        ).first()
        
        if not sensor_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Sensor data not found or no permission"
            )
        
        db.delete(sensor_data)
        db.commit()
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error deleting sensor data: {str(e)}"
        )
    

# Add this to your sensors.py router
@router.get("/pond/{pond_id}/anomaly-detector-status")
async def get_anomaly_detector_status(
    pond_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """Get Page-Hinkley detector status for a pond"""
    
    if current_user.role != UserRole.ADMIN:
        # Verify pond access
        pond = db.query(Pond).filter(
            Pond.id == pond_id,
            Pond.assigned_users.any(id=current_user.id)
        ).first()
    else:
        # Admins can access all ponds
        pond = db.query(Pond).filter(Pond.id == pond_id).first()

    print(f"üîç Checking anomaly detector status for pond {pond_id} by user {current_user.username}")
    
    if not pond:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Pond not found or no permission"
        )
    
    from app.services.page_hinkley import get_page_hinkley_diagnostics
    diagnostics = get_page_hinkley_diagnostics(pond_id)
    
    return {
        "pond_id": pond_id,
        "pond_name": pond.name,
        "detector_status": diagnostics,
        "timestamp": datetime.now(timezone.utc)
    }


--- FILE : app/api/endpoints/users.py ---


from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session, joinedload
from typing import List

from app.database import get_db
from app.models.pond import User, Pond, UserRole
from app.schemas import pond as pond_schemas
from app.api.deps import get_current_active_user
from app.core.health_calculator import calculate_pond_health

router = APIRouter(prefix="/users", tags=["User Management"])

def get_current_active_admin(current_user: User = Depends(get_current_active_user)):
    """Dependency to check if the current user is an admin."""
    if current_user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="The user doesn't have enough privileges"
        )
    return current_user

def convert_user_to_response(user: User, db: Session) -> pond_schemas.UserResponse:
    """
    Helper function to correctly convert a User model to a UserResponse schema,
    handling the nested PondSummary conversion.
    """
    assigned_ponds_summary = []
    for p in user.assigned_ponds:
        # Calculate health score and grade using the helper
        health_data = calculate_pond_health(pond_id=p.id, db=db)
        
        health_score = health_data["overall_score"] if health_data else 'N/A'
        health_grade = health_data["grade"] if health_data else "N/A"

        # This assumes your Alert model has an 'is_active' boolean field.
        active_alerts_count = sum(1 for alert in p.alerts if alert.status == "active")
        
        summary = pond_schemas.PondSummary(
            id=p.id,
            name=p.name,
            health_score=health_score,
            health_grade=health_grade,
            status="Active" if p.is_active else "Inactive",
            active_alerts_count=active_alerts_count,
            last_updated=p.updated_at
        )
        assigned_ponds_summary.append(summary)
    
    user_data = pond_schemas.UserInDB.from_orm(user).dict()
    
    user_response = pond_schemas.UserResponse(
        **user_data,
        assigned_ponds=assigned_ponds_summary
    )
    return user_response


@router.get("/", response_model=List[pond_schemas.UserResponse], dependencies=[Depends(get_current_active_admin)])
def get_all_users(db: Session = Depends(get_db), skip: int = 0, limit: int = 100):
    """
    Retrieve all users. (Admin only)
    """
    # Eager load all necessary relationships to prevent N+1 query issues
    users = db.query(User).options(
        joinedload(User.assigned_ponds).subqueryload(Pond.alerts),
        joinedload(User.assigned_ponds).subqueryload(Pond.sensor_data)
    ).offset(skip).limit(limit).all()
    
    return [convert_user_to_response(user, db) for user in users]


@router.post("/{user_id}/assign-pond/{pond_id}", response_model=pond_schemas.UserResponse, dependencies=[Depends(get_current_active_admin)])
def assign_pond_to_user(user_id: int, pond_id: int, db: Session = Depends(get_db)):
    """
    Assign a pond to a user. (Admin only)
    """
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    pond = db.query(Pond).filter(Pond.id == pond_id).first()
    if not pond:
        raise HTTPException(status_code=404, detail="Pond not found")

    if pond not in user.assigned_ponds:
        user.assigned_ponds.append(pond)
        db.commit()
    
    # Re-query the user with all relationships loaded for the response
    user_for_response = db.query(User).options(
        joinedload(User.assigned_ponds).subqueryload(Pond.alerts),
        joinedload(User.assigned_ponds).subqueryload(Pond.sensor_data)
    ).filter(User.id == user_id).first()
    
    return convert_user_to_response(user_for_response, db)

@router.delete("/{user_id}/unassign-pond/{pond_id}", response_model=pond_schemas.UserResponse, dependencies=[Depends(get_current_active_admin)])
def unassign_pond_from_user(user_id: int, pond_id: int, db: Session = Depends(get_db)):
    """
    Unassign a pond from a user. (Admin only)
    """
    user = db.query(User).options(joinedload(User.assigned_ponds)).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    pond = db.query(Pond).filter(Pond.id == pond_id).first()
    if not pond:
        raise HTTPException(status_code=404, detail="Pond not found")

    if pond in user.assigned_ponds:
        user.assigned_ponds.remove(pond)
        db.commit()

    # Re-query the user with all relationships loaded for the response
    user_for_response = db.query(User).options(
        joinedload(User.assigned_ponds).subqueryload(Pond.alerts),
        joinedload(User.assigned_ponds).subqueryload(Pond.sensor_data)
    ).filter(User.id == user_id).first()

    return convert_user_to_response(user_for_response, db)


--- FILE : app/api/endpoints/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/core/alert_engine.py ---


"""
Alert Processing Engine
Processes sensor data and triggers alerts based on configured rules
"""

import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc

from app.database import SessionLocal
from app.models.alert import Alert, AlertRule, AlertSeverity, AlertStatus
from app.models.sensor import SensorData
from app.models.pond import Pond, User, UserRole
from app.config import settings
from app.services.notification import NotificationService


async def process_sensor_data_for_alerts(
    sensor_reading_id: int,
    pond_id: int,
    sensor_data: Dict[str, Any]
) -> List[Alert]:
    """
    Process new sensor data and check for alert conditions
    This is called as a background task for each new sensor reading
    """
    db = SessionLocal()
    triggered_alerts = []
    
    try:
        # Get active alert rules for this pond
        alert_rules = db.query(AlertRule).filter(
            and_(
                AlertRule.pond_id == pond_id,
                AlertRule.is_active == True
            )
        ).all()
        
        for rule in alert_rules:
            # Check if this rule should trigger
            should_trigger = await _evaluate_alert_rule(rule, sensor_data, db)
            
            if should_trigger:
                # Check rate limiting
                if _is_rate_limited(rule, db):
                    continue
                
                # Create alert
                alert = await _create_alert(rule, sensor_reading_id, sensor_data, db)
                if alert:
                    triggered_alerts.append(alert)
                    
                    # Send notification asynchronously
                    asyncio.create_task(_send_alert_notification(alert, rule, db))
        
        return triggered_alerts
        
    except Exception as e:
        print(f"Error processing alerts: {e}")
        db.rollback()
        return []
    finally:
        db.close()


async def _evaluate_alert_rule(
    rule: AlertRule,
    sensor_data: Dict[str, Any],
    db: Session
) -> bool:
    """
    Evaluate if an alert rule should trigger based on sensor data
    """
    parameter_value = sensor_data.get(rule.parameter)
    
    if parameter_value is None:
        return False
    
    # Check threshold conditions
    threshold_violated = False
    
    if rule.min_threshold is not None and parameter_value < rule.min_threshold:
        threshold_violated = True
    
    if rule.max_threshold is not None and parameter_value > rule.max_threshold:
        threshold_violated = True
    
    # Check advanced conditions (if any)
    if rule.conditions and not _evaluate_advanced_conditions(rule.conditions, sensor_data, db):
        return False
    
    return threshold_violated


def _evaluate_advanced_conditions(
    conditions: Dict[str, Any],
    sensor_data: Dict[str, Any],
    db: Session
) -> bool:
    """
    Evaluate advanced alert conditions (JSON-based rules)
    Examples: consecutive readings, rate of change, multiple parameter conditions
    """
    # Example advanced conditions:
    # {"consecutive_violations": 3, "time_window_minutes": 30}
    # {"rate_of_change": {"threshold": 5, "time_minutes": 15}}
    # {"multiple_parameters": {"ph": {"min": 6.5}, "temperature": {"max": 30}}}
    
    consecutive_violations = conditions.get('consecutive_violations')
    if consecutive_violations:
        # Check if we've had consecutive violations
        # This would require querying recent sensor data
        pass
    
    rate_of_change = conditions.get('rate_of_change')
    if rate_of_change:
        # Check if parameter is changing too rapidly
        pass
    
    multiple_parameters = conditions.get('multiple_parameters')
    if multiple_parameters:
        # Check multiple parameter conditions
        for param, condition in multiple_parameters.items():
            param_value = sensor_data.get(param)
            if param_value is None:
                return False
            
            if 'min' in condition and param_value < condition['min']:
                return False
            if 'max' in condition and param_value > condition['max']:
                return False
    
    return True


def _is_rate_limited(rule: AlertRule, db: Session) -> bool:
    """
    Check if alert rule is rate limited (too many recent alerts)
    """
    now = datetime.utcnow()
    
    # Check cooldown period
    cooldown_start = now - timedelta(minutes=rule.cooldown_minutes)
    recent_alert = db.query(Alert).filter(
        and_(
            Alert.rule_id == rule.id,
            Alert.triggered_at >= cooldown_start
        )
    ).first()
    
    if recent_alert:
        return True
    
    # Check hourly limit
    hour_start = now - timedelta(hours=1)
    alerts_this_hour = db.query(Alert).filter(
        and_(
            Alert.rule_id == rule.id,
            Alert.triggered_at >= hour_start
        )
    ).count()
    
    if alerts_this_hour >= rule.max_alerts_per_hour:
        return True
    
    return False


async def _create_alert(
    rule: AlertRule,
    sensor_reading_id: int,
    sensor_data: Dict[str, Any],
    db: Session
) -> Optional[Alert]:
    """
    Create a new alert record
    """
    try:
        parameter_value = sensor_data.get(rule.parameter)
        threshold_value = rule.min_threshold if parameter_value < (rule.min_threshold or float('inf')) else rule.max_threshold
        
        # Generate multilingual messages
        messages = _generate_alert_messages(rule, parameter_value, threshold_value)
        
        alert = Alert(
            pond_id=rule.pond_id,
            rule_id=rule.id,
            parameter=rule.parameter,
            current_value=parameter_value,
            threshold_value=threshold_value,
            severity=rule.severity,
            title=messages['title'],
            message=messages['message'],
            message_fr=messages.get('message_fr'),
            message_ar=messages.get('message_ar'),
            sensor_reading_id=sensor_reading_id,
            context_data={
                'sensor_data': sensor_data,
                'rule_name': rule.rule_name,
                'rule_description': rule.description
            }
        )
        
        db.add(alert)
        db.commit()
        db.refresh(alert)
        
        return alert
        
    except Exception as e:
        print(f"Error creating alert: {e}")
        db.rollback()
        return None


def _generate_alert_messages(
    rule: AlertRule,
    current_value: float,
    threshold_value: Optional[float]
) -> Dict[str, str]:
    """
    Generate multilingual alert messages
    """
    pond = rule.pond
    parameter = rule.parameter
    severity = rule.severity
    unit = settings.ALERT_THRESHOLDS.get(parameter, {}).get('unit', '')
    
    # Determine alert type
    if rule.min_threshold and current_value < rule.min_threshold:
        alert_type = "low"
    else:
        alert_type = "high"
    
    # Generate message key
    if severity == AlertSeverity.CRITICAL:
        if parameter == "temperature":
            message_key = f"critical_temp_{alert_type}"
        elif parameter == "dissolved_oxygen":
            message_key = "critical_oxygen_low"
        elif parameter == "ph":
            message_key = f"critical_ph_{alert_type}"
        else:
            message_key = "critical_generic"
    else:
        message_key = f"warning_{parameter}"
    
    # Get messages from config
    messages_fr = settings.ALERT_MESSAGES.get('fr', {})
    messages_ar = settings.ALERT_MESSAGES.get('ar', {})
    
    # Format messages
    format_data = {
        'value': current_value,
        'unit': unit,
        'pond_name': pond.name if pond else f"Pond {rule.pond_id}",
        'threshold': threshold_value or 'N/A'
    }
    
    # Default English message
    title = f"{severity.value.title()} Alert: {parameter} {alert_type} in {pond.name if pond else 'pond'}"
    message = f"{parameter} is {current_value} {unit}, threshold: {threshold_value} {unit}"
    
    # French message
    message_fr = messages_fr.get(message_key, message).format(**format_data)
    
    # Arabic message
    message_ar = messages_ar.get(message_key, message).format(**format_data)
    
    return {
        'title': title,
        'message': message,
        'message_fr': message_fr,
        'message_ar': message_ar
    }


async def _send_alert_notification(alert: Alert, rule: AlertRule, db: Session):
    """
    Send alert notification via configured channels.
    Emails will be sent to assigned observers with admins in CC.
    """
    try:
        # Get pond
        pond = db.query(Pond).filter(Pond.id == alert.pond_id).first()
        if not pond:
            return

        # Get assigned observers for the pond
        observers = [user for user in pond.assigned_users if user.role == UserRole.OBSERVER and user.is_active]
        if not observers:
            # Fallback to pond owner if no observers are assigned
            if pond.owner and pond.owner.is_active:
                observers.append(pond.owner)
            else:
                return # No one to notify

        # Get all admin users to CC
        admins = db.query(User).filter(User.role == UserRole.ADMIN, User.is_active).all()
        
        notification_service = NotificationService()
        
        # Determine which notifications to send based on rule configuration
        # We will send one email to all observers with admins in CC.
        if rule.send_email:
            # Check if at least one observer has email notifications enabled
            if any(o.email_notifications for o in observers):
                try:
                    await notification_service.send_email_alert_to_observers(alert, observers, admins)
                except Exception as e:
                    print(f"Failed to send email alert for alert {alert.id}: {e}")

        # Send SMS and Push notifications to each observer individually
        for user in observers:
            notifications_sent = {}
            # SMS notification
            if rule.send_sms and user.sms_notifications and user.phone_number:
                try:
                    await notification_service.send_sms_alert(alert, user)
                    notifications_sent['sms'] = {'status': 'sent', 'recipient': user.phone_number}
                except Exception as e:
                    notifications_sent['sms'] = {'status': 'failed', 'error': str(e)}
            
            # Push notification
            if rule.send_push and user.push_notifications:
                try:
                    await notification_service.send_push_alert(alert, user)
                    notifications_sent['push'] = {'status': 'sent'}
                except Exception as e:
                    notifications_sent['push'] = {'status': 'failed', 'error': str(e)}
            
            # Update alert with notification status for the user
            if 'notifications' not in alert.context_data:
                alert.context_data['notifications'] = {}
            alert.context_data['notifications'][user.email] = notifications_sent
            
        db.add(alert)
        db.commit()

    except Exception as e:
        print(f"Error sending alert notification for alert {alert.id}: {e}")
        db.rollback()


def check_for_stale_data():
    """
    Check for ponds with stale data and create alerts
    This should be run as a scheduled task
    """
    db = SessionLocal()
    
    try:
        # Find ponds that haven't received data in the last hour
        stale_threshold = datetime.utcnow() - timedelta(hours=1)
        
        ponds_with_stale_data = db.query(Pond).filter(
            and_(
                Pond.is_active == True,
                ~Pond.id.in_(
                    db.query(SensorData.pond_id).filter(
                        SensorData.timestamp >= stale_threshold
                    ).distinct()
                )
            )
        ).all()
        
        for pond in ponds_with_stale_data:
            # Check if we already have a recent stale data alert
            recent_stale_alert = db.query(Alert).filter(
                and_(
                    Alert.pond_id == pond.id,
                    Alert.parameter == 'data_connectivity',
                    Alert.triggered_at >= datetime.utcnow() - timedelta(hours=2),
                    Alert.status == AlertStatus.ACTIVE
                )
            ).first()
            
            if not recent_stale_alert:
                # Create stale data alert
                alert = Alert(
                    pond_id=pond.id,
                    parameter='data_connectivity',
                    current_value=0,
                    threshold_value=1,
                    severity=AlertSeverity.WARNING,
                    title=f"No data received from {pond.name}",
                    message=f"No sensor data received from {pond.name} for over 1 hour",
                    message_fr=f"Aucune donnée reçue de {pond.name} depuis plus d'1 heure",
                    message_ar=f"لم يتم استلام بيانات من {pond.name} لأكثر من ساعة",
                    context_data={'alert_type': 'stale_data'}
                )
                
                db.add(alert)
        
        db.commit()
        
    except Exception as e:
        print(f"Error checking for stale data: {e}")
        db.rollback()
    finally:
        db.close()


--- FILE : app/core/health_calculator.py ---


"""
Pond Health Calculator
Implements the comprehensive health assessment algorithm based on your analysis
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc

from app.models.sensor import SensorData
from app.models.alert import Alert, AlertStatus, AlertSeverity
from app.models.pond import Pond
from app.config import settings


def calculate_pond_health(
    pond_id: int, 
    db: Session, 
    days: int = 7
) -> Optional[Dict[str, Any]]:
    """
    Calculate comprehensive pond health score based on your analysis algorithm
    
    Args:
        pond_id: Pond ID to analyze
        db: Database session
        days: Number of days to analyze (default 7)
    
    Returns:
        Dictionary with health assessment data or None if insufficient data
    """
    
    # Get sensor data for the specified period
    start_date = datetime.utcnow() - timedelta(days=days)
    
    sensor_data = db.query(SensorData).filter(
        and_(
            SensorData.pond_id == pond_id,
            SensorData.timestamp >= start_date
        )
    ).order_by(SensorData.timestamp).all()
    
    if len(sensor_data) < 10:  # Need minimum data points
        return None
    
    # Convert to DataFrame for easier analysis
    data_dict = {
        'temperature': [d.temperature for d in sensor_data if d.temperature is not None],
        'ph': [d.ph for d in sensor_data if d.ph is not None],
        'dissolved_oxygen': [d.dissolved_oxygen for d in sensor_data if d.dissolved_oxygen is not None],
        'turbidity': [d.turbidity for d in sensor_data if d.turbidity is not None],
        'ammonia': [d.ammonia for d in sensor_data if d.ammonia is not None],
        'nitrate': [d.nitrate for d in sensor_data if d.nitrate is not None]
    }
    
    # Calculate individual parameter scores
    parameter_scores = {}
    weighted_scores = []
    total_weight = 0
    warnings = []
    recommendations = []
    critical_issues = []
    
    parameters_assessed = 0
    
    for parameter, data in data_dict.items():
        if not data or len(data) < 3:  # Skip if insufficient data
            continue
            
        parameters_assessed += 1
        criteria = settings.ALERT_THRESHOLDS.get(parameter, {})
        weight = settings.HEALTH_WEIGHTS.get(parameter, 1.0)
        
        if not criteria:
            continue
            
        # Calculate parameter score using your algorithm
        score = calculate_parameter_score(data, criteria)
        parameter_scores[f"{parameter}_score"] = score
        
        # Add to weighted calculation
        weighted_scores.append(score * weight)
        total_weight += weight
        
        # Generate warnings and recommendations
        mean_val = np.mean(data)
        _analyze_parameter_health(parameter, mean_val, criteria, warnings, recommendations, critical_issues)
    
    if not weighted_scores:
        return None
    
    # Calculate overall scores
    overall_weighted_score = sum(weighted_scores) / total_weight
    overall_simple_score = np.mean(list(parameter_scores.values()))
    
    # Assign grade and status
    grade, status = _assign_grade_and_status(overall_weighted_score)
    
    # Risk assessment
    risk_level = _assess_risk_level(overall_weighted_score, len(warnings), len(critical_issues))
    
    # Action priority
    action_priority = _determine_action_priority(overall_weighted_score, len(critical_issues))
    
    # Data completeness
    total_possible_params = len(settings.ALERT_THRESHOLDS)
    data_completeness = (parameters_assessed / total_possible_params) * 100
    
    # Assessment confidence
    confidence = _calculate_confidence(len(sensor_data), parameters_assessed, data_completeness)
    
    # Get recent alert count
    recent_alerts = db.query(Alert).filter(
        and_(
            Alert.pond_id == pond_id,
            Alert.triggered_at >= start_date
        )
    ).count()
    
    # Prepare assessment result
    assessment = {
        "pond_id": pond_id,
        "overall_score": round(overall_weighted_score, 1),
        "weighted_score": round(overall_weighted_score, 1),
        "grade": grade,
        "status": status,
        "risk_level": risk_level,
        "warning_count": len(warnings),
        "critical_issues": critical_issues,
        "recommendations": recommendations,
        "action_priority": action_priority,
        "parameters_assessed": parameters_assessed,
        "data_completeness": round(data_completeness, 1),
        "assessment_confidence": round(confidence, 2),
        "assessment_period_start": start_date,
        "assessment_period_end": datetime.utcnow(),
        "calculated_at": datetime.utcnow(),
        **parameter_scores  # Individual parameter scores
    }
    
    return assessment


def calculate_parameter_score(data: List[float], criteria: Dict[str, float]) -> float:
    """
    Calculate score for a single parameter based on your scoring algorithm
    """
    if not data:
        return 0.0
    
    mean_value = np.mean(data)
    std_value = np.std(data)
    
    # Get thresholds
    optimal_min = criteria.get('optimal_min')
    optimal_max = criteria.get('optimal_max')
    warning_low = criteria.get('warning_low')
    warning_high = criteria.get('warning_high')
    critical_low = criteria.get('critical_low')
    critical_high = criteria.get('critical_high')
    
    # Determine if lower is better (for toxicity parameters)
    lower_is_better = 'optimal_min' not in criteria
    
    scores = []
    
    for value in data:
        if lower_is_better:
            # For parameters where lower is better (turbidity, ammonia, nitrate)
            if optimal_max and value <= optimal_max:
                score = 100  # Excellent
            elif warning_high and value <= warning_high:
                # Linear interpolation between optimal and warning
                score = 80 + (optimal_max - value) / (optimal_max - 0) * 20
                score = max(60, min(100, score))
            elif critical_high and value <= critical_high:
                # Linear interpolation between warning and critical
                score = 40 + (warning_high - value) / (warning_high - (optimal_max or 0)) * 40
                score = max(0, min(60, score))
            else:
                score = 0  # Critical/dangerous
        else:
            # For parameters with optimal ranges (temperature, pH, DO)
            if optimal_min and optimal_max and optimal_min <= value <= optimal_max:
                score = 100  # Excellent
            elif warning_low and warning_high and warning_low <= value <= warning_high:
                if value < optimal_min:
                    # Below optimal range
                    score = 80 + (value - warning_low) / (optimal_min - warning_low) * 20
                else:
                    # Above optimal range
                    score = 80 + (warning_high - value) / (warning_high - optimal_max) * 20
                score = max(60, min(100, score))
            elif critical_low and critical_high and critical_low <= value <= critical_high:
                if value < warning_low:
                    # Below warning range
                    score = 40 + (value - critical_low) / (warning_low - critical_low) * 20
                else:
                    # Above warning range
                    score = 40 + (critical_high - value) / (critical_high - warning_high) * 20
                score = max(0, min(60, score))
            else:
                score = 0  # Critical/dangerous
        
        scores.append(max(0, min(100, score)))
    
    # Account for variability (stability is good)
    base_score = np.mean(scores)
    stability_penalty = min(std_value / mean_value * 10, 10) if mean_value > 0 else 0
    
    final_score = max(0, base_score - stability_penalty)
    
    return round(final_score, 1)


def _analyze_parameter_health(
    parameter: str,
    mean_val: float,
    criteria: Dict[str, float],
    warnings: List[str],
    recommendations: List[str],
    critical_issues: List[str]
) -> None:
    """
    Analyze parameter health and add warnings/recommendations
    """
    optimal_min = criteria.get('optimal_min')
    optimal_max = criteria.get('optimal_max')
    warning_low = criteria.get('warning_low')
    warning_high = criteria.get('warning_high')
    critical_low = criteria.get('critical_low')
    critical_high = criteria.get('critical_high')
    unit = criteria.get('unit', '')
    
    # Check for critical issues
    if critical_low and mean_val < critical_low:
        critical_issues.append(f"Critical {parameter}: {mean_val:.2f} {unit} (< {critical_low})")
        recommendations.append(f"URGENT: Immediately address low {parameter}")
    elif critical_high and mean_val > critical_high:
        critical_issues.append(f"Critical {parameter}: {mean_val:.2f} {unit} (> {critical_high})")
        recommendations.append(f"URGENT: Immediately address high {parameter}")
    
    # Check for warnings
    elif warning_low and mean_val < warning_low:
        warnings.append(f"Low {parameter}: {mean_val:.2f} {unit}")
        recommendations.append(f"Monitor and consider increasing {parameter}")
    elif warning_high and mean_val > warning_high:
        warnings.append(f"High {parameter}: {mean_val:.2f} {unit}")
        recommendations.append(f"Monitor and consider reducing {parameter}")
    
    # Add specific recommendations based on parameter
    _add_parameter_specific_recommendations(parameter, mean_val, criteria, recommendations)


def _add_parameter_specific_recommendations(
    parameter: str,
    value: float,
    criteria: Dict[str, float],
    recommendations: List[str]
) -> None:
    """
    Add parameter-specific recommendations
    """
    if parameter == "temperature":
        if value < 20:
            recommendations.append("Consider adding heating system or insulation")
        elif value > 28:
            recommendations.append("Improve aeration and consider cooling methods")
    
    elif parameter == "ph":
        if value < 6.5:
            recommendations.append("Add lime or baking soda to increase pH")
        elif value > 8.5:
            recommendations.append("Add organic matter or use pH reducing agents")
    
    elif parameter == "dissolved_oxygen":
        if value < 5:
            recommendations.append("Increase aeration immediately")
            recommendations.append("Check for overstocking or overfeeding")
    
    elif parameter == "ammonia":
        if value > 0.5:
            recommendations.append("Reduce feeding and increase water changes")
            recommendations.append("Check biofilter efficiency")
    
    elif parameter == "turbidity":
        if value > 50:
            recommendations.append("Improve filtration system")
            recommendations.append("Reduce organic load in pond")


def _assign_grade_and_status(score: float) -> tuple:
    """
    Assign letter grade and status based on score
    """
    if score >= 90:
        return 'A+', 'Excellent'
    elif score >= 85:
        return 'A', 'Very Good'
    elif score >= 80:
        return 'B+', 'Good'
    elif score >= 75:
        return 'B', 'Satisfactory'
    elif score >= 70:
        return 'C+', 'Fair'
    elif score >= 60:
        return 'C', 'Poor'
    elif score >= 50:
        return 'D', 'Very Poor'
    else:
        return 'F', 'Critical'


def _assess_risk_level(score: float, warning_count: int, critical_count: int) -> str:
    """
    Assess risk level based on score and issues
    """
    if critical_count > 0 or score < 50:
        return "High"
    elif warning_count > 2 or score < 70:
        return "Medium"
    else:
        return "Low"


def _determine_action_priority(score: float, critical_count: int) -> str:
    """
    Determine action priority
    """
    if critical_count > 0 or score < 50:
        return "Urgent"
    elif score < 70:
        return "Improve"
    elif score < 85:
        return "Monitor"
    else:
        return "Maintain"


def _calculate_confidence(data_points: int, parameters_assessed: int, completeness: float) -> float:
    """
    Calculate confidence in the assessment
    """
    # Data volume factor (0-1)
    volume_factor = min(data_points / 100, 1.0)
    
    # Parameter completeness factor (0-1)
    completeness_factor = completeness / 100
    
    # Time factor (more data points over time = higher confidence)
    time_factor = min(data_points / 50, 1.0)
    
    # Combined confidence
    confidence = (volume_factor * 0.4 + completeness_factor * 0.4 + time_factor * 0.2)
    
    return confidence


--- FILE : app/core/security.py ---


"""
Security utilities
Handles password hashing, JWT token creation and verification
"""

from datetime import datetime, timedelta
from typing import Optional, Union
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import HTTPException, status

from app.config import settings

# Password hashing context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """
    Create a JWT access token
    """
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=settings.ALGORITHM)
    return encoded_jwt


def verify_token(token: str) -> Optional[dict]:
    """
    Verify and decode JWT token
    Returns token payload if valid, None if invalid
    """
    try:
        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM])
        return payload
    except JWTError:
        return None


def get_password_hash(password: str) -> str:
    """
    Hash a password using bcrypt
    """
    return pwd_context.hash(password)


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """
    Verify a password against its hash
    """
    return pwd_context.verify(plain_password, hashed_password)


def create_password_reset_token(email: str) -> str:
    """
    Create a password reset token
    """
    delta = timedelta(hours=1)  # Token expires in 1 hour
    now = datetime.utcnow()
    expires = now + delta
    exp = expires.timestamp()
    encoded_jwt = jwt.encode(
        {"exp": exp, "nbf": now, "sub": email, "type": "reset"},
        settings.SECRET_KEY,
        algorithm=settings.ALGORITHM,
    )
    return encoded_jwt


def verify_password_reset_token(token: str) -> Optional[str]:
    """
    Verify password reset token and return email if valid
    """
    try:
        decoded_token = jwt.decode(token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM])
        if decoded_token["type"] != "reset":
            return None
        return decoded_token["sub"]
    except JWTError:
        return None


def create_email_verification_token(email: str) -> str:
    """
    Create an email verification token
    """
    delta = timedelta(days=1)  # Token expires in 24 hours
    now = datetime.utcnow()
    expires = now + delta
    exp = expires.timestamp()
    encoded_jwt = jwt.encode(
        {"exp": exp, "nbf": now, "sub": email, "type": "verification"},
        settings.SECRET_KEY,
        algorithm=settings.ALGORITHM,
    )
    return encoded_jwt


def verify_email_verification_token(token: str) -> Optional[str]:
    """
    Verify email verification token and return email if valid
    """
    try:
        decoded_token = jwt.decode(token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM])
        if decoded_token["type"] != "verification":
            return None
        return decoded_token["sub"]
    except JWTError:
        return None


def get_user_id_from_token(token: str) -> Optional[int]:
    """
    Extract user ID from JWT token
    """
    try:
        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            return None
        return int(user_id)
    except (JWTError, ValueError):
        return None


def check_password_strength(password: str) -> dict:
    """
    Check password strength and return validation result
    """
    issues = []
    score = 0
    
    if len(password) < 8:
        issues.append("Password must be at least 8 characters long")
    else:
        score += 1
    
    if not any(c.isupper() for c in password):
        issues.append("Password must contain at least one uppercase letter")
    else:
        score += 1
    
    if not any(c.islower() for c in password):
        issues.append("Password must contain at least one lowercase letter")
    else:
        score += 1
    
    if not any(c.isdigit() for c in password):
        issues.append("Password must contain at least one number")
    else:
        score += 1
    
    if not any(c in "!@#$%^&*()_+-=[]{}|;:,.<>?" for c in password):
        issues.append("Password must contain at least one special character")
    else:
        score += 1
    
    strength_levels = ["Very Weak", "Weak", "Fair", "Good", "Strong"]
    strength = strength_levels[min(score, 4)]
    
    return {
        "is_valid": len(issues) == 0,
        "issues": issues,
        "strength": strength,
        "score": score
    }


--- FILE : app/core/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/models/alert.py ---


"""
Alert models - Manages alert rules, active alerts, and notifications
Implements the intelligent alerting system based on your threshold analysis
"""

from sqlalchemy import Column, Integer, String, Float, Boolean, DateTime, Text, ForeignKey, Enum as SQLEnum
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from enum import Enum 

from app.database import Base


# In app/models/alert.py - update the AlertType enum
class AlertType(str, Enum):
    # Temperature alerts
    HIGH_TEMPERATURE = "high_temperature"
    LOW_TEMPERATURE = "low_temperature"
    TEMPERATURE_FLUCTUATION = "temperature_fluctuation"
    
    # pH alerts
    HIGH_PH = "high_ph"
    LOW_PH = "low_ph"
    PH_FLUCTUATION = "ph_fluctuation"
    
    # Oxygen alerts
    LOW_OXYGEN = "low_oxygen"
    HIGH_OXYGEN = "high_oxygen"
    OXYGEN_FLUCTUATION = "oxygen_fluctuation"
    
    # Chemical alerts
    HIGH_AMMONIA = "high_ammonia"
    HIGH_NITRATE = "high_nitrate"
    HIGH_NITRITE = "high_nitrite"
    HIGH_TURBIDITY = "high_turbidity"
    
    # Water level alerts
    LOW_WATER_LEVEL = "low_water_level"
    HIGH_WATER_LEVEL = "high_water_level"
    
    # Equipment alerts
    PUMP_FAILURE = "pump_failure"
    FILTER_MAINTENANCE = "filter_maintenance"
    SENSOR_MALFUNCTION = "sensor_malfunction"
    
    # Fish health alerts
    FISH_MORTALITY = "fish_mortality"
    FISH_BEHAVIOR_CHANGE = "fish_behavior_change"
    
    # Data quality alerts
    DATA_QUALITY_LOW = "data_quality_low"
    SENSOR_OFFLINE = "sensor_offline"
    
    # System alerts
    SYSTEM_ERROR = "system_error"
    MAINTENANCE_REQUIRED = "maintenance_required"
    
    # Anomaly detection alerts - ADD THIS
    ANOMALY_DETECTED = "anomaly_detected"
    PATTERN_CHANGE = "pattern_change"
    UNUSUAL_TREND = "unusual_trend"

class AlertSeverity(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


class AlertStatus(Enum):
    """Alert status"""
    ACTIVE = "active"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"


class AlertRule(Base):
    """
    Alert Rules model
    Defines conditions that trigger alerts for each pond
    """
    __tablename__ = "alert_rules"
    
    id = Column(Integer, primary_key=True, index=True)
    pond_id = Column(Integer, ForeignKey("ponds.id"), nullable=False, index=True)
    
    # Rule definition
    parameter = Column(String(50), nullable=False, index=True)  # temperature, ph, etc.
    rule_name = Column(String(100), nullable=False)
    description = Column(Text, nullable=True)
    
    # Threshold values (based on your analysis)
    min_threshold = Column(Float, nullable=True, comment="Minimum acceptable value")
    max_threshold = Column(Float, nullable=True, comment="Maximum acceptable value")
    
    # Alert configuration
    severity = Column(SQLEnum(AlertSeverity), nullable=False, default=AlertSeverity.WARNING)
    is_active = Column(Boolean, default=True, index=True)
    
    # Advanced rule conditions (JSON for flexibility)
    conditions = Column(JSONB, nullable=True, default={})
    
    # Notification settings
    send_email = Column(Boolean, default=True)
    send_sms = Column(Boolean, default=False)
    send_push = Column(Boolean, default=True)
    
    # Rate limiting to prevent spam
    cooldown_minutes = Column(Integer, default=30, comment="Minutes between similar alerts")
    max_alerts_per_hour = Column(Integer, default=4)
    
    # Metadata
    created_by = Column(Integer, ForeignKey("users.id"), nullable=True)
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
    
    # Relationships
    pond = relationship("Pond")
    alerts = relationship("Alert", back_populates="rule")
    
    def __repr__(self):
        return f"<AlertRule(pond_id={self.pond_id}, parameter='{self.parameter}', severity='{self.severity.value}')>"


class Alert(Base):
    """
    Active Alerts model
    Stores triggered alerts and their status
    """
    __tablename__ = "alerts"
    
    id = Column(Integer, primary_key=True, index=True)
    pond_id = Column(Integer, ForeignKey("ponds.id"), nullable=False, index=True)
    rule_id = Column(Integer, ForeignKey("alert_rules.id"), nullable=True, index=True)
    
    # Alert details
    parameter = Column(String(50), nullable=False, index=True)
    current_value = Column(Float, nullable=False)
    threshold_value = Column(Float, nullable=True)
    
    # Severity and status
    severity = Column(SQLEnum(AlertSeverity), nullable=False, index=True)
    status = Column(SQLEnum(AlertStatus), nullable=False, default=AlertStatus.ACTIVE, index=True)
    
    # Messages (multilingual support)
    title = Column(String(200), nullable=False)
    message = Column(Text, nullable=False)
    message_ar = Column(Text, nullable=True, comment="Arabic translation")
    message_fr = Column(Text, nullable=True, comment="French translation")
    
    # Timing
    triggered_at = Column(DateTime, nullable=False, server_default=func.now(), index=True)
    acknowledged_at = Column(DateTime, nullable=True)
    resolved_at = Column(DateTime, nullable=True)
    
    # User actions
    acknowledged_by = Column(Integer, ForeignKey("users.id"), nullable=True)
    resolved_by = Column(Integer, ForeignKey("users.id"), nullable=True)
    
    # Additional context
    sensor_reading_id = Column(Integer, ForeignKey("sensor_data.id"), nullable=True)
    context_data = Column(JSONB, nullable=True, default={})
    
    # Notification tracking
    notifications_sent = Column(JSONB, nullable=True, default={})

    # Alert type
    alert_type = Column(SQLEnum(AlertType), nullable=False, default=AlertType.ANOMALY_DETECTED, index=True)
    
    # Relationships
    pond = relationship("Pond", back_populates="alerts")
    rule = relationship("AlertRule", back_populates="alerts")
    
    def __repr__(self):
        return f"<Alert(pond_id={self.pond_id}, parameter='{self.parameter}', severity='{self.severity.value}')>"


class PondHealth(Base):
    """
    Pond Health Records
    Stores calculated health scores and assessments based on your analysis
    """
    __tablename__ = "pond_health"
    
    id = Column(Integer, primary_key=True, index=True)
    pond_id = Column(Integer, ForeignKey("ponds.id"), nullable=False, index=True)
    
    # Health scores (based on your comprehensive assessment)
    overall_score = Column(Float, nullable=False, comment="Overall health score 0-100")
    weighted_score = Column(Float, nullable=False, comment="Weighted health score 0-100")
    grade = Column(String(5), nullable=False, comment="Letter grade A+ to F")
    status = Column(String(20), nullable=False, comment="Health status description")
    
    # Individual parameter scores
    temperature_score = Column(Float, nullable=True)
    ph_score = Column(Float, nullable=True)
    dissolved_oxygen_score = Column(Float, nullable=True)
    turbidity_score = Column(Float, nullable=True)
    ammonia_score = Column(Float, nullable=True)
    nitrate_score = Column(Float, nullable=True)
    
    # Risk assessment
    risk_level = Column(String(10), nullable=False, comment="Low, Medium, High")
    warning_count = Column(Integer, default=0)
    critical_issues = Column(JSONB, nullable=True, default=[])
    
    # Recommendations
    recommendations = Column(JSONB, nullable=True, default=[])
    action_priority = Column(String(20), nullable=True, comment="Maintain, Monitor, Improve, Urgent")
    
    # Data quality metrics
    parameters_assessed = Column(Integer, default=0)
    data_completeness = Column(Float, nullable=True, comment="Percentage of available parameters")
    assessment_confidence = Column(Float, nullable=True, comment="Confidence in assessment 0-1")
    
    # Time period for assessment
    assessment_period_start = Column(DateTime, nullable=False)
    assessment_period_end = Column(DateTime, nullable=False)
    calculated_at = Column(DateTime, server_default=func.now(), index=True)
    
    # Relationships
    pond = relationship("Pond", back_populates="health_records")
    
    def __repr__(self):
        return f"<PondHealth(pond_id={self.pond_id}, score={self.overall_score}, grade='{self.grade}')>"


class NotificationLog(Base):
    """
    Notification Log
    Tracks all sent notifications for debugging and analytics
    """
    __tablename__ = "notification_logs"
    
    id = Column(Integer, primary_key=True, index=True)
    alert_id = Column(Integer, ForeignKey("alerts.id"), nullable=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    
    # Notification details
    notification_type = Column(String(20), nullable=False)  # email, sms, push
    recipient = Column(String(200), nullable=False)  # email address, phone number, device token
    subject = Column(String(200), nullable=True)
    message = Column(Text, nullable=False)
    
    # Status tracking
    status = Column(String(20), nullable=False, default="pending")  # pending, sent, failed, delivered
    sent_at = Column(DateTime, nullable=True)
    delivered_at = Column(DateTime, nullable=True)
    
    # Error handling
    error_message = Column(Text, nullable=True)
    retry_count = Column(Integer, default=0)
    
    # Provider information
    provider = Column(String(50), nullable=True)  # twilio, firebase, smtp
    provider_response = Column(JSONB, nullable=True)
    
    created_at = Column(DateTime, server_default=func.now(), index=True)
    
    def __repr__(self):
        return f"<NotificationLog(type='{self.notification_type}', status='{self.status}')>"
    




--- FILE : app/models/pond.py ---


"""
Pond model - Represents fish ponds/tanks
Contains pond metadata, location, and configuration
"""

from sqlalchemy import Column, Integer, String, Float, Boolean, DateTime, Text, ForeignKey, Table, Enum
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
import uuid
import enum


from app.database import Base

# Define an Enum for user roles
class UserRole(str, enum.Enum):
    ADMIN = "admin"
    MANAGER = "manager"
    OBSERVER = "observer"

# Association table for the many-to-many relationship between users and ponds
user_pond_association = Table(
    'user_pond_association', Base.metadata,
    Column('user_id', Integer, ForeignKey('users.id'), primary_key=True),
    Column('pond_id', Integer, ForeignKey('ponds.id'), primary_key=True)
)

class Pond(Base):
    """
    Pond/Tank model
    Represents individual fish farming ponds with their characteristics
    """
    __tablename__ = "ponds"
    
    # Primary identification
    id = Column(Integer, primary_key=True, index=True)
    uuid = Column(UUID(as_uuid=True), default=uuid.uuid4, unique=True, index=True)
    
    # Basic information
    name = Column(String(100), nullable=False, index=True)
    description = Column(Text, nullable=True)
    
    # Physical characteristics
    capacity = Column(Float, nullable=True, comment="Capacity in liters")
    depth = Column(Float, nullable=True, comment="Depth in meters")
    surface_area = Column(Float, nullable=True, comment="Surface area in square meters")
    
    # Location (can store GPS coordinates)
    location_name = Column(String(200), nullable=True)
    latitude = Column(Float, nullable=True)
    longitude = Column(Float, nullable=True)
    
    # Fish information
    fish_species = Column(String(100), nullable=True)
    fish_count = Column(Integer, nullable=True, default=0)
    stocking_date = Column(DateTime, nullable=True)
    
    # System configuration
    aeration_system = Column(Boolean, default=False)
    filtration_system = Column(Boolean, default=False)
    heating_system = Column(Boolean, default=False)
    
    # Alert configuration (JSON field for flexible alert rules)
    alert_config = Column(JSONB, nullable=True, default={})
    
    # Owner/Manager information
    owner_id = Column(Integer, ForeignKey("users.id"), nullable=True)
    manager_contact = Column(String(100), nullable=True)
    
    # Status and metadata
    is_active = Column(Boolean, default=True, index=True)
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
    
    # Relationships
    sensor_data = relationship("SensorData", back_populates="pond", cascade="all, delete-orphan")
    alerts = relationship("Alert", back_populates="pond", cascade="all, delete-orphan")
    health_records = relationship("PondHealth", back_populates="pond", cascade="all, delete-orphan")
    owner = relationship("User", back_populates="owned_ponds")
    assigned_users = relationship(
        "User",
        secondary=user_pond_association,
        back_populates="assigned_ponds"
    )



    def __repr__(self):
        return f"<Pond(id={self.id}, name='{self.name}', active={self.is_active})>"


class User(Base):
    """
    User model for pond owners/managers
    Basic user management for the system
    """
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True, index=True)
    uuid = Column(UUID(as_uuid=True), default=uuid.uuid4, unique=True, index=True)
    
    # Authentication
    username = Column(String(50), unique=True, index=True, nullable=False)
    email = Column(String(100), unique=True, index=True, nullable=False)
    hashed_password = Column(String(255), nullable=False)
    
    # Personal information
    first_name = Column(String(50), nullable=True)
    last_name = Column(String(50), nullable=True)
    phone_number = Column(String(20), nullable=True)
    organization = Column(String(100), nullable=True)  # Add this missing field
    
    # Preferences
    language = Column(String(5), default="fr")  # Default to French for Algeria
    timezone = Column(String(50), default="Africa/Algiers")
    
    # Notification preferences
    email_notifications = Column(Boolean, default=True)
    sms_notifications = Column(Boolean, default=True)
    push_notifications = Column(Boolean, default=True)
    
    # Status and permissions
    role = Column(Enum(UserRole), nullable=False, default=UserRole.OBSERVER)
    is_active = Column(Boolean, default=True)
    is_verified = Column(Boolean, default=False)
    # is_admin = Column(Boolean, default=False)  # Add this for admin checking
    last_login = Column(DateTime, nullable=True)
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
    
    # Relationships
    owned_ponds = relationship("Pond", back_populates="owner")
    assigned_ponds = relationship(
        "Pond",
        secondary=user_pond_association,
        back_populates="assigned_users"
    )
    def __repr__(self):
        return f"<User(id={self.id}, username='{self.username}')>"


--- FILE : app/models/sensor.py ---


"""
Sensor data model - Stores all water quality measurements
This is the core data model containing all sensor readings from your datasets
"""

from sqlalchemy import Column, Integer, Float, DateTime, ForeignKey, Index, Text, String, Boolean
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.database import Base


class SensorData(Base):
    """
    Sensor Data model
    Stores time-series water quality measurements from IoT sensors
    Based on the parameters found in your pond datasets
    """
    __tablename__ = "sensor_data"
    
    # Primary key and relationships
    id = Column(Integer, primary_key=True, index=True)
    pond_id = Column(Integer, ForeignKey("ponds.id"), nullable=False, index=True)
    
    # Timestamp (critical for time-series analysis)
    timestamp = Column(DateTime, nullable=False, index=True)
    
    # Water quality parameters (based on your dataset analysis)
    temperature = Column(Float, nullable=True, comment="Temperature in Celsius")
    ph = Column(Float, nullable=True, comment="pH level (0-14)")
    dissolved_oxygen = Column(Float, nullable=True, comment="Dissolved oxygen in mg/L")
    turbidity = Column(Float, nullable=True, comment="Turbidity in NTU")
    ammonia = Column(Float, nullable=True, comment="Ammonia concentration in mg/L")
    nitrate = Column(Float, nullable=True, comment="Nitrate concentration in mg/L")
    nitrite = Column(Float, nullable=True, comment="Nitrite concentration in mg/L")
    salinity = Column(Float, nullable=True, comment="Salinity in ppt")
    
    # Fish-related measurements (from your datasets)
    fish_count = Column(Integer, nullable=True, comment="Number of fish observed")
    fish_length = Column(Float, nullable=True, comment="Average fish length in cm")
    fish_weight = Column(Float, nullable=True, comment="Average fish weight in grams")
    
    # Additional water parameters
    water_level = Column(Float, nullable=True, comment="Water level in cm")
    flow_rate = Column(Float, nullable=True, comment="Water flow rate in L/min")
    
    # Data quality and source tracking
    data_source = Column(String(50), nullable=True, default="sensor")  # sensor, manual, calculated
    quality_score = Column(Float, nullable=True, comment="Data quality score 0-1")
    is_anomaly = Column(Boolean, default=False, nullable=False, comment="Anomaly detection flag")
    
    # Metadata
    entry_id = Column(String(100), nullable=True, index=True)  # Original entry ID from your datasets
    notes = Column(Text, nullable=True, comment="Additional notes or observations")
    created_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    pond = relationship("Pond", back_populates="sensor_data")
    
    # Database indexes for performance (critical for time-series queries)
    __table_args__ = (
        Index('idx_pond_timestamp', 'pond_id', 'timestamp'),
        Index('idx_timestamp_desc', 'timestamp', postgresql_using='btree'),
        Index('idx_pond_temp', 'pond_id', 'temperature'),
        Index('idx_pond_ph', 'pond_id', 'ph'),
        Index('idx_pond_do', 'pond_id', 'dissolved_oxygen'),
    )
    
    def __repr__(self):
        return f"<SensorData(pond_id={self.pond_id}, timestamp={self.timestamp}, temp={self.temperature})>"


class SensorDataAggregated(Base):
    """
    Aggregated sensor data for performance
    Stores hourly/daily aggregations to speed up historical queries
    """
    __tablename__ = "sensor_data_aggregated"
    
    id = Column(Integer, primary_key=True, index=True)
    pond_id = Column(Integer, ForeignKey("ponds.id"), nullable=False, index=True)
    
    # Time period
    period_start = Column(DateTime, nullable=False, index=True)
    period_end = Column(DateTime, nullable=False)
    aggregation_type = Column(String(10), nullable=False)  # 'hour', 'day', 'week'
    
    # Aggregated statistics for each parameter
    # Temperature
    temp_avg = Column(Float, nullable=True)
    temp_min = Column(Float, nullable=True)
    temp_max = Column(Float, nullable=True)
    temp_std = Column(Float, nullable=True)
    
    # pH
    ph_avg = Column(Float, nullable=True)
    ph_min = Column(Float, nullable=True)
    ph_max = Column(Float, nullable=True)
    ph_std = Column(Float, nullable=True)
    
    # Dissolved Oxygen
    do_avg = Column(Float, nullable=True)
    do_min = Column(Float, nullable=True)
    do_max = Column(Float, nullable=True)
    do_std = Column(Float, nullable=True)
    
    # Other parameters (similar pattern)
    turbidity_avg = Column(Float, nullable=True)
    ammonia_avg = Column(Float, nullable=True)
    nitrate_avg = Column(Float, nullable=True)
    
    # Data quality metrics
    data_points_count = Column(Integer, nullable=False, default=0)
    quality_score_avg = Column(Float, nullable=True)
    anomaly_count = Column(Integer, nullable=True, default=0)
    
    created_at = Column(DateTime, server_default=func.now())
    
    __table_args__ = (
        Index('idx_pond_period', 'pond_id', 'period_start', 'aggregation_type'),
    )
    
    def __repr__(self):
        return f"<SensorDataAggregated(pond_id={self.pond_id}, period={self.aggregation_type}, start={self.period_start})>"


--- FILE : app/models/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/schemas/alert.py ---


"""
Pydantic schemas for alert system
Handles alert rules, active alerts, and notifications
"""

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime
from enum import Enum


class AlertSeverity(str, Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


class AlertStatus(str, Enum):
    """Alert status"""
    ACTIVE = "active"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"


class AlertRuleBase(BaseModel):
    """Base alert rule schema"""
    pond_id: int = Field(..., gt=0, description="Pond ID")
    parameter: str = Field(..., max_length=50, description="Parameter to monitor")
    rule_name: str = Field(..., max_length=100, description="Rule name")
    description: Optional[str] = Field(None, max_length=500)
    
    # Thresholds
    min_threshold: Optional[float] = Field(None, description="Minimum acceptable value")
    max_threshold: Optional[float] = Field(None, description="Maximum acceptable value")
    
    # Configuration
    severity: AlertSeverity = Field(default=AlertSeverity.WARNING)
    is_active: bool = Field(default=True)
    
    # Notification settings
    send_email: bool = Field(default=True)
    send_sms: bool = Field(default=False)
    send_push: bool = Field(default=True)
    
    # Rate limiting
    cooldown_minutes: int = Field(default=30, ge=1, le=1440)
    max_alerts_per_hour: int = Field(default=4, ge=1, le=100)
    
    # Advanced conditions
    conditions: Optional[Dict[str, Any]] = Field(default={})

    @validator('min_threshold', 'max_threshold')
    def validate_thresholds(cls, v, values):
        """Ensure at least one threshold is set"""
        if v is None and values.get('min_threshold') is None and values.get('max_threshold') is None:
            raise ValueError('At least one threshold (min or max) must be set')
        return v

    @validator('max_threshold')
    def validate_max_greater_than_min(cls, v, values):
        """Ensure max threshold is greater than min threshold"""
        min_val = values.get('min_threshold')
        if v is not None and min_val is not None and v <= min_val:
            raise ValueError('Max threshold must be greater than min threshold')
        return v


class AlertRuleCreate(AlertRuleBase):
    """Schema for creating alert rules"""
    pass


class AlertRuleUpdate(BaseModel):
    """Schema for updating alert rules"""
    rule_name: Optional[str] = Field(None, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    min_threshold: Optional[float] = None
    max_threshold: Optional[float] = None
    severity: Optional[AlertSeverity] = None
    is_active: Optional[bool] = None
    send_email: Optional[bool] = None
    send_sms: Optional[bool] = None
    send_push: Optional[bool] = None
    cooldown_minutes: Optional[int] = Field(None, ge=1, le=1440)
    max_alerts_per_hour: Optional[int] = Field(None, ge=1, le=100)
    conditions: Optional[Dict[str, Any]] = None


class AlertRuleInDB(AlertRuleBase):
    """Alert rule from database"""
    id: int
    created_by: Optional[int]
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True


class AlertRuleResponse(AlertRuleInDB):
    """Alert rule API response"""
    pass


class AlertBase(BaseModel):
    """Base alert schema"""
    pond_id: int = Field(..., gt=0)
    parameter: str = Field(..., max_length=50)
    current_value: float
    threshold_value: Optional[float] = None
    severity: AlertSeverity
    title: str = Field(..., max_length=200)
    message: str = Field(..., max_length=1000)
    message_ar: Optional[str] = Field(None, max_length=1000, description="Arabic message")
    message_fr: Optional[str] = Field(None, max_length=1000, description="French message")
    context_data: Optional[Dict[str, Any]] = Field(default={})
    created_at: datetime = Field(default_factory=datetime.utcnow)


class AlertCreate(AlertBase):
    """Schema for creating alerts"""
    rule_id: Optional[int] = None
    sensor_reading_id: Optional[int] = None


class AlertUpdate(BaseModel):
    """Schema for updating alerts"""
    status: Optional[AlertStatus] = None
    context_data: Optional[Dict[str, Any]] = None


class AlertInDB(AlertBase):
    """Alert from database"""
    id: int
    rule_id: Optional[int]
    status: AlertStatus
    triggered_at: datetime
    acknowledged_at: Optional[datetime]
    resolved_at: Optional[datetime]
    acknowledged_by: Optional[int]
    resolved_by: Optional[int]
    sensor_reading_id: Optional[int]
    notifications_sent: Optional[Dict[str, Any]]
    
    class Config:
        from_attributes = True


class AlertResponse(AlertInDB):
    """Alert API response"""
    pond_name: Optional[str] = None


class AlertQuery(BaseModel):
    """Schema for querying alerts"""
    pond_id: Optional[int] = None
    severity: Optional[AlertSeverity] = None
    status: Optional[AlertStatus] = None
    parameter: Optional[str] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    limit: Optional[int] = Field(default=50, ge=1, le=1000)
    offset: Optional[int] = Field(default=0, ge=0)
    order_by: Optional[str] = Field(default="triggered_at", pattern=r'^(triggered_at|severity|pond_id)$')
    order_direction: Optional[str] = Field(default="desc", pattern=r'^(asc|desc)$')


class AlertAcknowledge(BaseModel):
    """Schema for acknowledging alerts"""
    alert_ids: List[int] = Field(..., min_items=1, max_items=100)
    note: Optional[str] = Field(None, max_length=500)


class AlertResolve(BaseModel):
    """Schema for resolving alerts"""
    alert_ids: List[int] = Field(..., min_items=1, max_items=100)
    resolution_note: Optional[str] = Field(None, max_length=500)


--- FILE : app/schemas/auth.py ---


"""
Authentication schemas
Pydantic models for authentication requests and responses
"""

from typing import Optional
from pydantic import BaseModel, EmailStr, validator
from datetime import datetime


class UserBase(BaseModel):
    """Base user schema"""
    username: str
    email: EmailStr
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    phone_number: Optional[str] = None
    organization: Optional[str] = None
    language: Optional[str] = "fr"


class UserCreate(UserBase):
    """Schema for user registration"""
    password: str
    
    @validator('password')
    def validate_password(cls, v):
        if len(v) < 8:
            raise ValueError('Password must be at least 8 characters long')
        return v
    
    @validator('username')
    def validate_username(cls, v):
        if len(v) < 3:
            raise ValueError('Username must be at least 3 characters long')
        return v


class UserLogin(BaseModel):
    """Schema for user login"""
    username: str  # Can be username or email
    password: str


class UserResponse(UserBase):
    """Schema for user response (without password)"""
    id: int
    is_active: bool
    is_verified: bool
    created_at: datetime
    last_login: Optional[datetime] = None
    
    class Config:
        from_attributes = True


class Token(BaseModel):
    """Schema for authentication token response"""
    access_token: str
    token_type: str
    expires_in: int
    user: UserResponse


class TokenData(BaseModel):
    """Schema for token data"""
    user_id: Optional[int] = None


--- FILE : app/schemas/pond.py ---


"""
Pydantic schemas for pond-related API endpoints
Handles request/response validation and serialization
"""

from pydantic import BaseModel, Field, validator
from typing import Optional, Dict, Any, List
from datetime import datetime
from uuid import UUID
from app.schemas.alert import AlertSeverity
from app.models.pond import UserRole

class PondBase(BaseModel):
    """Base pond schema with common fields"""
    name: str = Field(..., min_length=1, max_length=100, description="Pond name")
    description: Optional[str] = Field(None, max_length=500, description="Pond description")
    capacity: Optional[float] = Field(None, gt=0, description="Capacity in liters")
    depth: Optional[float] = Field(None, gt=0, description="Depth in meters")
    surface_area: Optional[float] = Field(None, gt=0, description="Surface area in square meters")
    location_name: Optional[str] = Field(None, max_length=200)
    latitude: Optional[float] = Field(None, ge=-90, le=90)
    longitude: Optional[float] = Field(None, ge=-180, le=180)
    fish_species: Optional[str] = Field(None, max_length=100)
    fish_count: Optional[int] = Field(None, ge=0)
    stocking_date: Optional[datetime] = None
    aeration_system: bool = False
    filtration_system: bool = False
    heating_system: bool = False
    alert_config: Optional[Dict[str, Any]] = {}
    manager_contact: Optional[str] = Field(None, max_length=100)


class PondCreate(PondBase):
    """Schema for creating a new pond"""
    pass


class PondUpdate(BaseModel):
    """Schema for updating pond information"""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    capacity: Optional[float] = Field(None, gt=0)
    depth: Optional[float] = Field(None, gt=0)
    surface_area: Optional[float] = Field(None, gt=0)
    location_name: Optional[str] = Field(None, max_length=200)
    latitude: Optional[float] = Field(None, ge=-90, le=90)
    longitude: Optional[float] = Field(None, ge=-180, le=180)
    fish_species: Optional[str] = Field(None, max_length=100)
    fish_count: Optional[int] = Field(None, ge=0)
    stocking_date: Optional[datetime] = None
    aeration_system: Optional[bool] = None
    filtration_system: Optional[bool] = None
    heating_system: Optional[bool] = None
    alert_config: Optional[Dict[str, Any]] = None
    manager_contact: Optional[str] = Field(None, max_length=100)
    is_active: Optional[bool] = None


class PondInDB(PondBase):
    """Schema for pond data from database"""
    id: int
    uuid: UUID
    assigned_users: Optional[int]
    is_active: bool
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True


class PondResponse(PondInDB):
    """Schema for pond API responses"""
    pass


class PondSummary(BaseModel):
    """Simplified pond summary for lists"""
    id: int
    name: str
    health_score: Optional[float] = Field(None, ge=0, le=100)
    health_grade: Optional[str] = None
    status: str
    active_alerts_count: int = Field(default=0, ge=0)
    last_updated: datetime
    
    class Config:
        from_attributes = True


class PondWithStats(PondResponse):
    """Pond with additional statistics"""
    latest_reading: Optional[Dict[str, Any]] = None
    health_score: Optional[float] = Field(None, ge=0, le=100)
    health_grade: Optional[str] = None
    active_alerts_count: int = Field(default=0, ge=0)
    last_data_timestamp: Optional[datetime] = None
    push_notifications: bool = True

    class Config:
        from_attributes = True


class UserBase(BaseModel):
    """Base user schema"""
    username: str = Field(..., min_length=3, max_length=50)
    email: str = Field(..., pattern=r'^[^@]+@[^@]+\.[^@]+$')
    first_name: Optional[str] = Field(None, max_length=50)
    last_name: Optional[str] = Field(None, max_length=50)
    phone_number: Optional[str] = Field(None, max_length=20)
    language: str = Field(default="fr", pattern=r'^(fr|ar|en)$')
    timezone: str = Field(default="Africa/Algiers")
    email_notifications: bool = True
    sms_notifications: bool = True
    push_notifications: bool = True


class UserCreate(UserBase):
    """Schema for user registration"""
    password: str = Field(..., min_length=8, max_length=100)
    role: UserRole = Field(default=UserRole.OBSERVER, description="Role of the user")



class UserUpdate(BaseModel):
    """Schema for updating user information"""
    first_name: Optional[str] = Field(None, max_length=50)
    last_name: Optional[str] = Field(None, max_length=50)
    phone_number: Optional[str] = Field(None, max_length=20)
    language: Optional[str] = Field(None, pattern=r'^(fr|ar|en)$')
    timezone: Optional[str] = None
    email_notifications: Optional[bool] = None
    sms_notifications: Optional[bool] = None
    push_notifications: Optional[bool] = None
    role: UserRole


class UserInDB(UserBase):
    """User schema from database"""
    id: int
    uuid: UUID
    is_active: bool
    is_verified: bool
    last_login: Optional[datetime]
    created_at: datetime
    
    class Config:
        from_attributes = True


class UserResponse(UserInDB):
    """User response schema (excludes sensitive data)"""
    assigned_ponds: List[PondSummary] = [] # Show assigned ponds

    pass

class HealthAssessment(BaseModel):
    """Pond health assessment schema (based on your analysis)"""
    pond_id: int
    overall_score: float = Field(..., ge=0, le=100)
    weighted_score: float = Field(..., ge=0, le=100)
    grade: str = Field(..., pattern=r'^(A\+|A|B\+|B|C\+|C|D|F|N/A)$')
    status: str = Field(..., max_length=20)
    
    # Individual parameter scores
    temperature_score: Optional[float] = Field(None, ge=0, le=100)
    ph_score: Optional[float] = Field(None, ge=0, le=100)
    dissolved_oxygen_score: Optional[float] = Field(None, ge=0, le=100)
    turbidity_score: Optional[float] = Field(None, ge=0, le=100)
    ammonia_score: Optional[float] = Field(None, ge=0, le=100)
    nitrate_score: Optional[float] = Field(None, ge=0, le=100)
    
    # Risk and recommendations
    risk_level: str = Field(..., pattern=r'^(Low|Medium|High)$')
    warning_count: int = Field(default=0, ge=0)
    critical_issues: List[str] = Field(default=[])
    recommendations: List[str] = Field(default=[])
    action_priority: Optional[str] = Field(None, pattern=r'^(Maintain|Monitor|Improve|Urgent)$')
    
    # Assessment metadata
    parameters_assessed: int = Field(..., ge=0)
    data_completeness: Optional[float] = Field(None, ge=0, le=100)
    assessment_confidence: Optional[float] = Field(None, ge=0, le=1)
    assessment_period_start: datetime
    assessment_period_end: datetime
    calculated_at: datetime


class HealthAssessmentCreate(BaseModel):
    """Schema for creating health assessments"""
    pond_id: int
    assessment_period_start: datetime
    assessment_period_end: datetime


class NotificationPreferences(BaseModel):
    """User notification preferences"""
    email_enabled: bool = True
    sms_enabled: bool = True
    push_enabled: bool = True
    
    # Severity preferences
    email_min_severity: AlertSeverity = AlertSeverity.WARNING
    sms_min_severity: AlertSeverity = AlertSeverity.CRITICAL
    push_min_severity: AlertSeverity = AlertSeverity.INFO
    
    # Timing preferences
    quiet_hours_start: Optional[int] = Field(None, ge=0, le=23, description="Quiet hours start (24h format)")
    quiet_hours_end: Optional[int] = Field(None, ge=0, le=23, description="Quiet hours end (24h format)")
    weekend_notifications: bool = True
    
    # Language preference
    language: str = Field(default="fr", pattern=r'^(fr|ar|en)$')


class DashboardSummary(BaseModel):
    """Dashboard summary data"""
    total_ponds: int
    active_ponds: int
    total_alerts: int
    critical_alerts: int
    warning_alerts: int
    
    # Health distribution
    excellent_ponds: int  # A+ and A grades
    good_ponds: int       # B+ and B grades
    fair_ponds: int       # C+ and C grades
    poor_ponds: int       # D and F grades
    
    # Recent activity
    recent_readings_count: int
    last_reading_timestamp: Optional[datetime]
    
    # System health
    data_quality_avg: Optional[float]
    connectivity_status: str = Field(default="online", pattern=r'^(online|offline|degraded)$')


--- FILE : app/schemas/sensor.py ---


"""
Pydantic schemas for sensor data endpoints
Handles validation for water quality measurements based on your pond analysis
"""

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
from enum import Enum


class SensorDataBase(BaseModel):
    """Base sensor data schema with all water quality parameters"""
    pond_id: int = Field(..., gt=0, description="Pond ID")
    
    # Core water quality parameters (from your analysis)
    temperature: Optional[float] = Field(None, ge=-10, le=50, description="Temperature in Celsius")
    ph: Optional[float] = Field(None, ge=0, le=14, description="pH level")
    dissolved_oxygen: Optional[float] = Field(None, ge=0, le=25, description="Dissolved oxygen in mg/L")
    turbidity: Optional[float] = Field(None, ge=0, description="Turbidity in NTU")
    ammonia: Optional[float] = Field(None, ge=0, description="Ammonia in mg/L")
    nitrate: Optional[float] = Field(None, ge=0, description="Nitrate in mg/L")
    nitrite: Optional[float] = Field(None, ge=0, description="Nitrite in mg/L")
    salinity: Optional[float] = Field(None, ge=0, description="Salinity in ppt")
    
    # Fish measurements
    fish_count: Optional[int] = Field(None, ge=0, description="Number of fish")
    fish_length: Optional[float] = Field(None, ge=0, description="Average fish length in cm")
    fish_weight: Optional[float] = Field(None, ge=0, description="Average fish weight in grams")
    
    # Additional measurements
    water_level: Optional[float] = Field(None, ge=0, description="Water level in cm")
    flow_rate: Optional[float] = Field(None, ge=0, description="Flow rate in L/min")
    
    # Metadata
    data_source: Optional[str] = Field(default="sensor", max_length=50)
    notes: Optional[str] = Field(None, max_length=500)
    timestamp: Optional[datetime] = Field(None, description="Measurement timestamp")


class SensorDataCreate(SensorDataBase):
    """Schema for creating new sensor data"""
    
    @validator('timestamp', pre=True, always=True)
    def validate_timestamp(cls, v):
        """Validate and set default timestamp with proper timezone handling"""
        if v is None:
            # Use UTC timezone-aware datetime as default
            return datetime.now(timezone.utc)
        
        # Handle string timestamps
        if isinstance(v, str):
            try:
                # Try parsing ISO format first
                v = datetime.fromisoformat(v.replace('Z', '+00:00'))
            except ValueError:
                # Try other common formats
                try:
                    v = datetime.strptime(v, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    raise ValueError('Invalid timestamp format. Use ISO format or YYYY-MM-DD HH:MM:SS')
        
        # Handle datetime objects
        if isinstance(v, datetime):
            # If no timezone info, assume UTC
            if v.tzinfo is None:
                v = v.replace(tzinfo=timezone.utc)
            
            # Compare with timezone-aware datetime
            now_utc = datetime.now(timezone.utc)
            if v > now_utc:
                raise ValueError('Timestamp cannot be in the future')
        return v
    
    @validator('temperature')
    def validate_temperature(cls, v):
        """Validate temperature range for aquaculture"""
        if v is not None:
            if v < -10 or v > 45:
                raise ValueError('Temperature should be between -10°C and 45°C for aquaculture, probable sensor failure.')
            # Warning for extreme values
            if v < 10 or v > 35:
                # Log warning but don't raise error
                pass
        return v
    
    @validator('ph')
    def validate_ph(cls, v):
        """Validate pH range for aquaculture"""
        if v is not None:
            if v < 4.0 or v > 10.0:
                raise ValueError('pH should be between 4.0 and 10.0 for aquaculture systems')
            # Optimal range warning
            if v < 6.5 or v > 8.5:
                # Log warning for suboptimal pH
                pass
        return v
    
    @validator('dissolved_oxygen')
    def validate_dissolved_oxygen(cls, v):
        """Validate dissolved oxygen levels"""
        if v is not None:
            if v < 0:
                raise ValueError('Dissolved oxygen cannot be negative')
            if v > 20:  # Very high DO might indicate measurement error
                raise ValueError('Dissolved oxygen level seems unusually high (>20 mg/L)')
        return v
    
    @validator('ammonia')
    def validate_ammonia(cls, v):
        """Validate ammonia levels"""
        if v is not None:
            if v < 0:
                raise ValueError('Ammonia level cannot be negative')
            if v > 10:  # High ammonia is dangerous
                # This is a critical level but might be valid in some conditions
                pass
        return v
    
    @validator('turbidity')
    def validate_turbidity(cls, v):
        """Validate turbidity levels"""
        if v is not None:
            if v < 0:
                raise ValueError('Turbidity cannot be negative')
        return v
    
    @validator('fish_count')
    def validate_fish_count(cls, v):
        """Validate fish count"""
        if v is not None:
            if v < 0:
                raise ValueError('Fish count cannot be negative')
            if v > 100000:  # Sanity check
                raise ValueError('Fish count seems unusually high')
        return v


class SensorDataBulkCreate(BaseModel):
    """Schema for bulk sensor data creation"""
    readings: List[SensorDataCreate] = Field(..., min_items=1, max_items=1000)
    
    @validator('readings')
    def validate_batch_consistency(cls, v):
        """Validate batch readings consistency"""
        if len(v) > 1000:
            raise ValueError('Batch size cannot exceed 1000 readings')
        
        # Check for duplicate timestamps per pond
        pond_timestamps = {}
        for reading in v:
            pond_id = reading.pond_id
            timestamp = reading.timestamp
            
            if pond_id not in pond_timestamps:
                pond_timestamps[pond_id] = set()
            
            if timestamp in pond_timestamps[pond_id]:
                raise ValueError(f'Duplicate timestamp {timestamp} for pond {pond_id}')
            
            pond_timestamps[pond_id].add(timestamp)
        
        return v


class SensorDataUpdate(BaseModel):
    """Schema for updating sensor data (limited fields)"""
    temperature: Optional[float] = Field(None, ge=-10, le=50)
    ph: Optional[float] = Field(None, ge=0, le=14)
    dissolved_oxygen: Optional[float] = Field(None, ge=0, le=25)
    turbidity: Optional[float] = Field(None, ge=0)
    ammonia: Optional[float] = Field(None, ge=0)
    nitrate: Optional[float] = Field(None, ge=0)
    nitrite: Optional[float] = Field(None, ge=0)
    salinity: Optional[float] = Field(None, ge=0)
    fish_count: Optional[int] = Field(None, ge=0)
    fish_length: Optional[float] = Field(None, ge=0)
    fish_weight: Optional[float] = Field(None, ge=0)
    water_level: Optional[float] = Field(None, ge=0)
    flow_rate: Optional[float] = Field(None, ge=0)
    notes: Optional[str] = Field(None, max_length=500)
    data_source: Optional[str] = Field(None, max_length=50)


class SensorDataInDB(SensorDataBase):
    """Sensor data from database"""
    id: int
    timestamp: datetime  # Override to make it required from DB
    quality_score: Optional[float] = Field(None, ge=0, le=100)
    is_anomaly: Optional[bool] = Field(None, description="Whether this reading is anomalous")
    entry_id: Optional[str] = Field(None, description="Unique entry identifier")
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True


class SensorDataResponse(SensorDataInDB):
    """Sensor data API response"""
    
    # Add computed fields
    health_indicators: Optional[Dict[str, Any]] = Field(None, description="Health assessment indicators")
    alerts_triggered: Optional[List[str]] = Field(None, description="List of alerts this reading triggered")
    
    class Config:
        from_attributes = True


class SensorDataQuery(BaseModel):
    """Schema for querying sensor data"""
    pond_id: Optional[int] = Field(None, gt=0)
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    parameters: Optional[List[str]] = Field(
        None, 
        description="Specific parameters to retrieve",
        example=["temperature", "ph", "dissolved_oxygen"]
    )
    limit: Optional[int] = Field(default=100, ge=1, le=10000)
    offset: Optional[int] = Field(default=0, ge=0)
    include_anomalies: Optional[bool] = Field(default=True)
    order_by: Optional[str] = Field(
        default="timestamp", 
        pattern=r'^(timestamp|pond_id|temperature|ph|dissolved_oxygen)$'
    )
    order_direction: Optional[str] = Field(default="desc", pattern=r'^(asc|desc)$')
    
    @validator('end_date')
    def validate_date_range(cls, v, values):
        """Validate that end_date is after start_date"""
        if v and 'start_date' in values and values['start_date']:
            if v <= values['start_date']:
                raise ValueError('end_date must be after start_date')
        return v
    
    @validator('parameters')
    def validate_parameters(cls, v):
        """Validate parameter names"""
        if v:
            valid_params = {
                'temperature', 'ph', 'dissolved_oxygen', 'turbidity', 
                'ammonia', 'nitrate', 'nitrite', 'salinity', 'fish_count',
                'fish_length', 'fish_weight', 'water_level', 'flow_rate'
            }
            invalid_params = set(v) - valid_params
            if invalid_params:
                raise ValueError(f'Invalid parameters: {", ".join(invalid_params)}')
        return v


class AggregationType(str, Enum):
    """Data aggregation types"""
    HOUR = "hour"
    DAY = "day"
    WEEK = "week"
    MONTH = "month"


class SensorDataAggregated(BaseModel):
    """Aggregated sensor data response"""
    pond_id: int
    period_start: datetime
    period_end: datetime
    aggregation_type: AggregationType
    
    # Temperature statistics
    temp_avg: Optional[float] = Field(None, description="Average temperature")
    temp_min: Optional[float] = Field(None, description="Minimum temperature")
    temp_max: Optional[float] = Field(None, description="Maximum temperature")
    temp_std: Optional[float] = Field(None, description="Temperature standard deviation")
    
    # pH statistics
    ph_avg: Optional[float] = Field(None, description="Average pH")
    ph_min: Optional[float] = Field(None, description="Minimum pH")
    ph_max: Optional[float] = Field(None, description="Maximum pH")
    ph_std: Optional[float] = Field(None, description="pH standard deviation")
    
    # Dissolved oxygen statistics
    do_avg: Optional[float] = Field(None, description="Average dissolved oxygen")
    do_min: Optional[float] = Field(None, description="Minimum dissolved oxygen")
    do_max: Optional[float] = Field(None, description="Maximum dissolved oxygen")
    do_std: Optional[float] = Field(None, description="Dissolved oxygen standard deviation")
    
    # Other parameters
    turbidity_avg: Optional[float] = Field(None, description="Average turbidity")
    ammonia_avg: Optional[float] = Field(None, description="Average ammonia")
    nitrate_avg: Optional[float] = Field(None, description="Average nitrate")
    
    # Data quality metrics
    data_points_count: int = Field(..., ge=0, description="Number of data points in aggregation")
    quality_score_avg: Optional[float] = Field(None, ge=0, le=100, description="Average quality score")
    anomaly_count: Optional[int] = Field(None, ge=0, description="Number of anomalous readings")
    completeness_score: Optional[float] = Field(None, ge=0, le=100, description="Data completeness percentage")
    
    class Config:
        from_attributes = True


class ParameterStatistics(BaseModel):
    """Statistics for a single parameter"""
    parameter: str = Field(..., description="Parameter name")
    count: int = Field(..., ge=0, description="Number of readings")
    mean: Optional[float] = Field(None, description="Mean value")
    median: Optional[float] = Field(None, description="Median value")
    std: Optional[float] = Field(None, ge=0, description="Standard deviation")
    min: Optional[float] = Field(None, description="Minimum value")
    max: Optional[float] = Field(None, description="Maximum value")
    q25: Optional[float] = Field(None, description="25th percentile")
    q75: Optional[float] = Field(None, description="75th percentile")
    latest_value: Optional[float] = Field(None, description="Latest recorded value")
    latest_timestamp: Optional[datetime] = Field(None, description="Timestamp of latest reading")
    trend: Optional[str] = Field(None, description="Trend direction: increasing, decreasing, stable")
    
    class Config:
        from_attributes = True


class PondDataSummary(BaseModel):
    """Summary of pond sensor data"""
    pond_id: int = Field(..., gt=0)
    pond_name: str = Field(..., description="Name of the pond")
    total_readings: int = Field(..., ge=0, description="Total number of readings")
    date_range_start: Optional[datetime] = Field(None, description="First reading timestamp")
    date_range_end: Optional[datetime] = Field(None, description="Last reading timestamp")
    parameters: List[ParameterStatistics] = Field(..., description="Statistics for each parameter")
    last_reading_timestamp: Optional[datetime] = Field(None, description="Most recent reading timestamp")
    data_quality_score: Optional[float] = Field(None, ge=0, le=100, description="Overall data quality score")
    health_score: Optional[float] = Field(None, ge=0, le=100, description="Current pond health score")
    active_alerts: Optional[int] = Field(None, ge=0, description="Number of active alerts")
    
    class Config:
        from_attributes = True


class SensorCalibration(BaseModel):
    """Schema for sensor calibration data"""
    sensor_id: str = Field(..., description="Sensor identifier")
    parameter: str = Field(..., description="Parameter being calibrated")
    calibration_date: datetime = Field(..., description="Calibration timestamp")
    calibration_factor: float = Field(..., description="Calibration multiplier")
    offset: float = Field(default=0.0, description="Calibration offset")
    reference_value: Optional[float] = Field(None, description="Reference standard value")
    measured_value: Optional[float] = Field(None, description="Measured value before calibration")
    notes: Optional[str] = Field(None, max_length=500)
    
    class Config:
        from_attributes = True


class DataQualityReport(BaseModel):
    """Data quality assessment report"""
    pond_id: int
    assessment_period: Dict[str, datetime] = Field(..., description="Start and end dates")
    overall_score: float = Field(..., ge=0, le=100, description="Overall quality score")
    
    # Quality metrics
    completeness_score: float = Field(..., ge=0, le=100)
    accuracy_score: Optional[float] = Field(None, ge=0, le=100)
    consistency_score: float = Field(..., ge=0, le=100)
    timeliness_score: float = Field(..., ge=0, le=100)
    
    # Issues found
    missing_data_periods: List[Dict[str, datetime]] = Field(default_factory=list)
    anomalous_readings: int = Field(default=0, ge=0)
    calibration_issues: List[str] = Field(default_factory=list)
    
    # Recommendations
    recommendations: List[str] = Field(default_factory=list)
    
    class Config:
        from_attributes = True


--- FILE : app/schemas/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/services/alert_service.py ---


"""
Enhanced Alert Service with Email Notifications
"""

from typing import Optional, List, Dict
from sqlalchemy.orm import Session
from datetime import datetime, timezone
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import asyncio
from jinja2 import Template

from app.models.alert import Alert, AlertStatus, AlertSeverity
from app.models.pond import Pond, User
from app.config import settings


class EmailService:
    """Email notification service for alerts"""
    
    def __init__(self):
        self.smtp_server = settings.SMTP_SERVER
        self.smtp_port = settings.SMTP_PORT
        self.smtp_username = settings.SMTP_USERNAME
        self.smtp_password = settings.SMTP_PASSWORD
        self.from_email = settings.FROM_EMAIL
        self.enabled = settings.ENABLE_EMAIL_ALERTS
        
        # Check credentials but don't fail initialization
        if not self.smtp_username or not self.smtp_password:
            print("‚ö†Ô∏è  Warning: SMTP credentials not configured. Email alerts will be disabled.")
            self.enabled = False
        
    async def send_anomaly_alert_email(self, alert: Alert, pond: Pond, user: User) -> bool:
        """Send anomaly alert email to pond owner"""
        if not self.enabled:
            print("üìß Email alerts are disabled in configuration")
            return False
            
        if not self.smtp_username or not self.smtp_password:
            print("‚ùå Email credentials not configured")
            return False
            
        try:
            # Determine user's preferred language
            user_language = getattr(user, 'language', 'fr')
            
            # Get alert message in user's language
            if user_language == 'ar':
                alert_message = getattr(alert, 'message_ar', alert.message_fr)
                subject = f"ÿ™ŸÜÿ®ŸäŸá ÿ¥ÿ∞Ÿàÿ∞ - ÿ≠Ÿàÿ∂ {pond.name}"
            else:  # Default to French
                alert_message = alert.message_fr
                subject = f"Alerte Anomalie - Bassin {pond.name}"
            
            # Create email content
            email_content = self._create_email_content(alert, pond, user, user_language)
            
            # Send email
            return await self._send_email(
                to_email=user.email,
                subject=subject,
                content=email_content
            )
            
        except Exception as e:
            print(f"‚ùå Error sending anomaly alert email: {e}")
            return False
    
    def _create_email_content(self, alert: Alert, pond: Pond, user: User, language: str) -> str:
        """Create HTML email content"""
        
        # Simple email template
        template_content = """
        <html>
        <head>
            <style>
                .container { font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; }
                .header { background-color: #ff6b6b; color: white; padding: 20px; text-align: center; }
                .content { padding: 20px; }
                .alert-box { background-color: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; margin: 10px 0; border-radius: 5px; }
                .critical { background-color: #f8d7da; border-color: #f5c6cb; }
                .parameter { margin: 5px 0; padding: 8px; background-color: #f8f9fa; border-radius: 3px; }
                .footer { background-color: #f8f9fa; padding: 15px; text-align: center; font-size: 12px; }
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h2>üö® Alerte Anomalie D√©tect√©e</h2>
                </div>
                <div class="content">
                    <p>Bonjour {{ user_name }},</p>
                    <p>Une anomalie a √©t√© d√©tect√©e dans votre bassin <strong>{{ pond_name }}</strong>.</p>
                    
                    <div class="alert-box {{ severity_class }}">
                        <h3>D√©tails de l'Anomalie</h3>
                        <p><strong>Message:</strong> {{ alert_message }}</p>
                        <p><strong>S√©v√©rit√©:</strong> {{ severity }}</p>
                        <p><strong>Heure:</strong> {{ timestamp }}</p>
                        <p><strong>Score:</strong> {{ anomaly_score }}/1.0</p>
                    </div>
                    
                    <p><strong>Action recommand√©e:</strong> V√©rifiez imm√©diatement les conditions de votre bassin.</p>
                    
                    <p>Cordialement,<br>Syst√®me de Surveillance Aquaculture</p>
                </div>
                <div class="footer">
                    <p>Email automatique - Ne pas r√©pondre</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Template variables
        template_vars = {
            'user_name': f"{user.first_name} {user.last_name}",
            'pond_name': pond.name,
            'alert_message': alert.message_fr,
            'severity': self._get_severity_text(alert.severity, language),
            'severity_class': alert.severity.value.lower(),
            'timestamp': alert.triggered_at.strftime('%Y-%m-%d %H:%M:%S UTC'),
            'anomaly_score': alert.context_data.get('anomaly_score', 0) if alert.current_value else 0
        }
        
        # Render template
        template = Template(template_content)
        return template.render(**template_vars)
    
    def _get_severity_text(self, severity: AlertSeverity, language: str) -> str:
        """Get severity text in specified language"""
        severity_texts = {
            AlertSeverity.INFO: 'Information',
            AlertSeverity.WARNING: 'Avertissement',
            AlertSeverity.CRITICAL: 'Critique'
        }
        return severity_texts.get(severity, str(severity.value))
    
    async def _send_email(self, to_email: str, subject: str, content: str) -> bool:
        """Send email using SMTP"""
        try:
            # Create message
            msg = MIMEMultipart('alternative')
            msg['Subject'] = subject
            msg['From'] = self.from_email
            msg['To'] = to_email
            
            # Add HTML content
            html_part = MIMEText(content, 'html', 'utf-8')
            msg.attach(html_part)
            
            # Send email
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls()
                if self.smtp_username and self.smtp_password:
                    server.login(self.smtp_username, self.smtp_password)
                server.send_message(msg)
            
            print(f"‚úÖ Anomaly alert email sent successfully to {to_email}")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to send email to {to_email}: {e}")
            return False


# Global email service instance
email_service = EmailService()


async def send_anomaly_alert_notification(alert: Alert, db: Session) -> bool:
    """Send anomaly alert notification via email"""
    try:
        # Get pond and user information
        pond = db.query(Pond).filter(Pond.id == alert.pond_id).first()
        if not pond:
            print(f"Pond not found for alert {alert.id}")
            return False
        
        user = db.query(User).filter(User.id == pond.owner_id).first()
        if not user:
            print(f"User not found for pond {pond.id}")
            return False
        
        # Check if user wants email notifications
        if not getattr(user, 'email_notifications', True):
            print(f"Email notifications disabled for user {user.id}")
            return False
        
        # Send email
        return await email_service.send_anomaly_alert_email(alert, pond, user)
        
    except Exception as e:
        print(f"Error sending anomaly alert notification: {e}")
        return False


--- FILE : app/services/data_processor.py ---


"""
Data Processing Service
Handles data validation, anomaly detection, and aggregation
"""

import statistics
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
from sqlalchemy import and_, func, desc
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

from app.models.sensor import SensorData, SensorDataAggregated
from app.models.pond import Pond
from app.schemas.sensor import SensorDataCreate
from app.config import settings

from typing import Optional, List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc
from datetime import datetime, timedelta, timezone
import asyncio

from app.models.alert import Alert, AlertType, AlertSeverity, AlertStatus
from app.database import SessionLocal

from app.services.page_hinkley import detect_anomalies_page_hinkley, get_page_hinkley_diagnostics



def validate_sensor_data(sensor_data: SensorDataCreate) -> float:
    """
    Validate sensor data quality and return quality score (0-1)
    """
    quality_score = 1.0
    
    # Check for missing critical parameters
    critical_params = ['temperature', 'ph', 'dissolved_oxygen']
    missing_critical = sum(1 for param in critical_params if getattr(sensor_data, param) is None)
    quality_score -= (missing_critical / len(critical_params)) * 0.3
    
    # Check for unrealistic values
    unrealistic_penalty = 0
    
    # Temperature validation
    if sensor_data.temperature is not None:
        if sensor_data.temperature < -5 or sensor_data.temperature > 50:
            unrealistic_penalty += 0.2
    
    # pH validation
    if sensor_data.ph is not None:
        if sensor_data.ph < 0 or sensor_data.ph > 14:
            unrealistic_penalty += 0.2
    
    # Dissolved oxygen validation
    if sensor_data.dissolved_oxygen is not None:
        if sensor_data.dissolved_oxygen < 0 or sensor_data.dissolved_oxygen > 30:
            unrealistic_penalty += 0.2
    
    quality_score -= unrealistic_penalty
    
    # Check timestamp validity - FIXED: Use timezone-aware comparison
    if sensor_data.timestamp:
        current_time = datetime.now(timezone.utc)
        # Ensure both datetimes are timezone-aware
        sensor_timestamp = sensor_data.timestamp
        if sensor_timestamp.tzinfo is None:
            sensor_timestamp = sensor_timestamp.replace(tzinfo=timezone.utc)
        
        if sensor_timestamp > current_time:
            quality_score -= 0.1
    
    # Check for data source
    if sensor_data.data_source and sensor_data.data_source != 'sensor':
        quality_score -= 0.1  # Manual data might be less accurate
    
    return max(0.0, min(1.0, quality_score))


async def detect_anomalies(sensor_data: SensorDataCreate, db: Session) -> bool:
    """
    Detect anomalies using Page-Hinkley change point detection
    """
    return await detect_anomalies_page_hinkley(sensor_data, db)


async def get_pond_latest_data(pond_id: int, db: Session) -> Optional[Dict[str, Any]]:
    """
    Get the latest sensor data for a pond
    """
    latest_data = db.query(SensorData).filter(
        SensorData.pond_id == pond_id
    ).order_by(desc(SensorData.timestamp)).first()
    
    if not latest_data:
        return None
    
    return {
        'timestamp': latest_data.timestamp,
        'temperature': latest_data.temperature,
        'ph': latest_data.ph,
        'dissolved_oxygen': latest_data.dissolved_oxygen,
        'turbidity': latest_data.turbidity,
        'ammonia': latest_data.ammonia,
        'nitrate': latest_data.nitrate,
        'salinity': latest_data.salinity,
        'water_level': latest_data.water_level,
        'fish_count': latest_data.fish_count,
        'data_source': latest_data.data_source,
        'quality_score': getattr(latest_data, 'quality_score', None)
    }


async def get_pond_statistics(pond_id: int, db: Session, days: int = 30) -> Dict[str, Any]:
    """Get comprehensive pond statistics"""
    
    start_date = datetime.now(timezone.utc) - timedelta(days=days)
    
    # Get sensor data for the period
    sensor_data = db.query(SensorData).filter(
        and_(
            SensorData.pond_id == pond_id,
            SensorData.timestamp >= start_date
        )
    ).order_by(SensorData.timestamp.asc()).all()
    
    if not sensor_data:
        return {
            "message": "No data available for the specified period",
            "period_days": days,
            "total_readings": 0,
            "date_range": {
                "start": start_date.isoformat(),
                "end": datetime.now(timezone.utc).isoformat()
            }
        }
    
    # Calculate statistics
    stats = {
        "period_days": days,
        "total_readings": len(sensor_data),
        "date_range": {
            "start": start_date.isoformat(),
            "end": datetime.now(timezone.utc).isoformat()
        },
        "parameters": {},
        "data_quality": {
            "completeness": 0,
            "missing_readings": 0
        }
    }
    
    # Calculate parameter statistics
    parameters = ['temperature', 'ph', 'dissolved_oxygen', 'turbidity', 'ammonia', 'nitrate']
    
    for param in parameters:
        values = [getattr(reading, param) for reading in sensor_data if getattr(reading, param) is not None]
        
        if values:
            param_stats = {
                "count": len(values),
                "min": min(values),
                "max": max(values),
                "average": round(sum(values) / len(values), 2),
                "median": round(statistics.median(values), 2),
                "std_dev": round(statistics.stdev(values) if len(values) > 1 else 0, 2),
                "latest": values[-1] if values else None,
                "first": values[0] if values else None
            }
            
            # Calculate trend (simple linear trend)
            if len(values) > 1:
                x_values = list(range(len(values)))
                try:
                    # Simple linear regression slope
                    n = len(values)
                    sum_x = sum(x_values)
                    sum_y = sum(values)
                    sum_xy = sum(x * y for x, y in zip(x_values, values))
                    sum_x2 = sum(x * x for x in x_values)
                    
                    denominator = n * sum_x2 - sum_x * sum_x
                    if denominator != 0:
                        slope = (n * sum_xy - sum_x * sum_y) / denominator
                        param_stats["trend_slope"] = round(slope, 4)
                        
                        if slope > 0.01:
                            param_stats["trend"] = "increasing"
                        elif slope < -0.01:
                            param_stats["trend"] = "decreasing"
                        else:
                            param_stats["trend"] = "stable"
                    else:
                        param_stats["trend"] = "stable"
                        param_stats["trend_slope"] = 0
                except (ZeroDivisionError, ValueError):
                    param_stats["trend"] = "stable"
                    param_stats["trend_slope"] = 0
            else:
                param_stats["trend"] = "insufficient_data"
                param_stats["trend_slope"] = 0
            
            stats["parameters"][param] = param_stats
        else:
            # No data for this parameter
            stats["parameters"][param] = {
                "count": 0,
                "message": "No data available for this parameter"
            }
    
    # Calculate data quality metrics
    expected_readings = days * 24  # Assuming hourly readings
    if expected_readings > 0:
        stats["data_quality"]["completeness"] = round((len(sensor_data) / expected_readings) * 100, 1)
        stats["data_quality"]["missing_readings"] = expected_readings - len(sensor_data)
    else:
        stats["data_quality"]["completeness"] = 0
        stats["data_quality"]["missing_readings"] = 0
    
    return stats


def _calculate_trend(data: np.ndarray) -> str:
    """
    Calculate trend direction for a parameter
    """
    if len(data) < 3:
        return 'insufficient_data'
    
    try:
        # Use linear regression to determine trend
        x = np.arange(len(data))
        slope, _, r_value, p_value, _ = stats.linregress(x, data)
        
        # Consider trend significant if p-value < 0.05 and R² > 0.1
        if p_value < 0.05 and r_value**2 > 0.1:
            if slope > 0:
                return 'increasing'
            else:
                return 'decreasing'
        else:
            return 'stable'
    except Exception:
        return 'stable'



async def process_sensor_data_batch(
    sensor_data_list: List[SensorDataCreate], 
    db: Session
) -> Dict[str, Any]:
    """
    Process a batch of sensor data entries
    """
    results = {
        "processed": 0,
        "errors": [],
        "quality_scores": [],
        "anomalies": 0
    }
    
    for i, sensor_data in enumerate(sensor_data_list):
        try:
            # Validate data quality
            quality_score = validate_sensor_data(sensor_data)
            results["quality_scores"].append(quality_score)
            
            # Detect anomalies
            is_anomaly = await detect_anomalies(sensor_data, db)
            if is_anomaly:
                results["anomalies"] += 1
            
            results["processed"] += 1
            
        except Exception as e:
            results["errors"].append(f"Error processing entry {i}: {str(e)}")
    
    return results

async def process_sensor_alerts(pond_id: int, sensor_reading_id: Optional[int] = None):
    """
    Process alerts for sensor data
    This runs in the background after sensor data is saved
    """
    try:
        # Create a new database session for background processing
        db = SessionLocal()
        
        try:
            # Get the pond
            pond = db.query(Pond).filter(Pond.id == pond_id).first()
            if not pond:
                return
            
            # Get recent sensor data (last reading or specific one)
            if sensor_reading_id:
                sensor_data = db.query(SensorData).filter(
                    SensorData.id == sensor_reading_id
                ).first()
                if sensor_data:
                    await _check_sensor_alerts(sensor_data, db)
            else:
                # Process recent data for the pond
                recent_data = db.query(SensorData).filter(
                    SensorData.pond_id == pond_id
                ).order_by(desc(SensorData.timestamp)).limit(5).all()
                
                for data in recent_data:
                    await _check_sensor_alerts(data, db)
            
            db.commit()
            
        finally:
            db.close()
            
    except Exception as e:
        print(f"Error in alert processing: {e}")


async def _check_sensor_alerts(sensor_data: SensorData, db: Session):
    """
    Check individual sensor data for alert conditions
    """
    alerts_to_create = []
    
    # Temperature alerts
    if sensor_data.temperature is not None:
        if sensor_data.temperature > 35:
            alerts_to_create.append({
                'type': AlertType.HIGH_TEMPERATURE,
                'severity': AlertSeverity.CRITICAL if sensor_data.temperature > 40 else AlertSeverity.WARNING,
                'parameter': 'temperature',
                'title': 'High Temperature Alert',
                'message': f"High temperature detected: {sensor_data.temperature}°C",
                'value': sensor_data.temperature,
                'threshold': 35
            })
        elif sensor_data.temperature < 15:
            alerts_to_create.append({
                'type': AlertType.LOW_TEMPERATURE,
                'severity': AlertSeverity.CRITICAL if sensor_data.temperature < 10 else AlertSeverity.WARNING,
                'parameter': 'temperature',
                'title': 'Low Temperature Alert',
                'message': f"Low temperature detected: {sensor_data.temperature}°C",
                'value': sensor_data.temperature,
                'threshold': 15
            })
    
    # pH alerts
    if sensor_data.ph is not None:
        if sensor_data.ph > 8.5:
            alerts_to_create.append({
                'type': AlertType.HIGH_PH,
                'severity': AlertSeverity.CRITICAL if sensor_data.ph > 9.0 else AlertSeverity.WARNING,
                'parameter': 'ph',
                'title': 'High pH Alert',
                'message': f"High pH detected: {sensor_data.ph}",
                'value': sensor_data.ph,
                'threshold': 8.5
            })
        elif sensor_data.ph < 6.5:
            alerts_to_create.append({
                'type': AlertType.LOW_PH,
                'severity': AlertSeverity.CRITICAL if sensor_data.ph < 6.0 else AlertSeverity.WARNING,
                'parameter': 'ph',
                'title': 'Low pH Alert',
                'message': f"Low pH detected: {sensor_data.ph}",
                'value': sensor_data.ph,
                'threshold': 6.5
            })
    
    # Dissolved Oxygen alerts
    if sensor_data.dissolved_oxygen is not None:
        if sensor_data.dissolved_oxygen < 4.0:
            alerts_to_create.append({
                'type': AlertType.LOW_OXYGEN,
                'severity': AlertSeverity.CRITICAL if sensor_data.dissolved_oxygen < 2.0 else AlertSeverity.WARNING,
                'parameter': 'dissolved_oxygen',
                'title': 'Low Oxygen Alert',
                'message': f"Low dissolved oxygen: {sensor_data.dissolved_oxygen} mg/L",
                'value': sensor_data.dissolved_oxygen,
                'threshold': 4.0
            })
    
    # Ammonia alerts
    if sensor_data.ammonia is not None and sensor_data.ammonia > 0.5:
        alerts_to_create.append({
            'type': AlertType.HIGH_AMMONIA,
            'severity': AlertSeverity.CRITICAL if sensor_data.ammonia > 2.0 else AlertSeverity.WARNING,
            'parameter': 'ammonia',
            'title': 'High Ammonia Alert',
            'message': f"High ammonia detected: {sensor_data.ammonia} mg/L",
            'value': sensor_data.ammonia,
            'threshold': 0.5
        })
    
    # Create alerts in database
    for alert_data in alerts_to_create:
        # Check if similar alert already exists recently
        existing_alert = db.query(Alert).filter(
            and_(
                Alert.pond_id == sensor_data.pond_id,
                Alert.alert_type == alert_data['type'],
                Alert.parameter == alert_data['parameter'],
                Alert.status == AlertStatus.ACTIVE,
                Alert.triggered_at >= datetime.now(timezone.utc) - timedelta(hours=1)
            )
        ).first()
        
        if not existing_alert:
            # Create multilingual messages
            message_fr = alert_data['message']
            message_ar = _translate_to_arabic(alert_data['message'], alert_data['parameter'])
            
            alert = Alert(
                pond_id=sensor_data.pond_id,
                sensor_reading_id=sensor_data.id,
                alert_type=alert_data['type'],
                severity=alert_data['severity'],
                status=AlertStatus.ACTIVE,
                parameter=alert_data['parameter'],
                current_value=alert_data['value'],
                threshold_value=alert_data['threshold'],
                title=alert_data['title'],
                message=alert_data['message'],  # Default message
                message_fr=message_fr,
                message_ar=message_ar,
                triggered_at=datetime.now(timezone.utc),
                context_data={
                    'sensor_data_id': sensor_data.id,
                    'pond_id': sensor_data.pond_id,
                    'detection_time': datetime.now(timezone.utc).isoformat(),
                    'data_source': getattr(sensor_data, 'data_source', 'unknown')
                },
                notifications_sent={}
            )
            db.add(alert)

def _translate_to_arabic(message: str, parameter: str) -> str:
    """Translate alert messages to Arabic"""
    translations = {
        'temperature': {
            'High temperature detected': 'تم اكتشاف درجة حرارة عالية',
            'Low temperature detected': 'تم اكتشاف درجة حرارة منخفضة'
        },
        'ph': {
            'High pH detected': 'تم اكتشاف رقم هيدروجيني عالي',
            'Low pH detected': 'تم اكتشاف رقم هيدروجيني منخفض'
        },
        'dissolved_oxygen': {
            'Low dissolved oxygen': 'أكسجين منحل منخفض'
        },
        'ammonia': {
            'High ammonia detected': 'تم اكتشاف أمونيا عالية'
        }
    }
    
    # Simple translation lookup
    for key, translation in translations.get(parameter, {}).items():
        if key in message:
            return message.replace(key, translation)
    
    return message  # Return original if no translation found



def get_active_alerts(pond_id: int, db: Session) -> List[Alert]:
    """
    Get active alerts for a pond
    """
    return db.query(Alert).filter(
        and_(
            Alert.pond_id == pond_id,
            Alert.status == AlertStatus.ACTIVE
        )
    ).order_by(desc(Alert.triggered_at)).all()


def acknowledge_alert(alert_id: int, db: Session, user_id: int) -> bool:
    """
    Acknowledge an alert
    """
    try:
        alert = db.query(Alert).filter(Alert.id == alert_id).first()
        if alert:
            alert.status = AlertStatus.ACKNOWLEDGED
            alert.acknowledged_by = user_id
            alert.acknowledged_at = datetime.now(timezone.utc)
            db.commit()
            return True
    except Exception as e:
        print(f"Error acknowledging alert: {e}")
    return False





--- FILE : app/services/notification.py ---


"""
Notification Service
Handles sending notifications via email, SMS, and push notifications
"""

import asyncio
import aiosmtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import List, Optional, Dict, Any
from datetime import datetime
import json

from twilio.rest import Client as TwilioClient
from pyfcm import FCMNotification

from app.config import settings
from app.models.alert import Alert, NotificationLog
from app.models.pond import User
from app.database import SessionLocal


class NotificationService:
    """
    Service for sending notifications through various channels
    """
    
    def __init__(self):
        self.twilio_client = None
        self.fcm_service = None
        
        # Initialize Twilio if configured
        if settings.TWILIO_ACCOUNT_SID and settings.TWILIO_AUTH_TOKEN:
            self.twilio_client = TwilioClient(
                settings.TWILIO_ACCOUNT_SID,
                settings.TWILIO_AUTH_TOKEN
            )
        
        # Initialize Firebase if configured
        if settings.FIREBASE_SERVER_KEY:
            self.fcm_service = FCMNotification(api_key=settings.FIREBASE_SERVER_KEY)
    
    async def send_email_alert_to_observers(self, alert: Alert, observers: List[User], admins: List[User]) -> bool:
        """
        Send email alert notification to a list of observers and CC admins.
        """
        if not settings.SMTP_USERNAME or not settings.SMTP_PASSWORD:
            return False

        observer_emails = [u.email for u in observers if u.email and u.email_notifications]
        if not observer_emails:
            return False

        admin_emails_cc = [a.email for a in admins if a.email]

        # Use the language of the first observer for the message
        message_text = self._get_localized_message(alert, observers[0].language)

        try:
            msg = MIMEMultipart('alternative')
            msg['Subject'] = f"ðŸš¨ Aquaculture Alert - {alert.title}"
            msg['From'] = settings.SMTP_USERNAME
            msg['To'] = ", ".join(observer_emails)
            if admin_emails_cc:
                msg['Cc'] = ", ".join(admin_emails_cc)

            html_content = self._create_email_html(alert, observers[0], message_text)
            html_part = MIMEText(html_content, 'html', 'utf-8')
            msg.attach(html_part)

            async with aiosmtplib.SMTP(
                hostname=settings.SMTP_SERVER, port=settings.SMTP_PORT, start_tls=True
            ) as smtp:
                await smtp.login(settings.SMTP_USERNAME, settings.SMTP_PASSWORD)
                await smtp.send_message(msg)

            # Log notification for each recipient
            all_recipients = observers + admins
            for user in all_recipients:
                if user.email:
                    await self._log_notification(alert.id, user.id, 'email', user.email, message_text, 'sent')
            
            return True
        except Exception as e:
            print(f"Failed to send observer email alert: {e}")
            # Log failure for each recipient
            all_recipients = observers + admins
            for user in all_recipients:
                if user.email:
                    await self._log_notification(alert.id, user.id, 'email', user.email, message_text, 'failed', str(e))
            return False
    
    async def send_sms_alert(self, alert: Alert, user: User) -> bool:
        """
        Send SMS alert notification
        """
        if not self.twilio_client or not user.phone_number:
            return False
        
        try:
            # Get localized message
            message_text = self._get_localized_message(alert, user.language)
            
            # Keep SMS short
            sms_message = f"{alert.title}\n{message_text[:100]}..."
            
            # Send SMS
            message = self.twilio_client.messages.create(
                body=sms_message,
                from_=settings.TWILIO_PHONE_NUMBER,
                to=user.phone_number
            )
            
            # Log notification
            await self._log_notification(
                alert.id, user.id, 'sms', user.phone_number, 
                sms_message, 'sent', provider_response={'sid': message.sid}
            )
            
            return True
            
        except Exception as e:
            await self._log_notification(
                alert.id, user.id, 'sms', user.phone_number, 
                message_text, 'failed', str(e)
            )
            print(f"Failed to send SMS: {e}")
            return False
    
    async def send_push_alert(self, alert: Alert, user: User) -> bool:
        """
        Send push notification alert
        """
        if not self.fcm_service:
            return False
        
        try:
            # Get localized message
            message_text = self._get_localized_message(alert, user.language)
            
            # Get user's device tokens (would be stored in user profile)
            device_tokens = self._get_user_device_tokens(user.id)
            
            if not device_tokens:
                return False
            
            # Create notification data
            notification_data = {
                'title': alert.title,
                'body': message_text[:100],
                'icon': 'alert_icon',
                'click_action': f'/pond/{alert.pond_id}/alerts',
                'sound': 'default' if alert.severity.value == 'critical' else 'notification'
            }
            
            # Additional data payload
            data_payload = {
                'alert_id': str(alert.id),
                'pond_id': str(alert.pond_id),
                'severity': alert.severity.value,
                'parameter': alert.parameter,
                'value': str(alert.current_value)
            }
            
            # Send to all user devices
            results = []
            for token in device_tokens:
                try:
                    result = self.fcm_service.notify_single_device(
                        registration_id=token,
                        message_title=notification_data['title'],
                        message_body=notification_data['body'],
                        data_message=data_payload,
                        sound=notification_data['sound']
                    )
                    results.append(result)
                except Exception as e:
                    print(f"Failed to send to device {token}: {e}")
            
            # Log notification
            await self._log_notification(
                alert.id, user.id, 'push', f"{len(device_tokens)} devices", 
                message_text, 'sent', provider_response={'results': results}
            )
            
            return True
            
        except Exception as e:
            await self._log_notification(
                alert.id, user.id, 'push', 'unknown', 
                message_text, 'failed', str(e)
            )
            print(f"Failed to send push notification: {e}")
            return False
    
    async def send_daily_summary(self, user: User, summary_data: Dict[str, Any]) -> bool:
        """
        Send daily summary email
        """
        try:
            # Create summary email content
            html_content = self._create_daily_summary_html(user, summary_data)
            
            msg = MIMEMultipart('alternative')
            msg['Subject'] = f"ðŸ“Š Daily Aquaculture Summary - {datetime.now().strftime('%Y-%m-%d')}"
            msg['From'] = settings.SMTP_USERNAME
            msg['To'] = user.email
            
            html_part = MIMEText(html_content, 'html', 'utf-8')
            msg.attach(html_part)
            
            # Send email
            async with aiosmtplib.SMTP(
                hostname=settings.SMTP_SERVER,
                port=settings.SMTP_PORT,
                start_tls=True
            ) as smtp:
                await smtp.login(settings.SMTP_USERNAME, settings.SMTP_PASSWORD)
                await smtp.send_message(msg)
            
            return True
            
        except Exception as e:
            print(f"Failed to send daily summary: {e}")
            return False
    
    def _get_localized_message(self, alert: Alert, language: str) -> str:
        """
        Get localized alert message based on user language
        """
        if language == 'fr' and alert.message_fr:
            return alert.message_fr
        elif language == 'ar' and alert.message_ar:
            return alert.message_ar
        else:
            return alert.message
    
    def _create_email_html(self, alert: Alert, user: User, message: str) -> str:
        """
        Create HTML email content for alerts
        """
        severity_colors = {
            'critical': '#dc3545',
            'warning': '#ffc107',
            'info': '#17a2b8'
        }
        
        color = severity_colors.get(alert.severity.value, '#6c757d')
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>Aquaculture Alert</title>
        </head>
        <body style="font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f8f9fa;">
            <div style="max-width: 600px; margin: 0 auto; background-color: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1);">
                
                <!-- Header -->
                <div style="background-color: {color}; color: white; padding: 20px; text-align: center;">
                    <h1 style="margin: 0; font-size: 24px;">ðŸš¨ Alert Aquaculture</h1>
                    <p style="margin: 5px 0 0 0; opacity: 0.9;">SystÃ¨me de surveillance des bassins</p>
                </div>
                
                <!-- Content -->
                <div style="padding: 30px;">
                    <h2 style="color: {color}; margin-top: 0; font-size: 20px;">{alert.title}</h2>
                    
                    <div style="background-color: #f8f9fa; padding: 20px; border-radius: 6px; margin: 20px 0;">
                        <p style="margin: 0; font-size: 16px; line-height: 1.5;">{message}</p>
                    </div>
                    
                    <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6; font-weight: bold; width: 40%;">ParamÃ¨tre:</td>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6;">{alert.parameter}</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6; font-weight: bold;">Valeur actuelle:</td>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6; color: {color}; font-weight: bold;">{alert.current_value}</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6; font-weight: bold;">Seuil:</td>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6;">{alert.threshold_value}</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6; font-weight: bold;">GravitÃ©:</td>
                            <td style="padding: 10px; border-bottom: 1px solid #dee2e6;">{alert.severity.value.title()}</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; font-weight: bold;">Date/Heure:</td>
                            <td style="padding: 10px;">{alert.triggered_at.strftime('%d/%m/%Y %H:%M')}</td>
                        </tr>
                    </table>
                    
                    <div style="text-align: center; margin: 30px 0;">
                        <a href="#" style="background-color: {color}; color: white; padding: 12px 30px; text-decoration: none; border-radius: 6px; display: inline-block; font-weight: bold;">Voir le tableau de bord</a>
                    </div>
                </div>
                
                <!-- Footer -->
                <div style="background-color: #f8f9fa; padding: 20px; text-align: center; border-top: 1px solid #dee2e6;">
                    <p style="margin: 0; color: #6c757d; font-size: 14px;">
                        SystÃ¨me de gestion aquacole - AlgÃ©rie<br>
                        Cette alerte a Ã©tÃ© gÃ©nÃ©rÃ©e automatiquement
                    </p>
                </div>
            </div>
        </body>
        </html>
        """
        
        return html
    
    def _create_daily_summary_html(self, user: User, summary_data: Dict[str, Any]) -> str:
        """
        Create HTML email content for daily summary
        """
        # This would create a comprehensive daily summary email
        # Including pond health scores, recent alerts, trends, etc.
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>Daily Aquaculture Summary</title>
        </head>
        <body style="font-family: Arial, sans-serif;">
            <h1>Daily Summary for {user.first_name or user.username}</h1>
            <!-- Summary content would go here -->
        </body>
        </html>
        """
        return html
    
    def _get_user_device_tokens(self, user_id: int) -> list:
        """
        Get user's device tokens for push notifications
        This would query a user_devices table
        """
        # Placeholder - in real implementation, this would query device tokens
        return []
    
    async def _log_notification(
        self,
        alert_id: Optional[int],
        user_id: int,
        notification_type: str,
        recipient: str,
        message: str,
        status: str,
        error_message: Optional[str] = None,
        provider_response: Optional[Dict[str, Any]] = None
    ):
        """
        Log notification attempt to database
        """
        db = SessionLocal()
        try:
            log_entry = NotificationLog(
                alert_id=alert_id,
                user_id=user_id,
                notification_type=notification_type,
                recipient=recipient,
                message=message,
                status=status,
                error_message=error_message,
                provider_response=provider_response,
                sent_at=datetime.utcnow() if status == 'sent' else None
            )
            
            db.add(log_entry)
            db.commit()
            
        except Exception as e:
            print(f"Failed to log notification: {e}")
            db.rollback()
        finally:
            db.close()


--- FILE : app/services/page_hinkley.py ---


"""
Page-Hinkley Change Point Detection Service
Advanced anomaly detection for aquaculture sensor data
"""

import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timezone
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc

from app.models.sensor import SensorData
from app.models.alert import Alert, AlertType, AlertSeverity, AlertStatus
from app.schemas.sensor import SensorDataCreate


@dataclass
class PageHinkleyState:
    """State for Page-Hinkley algorithm"""
    cumulative_sum: float = 0.0
    min_cumulative_sum: float = 0.0
    max_cumulative_sum: float = 0.0
    mean_estimate: float = 0.0
    sample_count: int = 0
    last_change_point: int = 0
    
    
class PageHinkleyDetector:
    """
    Page-Hinkley Change Point Detection Algorithm
    Detects abrupt changes in sensor data streams
    """
    
    def __init__(self, threshold: float = 5.0, alpha: float = 0.01, min_samples: int = 3):
        """
        Initialize Page-Hinkley detector
        
        Args:
            threshold: Detection threshold (lower = more sensitive)
            alpha: Learning rate for mean estimation
            min_samples: Minimum samples before detection starts
        """
        self.threshold = threshold
        self.alpha = alpha
        self.min_samples = min_samples
        self.state = PageHinkleyState()
    
    def update_and_detect(self, value: float) -> Tuple[bool, float]:
        """
        Update detector state and check for change point
        
        Args:
            value: New sensor value
            
        Returns:
            Tuple of (is_change_point, anomaly_score)
        """
        state = self.state
        
        # Update mean estimate using exponential moving average
        if state.sample_count == 0:
            state.mean_estimate = value
        else:
            state.mean_estimate = (1 - self.alpha) * state.mean_estimate + self.alpha * value
        
        state.sample_count += 1
        
        # Calculate deviation from mean
        deviation = value - state.mean_estimate
        
        # Update cumulative sum
        state.cumulative_sum += deviation
        
        # Update min and max cumulative sums
        state.min_cumulative_sum = min(state.min_cumulative_sum, state.cumulative_sum)
        state.max_cumulative_sum = max(state.max_cumulative_sum, state.cumulative_sum)
        
        # Calculate Page-Hinkley statistics
        ph_up = state.cumulative_sum - state.min_cumulative_sum
        ph_down = state.max_cumulative_sum - state.cumulative_sum
        
        # Calculate anomaly score (0-1, higher means more anomalous)
        anomaly_score = max(ph_up, ph_down) / max(self.threshold, 1.0)
        anomaly_score = min(1.0, anomaly_score)  # Cap at 1.0
        
        # Detect change point
        is_change_point = False
        if state.sample_count >= self.min_samples:
            if ph_up > self.threshold or ph_down > self.threshold:
                is_change_point = True
                print(f"üîç Change detected: ph_up={ph_up:.2f}, ph_down={ph_down:.2f}, threshold={self.threshold}")
        
        return is_change_point, anomaly_score


class AquaculturePageHinkleyService:
    """
    Aquaculture-specific Page-Hinkley anomaly detection service.
    Uses sliding window approach per parameter for consistent detection.
    """
    
    def __init__(self):
        # Parameter-specific detector configurations - made more sensitive
        self.detector_configs = {
            'temperature': {'threshold': 1.5, 'alpha': 0.1, 'min_samples': 3},  # More sensitive
            'ph': {'threshold': 1.0, 'alpha': 0.05, 'min_samples': 3},
            'dissolved_oxygen': {'threshold': 1.2, 'alpha': 0.06, 'min_samples': 3},
            'ammonia': {'threshold': 0.8, 'alpha': 0.04, 'min_samples': 3},
            'nitrate': {'threshold': 1.0, 'alpha': 0.04, 'min_samples': 3},
            'nitrite': {'threshold': 0.9, 'alpha': 0.04, 'min_samples': 3},
            'turbidity': {'threshold': 1.5, 'alpha': 0.08, 'min_samples': 3},
            'salinity': {'threshold': 1.2, 'alpha': 0.05, 'min_samples': 3},
            'fish_count': {'threshold': 2.0, 'alpha': 0.1, 'min_samples': 4},
            'fish_length': {'threshold': 1.8, 'alpha': 0.08, 'min_samples': 4},
            'fish_weight': {'threshold': 1.8, 'alpha': 0.08, 'min_samples': 4},
            'water_level': {'threshold': 1.4, 'alpha': 0.06, 'min_samples': 3},
            'flow_rate': {'threshold': 1.6, 'alpha': 0.08, 'min_samples': 3}
        }

    def _get_historical_data_for_parameter(self, pond_id: int, parameter: str, db: Session, limit: int = 10) -> List[float]:
        """
        Get historical data for a specific parameter from the database.
        """
        try:
            historical_records = db.query(SensorData).filter(
                SensorData.pond_id == pond_id,
                # SensorData.is_anomaly == False,
                getattr(SensorData, parameter).isnot(None)
            ).order_by(desc(SensorData.timestamp)).limit(limit).all()
            
            # Reverse to get chronological order (oldest to newest)
            historical_records.reverse()
            
            # Extract parameter values
            values = [getattr(record, parameter) for record in historical_records]
            # print(f"üìä Historical {parameter} data: {values}")
            return values
        except Exception as e:
            print(f"Error fetching historical data for {parameter}: {e}")
            return []

    def _run_detection_on_parameter_window(self, parameter: str, window: List[float]) -> Tuple[bool, float, Dict]:
        """
        Runs Page-Hinkley detector on a window of data for a specific parameter.
        """
        if not window or len(window) < 2:
            return False, 0.0, {'error': 'insufficient_data', 'window_size': len(window)}
            
        config = self.detector_configs.get(parameter, 
                                         {'threshold': 1.5, 'alpha': 0.05, 'min_samples': 3})
        
        print(f"üîç Running detection for {parameter} with config: {config}")
        print(f"   Window data: {window}")
        
        detector = PageHinkleyDetector(**config)
        
        final_is_change_point = False
        final_anomaly_score = 0.0
        detection_details = {
            'window_size': len(window),
            'parameter': parameter,
            'config': config,
            'final_mean': 0.0,
            'final_cumsum': 0.0,
            'step_by_step': []
        }

        # Process each value in the window
        for i, value in enumerate(window):
            is_change_point, anomaly_score = detector.update_and_detect(value)
            
            step_info = {
                'step': i,
                'value': value,
                'mean': detector.state.mean_estimate,
                'cumsum': detector.state.cumulative_sum,
                'is_change': is_change_point,
                'score': anomaly_score
            }
            detection_details['step_by_step'].append(step_info)
            
            print(f"   Step {i}: value={value:.2f}, mean={detector.state.mean_estimate:.2f}, "
                  f"cumsum={detector.state.cumulative_sum:.2f}, change={is_change_point}, score={anomaly_score:.3f}")
            
            # Store final results (last point)
            if i == len(window) - 1:
                final_is_change_point = is_change_point
                final_anomaly_score = anomaly_score
                detection_details['final_mean'] = detector.state.mean_estimate
                detection_details['final_cumsum'] = detector.state.cumulative_sum
                detection_details['is_last_point_anomaly'] = is_change_point
                detection_details['anomaly_score'] = anomaly_score
                detection_details['sample_count'] = detector.state.sample_count
        
        print(f"   üéØ Final result for {parameter}: anomaly={final_is_change_point}, score={final_anomaly_score:.3f}")
        return final_is_change_point, final_anomaly_score, detection_details

    async def detect_anomaly_with_alerts(self, pond_id: int, sensor_data: SensorDataCreate, db: Session) -> Dict[str, any]:
        """
        Detects anomalies by analyzing the new data point against historical data per parameter.
        Creates an alert if anomalies are found.
        """
        print(f"üîç Starting anomaly detection for pond {pond_id}")
        
        results = {
            'is_anomaly': False,
            'anomaly_score': 0.0,
            'parameter_results': {},
            'change_points_detected': [],
            'alert_id': None
        }

        # All possible parameters to check
        parameters_to_check = [
            'temperature', 'ph', 'dissolved_oxygen', 'ammonia', 'nitrate', 'nitrite',
            'turbidity', 'salinity', 'fish_count', 'fish_length', 'fish_weight',
            'water_level', 'flow_rate'
        ]

        max_anomaly_score = 0.0
        total_anomalies = 0

        for param in parameters_to_check:
            new_value = getattr(sensor_data, param)
            
            # Skip if new value is None
            if new_value is None:
                print(f"   ‚è≠Ô∏è  Skipping {param}: value is None")
                continue

            # print(f"üìä Processing {param}: new_value={new_value}")

            # Get historical data for this parameter
            historical_values = self._get_historical_data_for_parameter(pond_id, param, db, limit=10)
            
            # Create window: historical + new value
            window = historical_values + [new_value]
            
            # print(f"   Window size: {len(window)} (historical: {len(historical_values)} + new: 1)")
            
            # Run detection on this parameter's window
            is_anomaly, anomaly_score, detection_details = self._run_detection_on_parameter_window(param, window)
            
            # Store results for this parameter
            results['parameter_results'][param] = {
                'value': new_value,
                'is_anomaly': is_anomaly,
                'anomaly_score': anomaly_score,
                'historical_count': len(historical_values),
                'window_size': len(window),
                'detection_details': detection_details
            }
            
            # Track overall anomaly status
            if is_anomaly:
                print(f"üö® ANOMALY DETECTED in {param}!")
                results['change_points_detected'].append(param)
                total_anomalies += 1
                max_anomaly_score = max(max_anomaly_score, anomaly_score)

        # Determine overall anomaly status
        results['is_anomaly'] = total_anomalies > 0
        results['anomaly_score'] = max_anomaly_score
        results['total_anomalous_parameters'] = total_anomalies

        print(f"üéØ Final detection results:")
        print(f"   Total anomalies: {total_anomalies}")
        print(f"   Max anomaly score: {max_anomaly_score:.3f}")
        print(f"   Anomalous parameters: {results['change_points_detected']}")

        # Create alert if anomaly detected
        if results['is_anomaly']:
            alert = await self.create_anomaly_alert(pond_id, sensor_data, results, db)
            results['alert_id'] = alert.id if alert else None
        
        return results

    async def create_anomaly_alert(self, pond_id: int, sensor_data: SensorDataCreate, 
                                detection_results: Dict, db: Session) -> Optional[Alert]:
        """Create an alert when anomaly is detected"""
        try:
            anomaly_score = detection_results['anomaly_score']
            change_points = detection_results['change_points_detected']
            total_anomalies = detection_results['total_anomalous_parameters']
            
            # Determine severity based on number of anomalous parameters and score
            if total_anomalies >= 3 or anomaly_score >= 0.8:
                severity = AlertSeverity.CRITICAL
            elif total_anomalies >= 2 or anomaly_score >= 0.6:
                severity = AlertSeverity.WARNING
            else:
                severity = AlertSeverity.INFO
            
            affected_params = ', '.join(change_points[:5])  # Limit to first 5 for readability
            if len(change_points) > 5:
                affected_params += f" and {len(change_points) - 5} more"
            
            message = f"Anomaly detected in {affected_params}. Score: {anomaly_score:.2f}"
            
            # Serialize sensor_data to ensure JSON compatibility
            def make_json_serializable(obj):
                """Convert datetime objects to ISO format strings for JSON serialization"""
                if isinstance(obj, datetime):
                    return obj.isoformat()
                elif isinstance(obj, dict):
                    return {key: make_json_serializable(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [make_json_serializable(item) for item in obj]
                else:
                    return obj
            
            # Create JSON-safe context data
            alert_context = {
                'detection_method': 'page_hinkley_windowed_per_parameter',
                'anomaly_score': anomaly_score,
                'total_anomalous_parameters': total_anomalies,
                'change_points_detected': change_points,
                'parameter_results': make_json_serializable(detection_results['parameter_results']),
                'sensor_values': make_json_serializable(sensor_data.dict())
            }
            
            alert = Alert(
                pond_id=pond_id,
                alert_type=AlertType.ANOMALY_DETECTED,
                severity=severity,
                status=AlertStatus.ACTIVE,
                parameter=change_points[0] if change_points else 'multiple',
                current_value=anomaly_score,
                threshold_value=0.5,
                title="Parameter Anomaly Detected",
                message=message,
                message_fr=f"Anomalie d√©tect√©e - Param√®tres: {affected_params}. Score: {anomaly_score:.2f}",
                message_ar=f"ÿ™ŸÖ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿ¥ÿ∞Ÿàÿ∞ - ÿßŸÑŸÖÿπÿßŸäŸäÿ±: {affected_params}. ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©: {anomaly_score:.2f}",
                triggered_at=datetime.now(timezone.utc),
                context_data=alert_context,
                notifications_sent={}
            )
            
            db.add(alert)
            db.commit()
            db.refresh(alert)
            
            print(f"‚úÖ Parameter anomaly alert created successfully: ID {alert.id}")
            print(f"   Affected parameters: {affected_params}")
            print(f"   Total anomalous parameters: {total_anomalies}")
            return alert
            
        except Exception as e:
            print(f"‚ùå Error creating parameter anomaly alert: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
            return None

    def get_pond_detector_status(self, pond_id: int) -> Dict[str, any]:
        """Get status of windowed detection for a pond"""
        return {
            'status': 'windowed_per_parameter_detection',
            'description': 'Uses sliding window of 10 previous points per parameter plus new point',
            'window_size': 10,
            'parameters_monitored': list(self.detector_configs.keys()),
            'detection_method': 'page_hinkley_change_point_per_parameter',
            'parameter_configs': self.detector_configs
        }


# Global service instance
page_hinkley_service = AquaculturePageHinkleyService()


async def detect_anomalies_page_hinkley(sensor_data: SensorDataCreate, db: Session) -> bool:
    """Main anomaly detection function using Page-Hinkley method"""
    try:
        pond_id = sensor_data.pond_id
        results = await page_hinkley_service.detect_anomaly_with_alerts(pond_id, sensor_data, db)
        return results['is_anomaly']
    except Exception as e:
        print(f"Error in Page-Hinkley anomaly detection: {e}")
        return False


def get_page_hinkley_diagnostics(pond_id: int) -> Dict[str, any]:
    """Get diagnostic information about Page-Hinkley detectors"""
    return page_hinkley_service.get_pond_detector_status(pond_id)


--- FILE : app/services/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/tasks/data_aggregation.py ---


"""
Data Aggregation Tasks
Scheduled tasks for aggregating sensor data and maintaining system health
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_, func, desc
import pandas as pd

from app.database import SessionLocal
from app.models.sensor import SensorData, SensorDataAggregated
from app.models.pond import Pond, User
from app.core.alert_engine import check_for_stale_data
from app.services.notification import NotificationService


async def aggregate_hourly_data():
    """
    Aggregate sensor data into hourly summaries
    Should be run every hour
    """
    db = SessionLocal()
    
    try:
        # Get the last hour's data
        end_time = datetime.utcnow().replace(minute=0, second=0, microsecond=0)
        start_time = end_time - timedelta(hours=1)
        
        # Get all ponds with data in the last hour
        ponds_with_data = db.query(SensorData.pond_id).filter(
            and_(
                SensorData.timestamp >= start_time,
                SensorData.timestamp < end_time
            )
        ).distinct().all()
        
        for (pond_id,) in ponds_with_data:
            await _create_hourly_aggregation(db, pond_id, start_time, end_time)
        
        db.commit()
        print(f"Completed hourly aggregation for {len(ponds_with_data)} ponds")
        
    except Exception as e:
        print(f"Error in hourly aggregation: {e}")
        db.rollback()
    finally:
        db.close()


async def aggregate_daily_data():
    """
    Aggregate sensor data into daily summaries
    Should be run daily at midnight
    """
    db = SessionLocal()
    
    try:
        # Get yesterday's data
        end_time = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
        start_time = end_time - timedelta(days=1)
        
        # Get all ponds with data yesterday
        ponds_with_data = db.query(SensorData.pond_id).filter(
            and_(
                SensorData.timestamp >= start_time,
                SensorData.timestamp < end_time
            )
        ).distinct().all()
        
        for (pond_id,) in ponds_with_data:
            await _create_daily_aggregation(db, pond_id, start_time, end_time)
        
        db.commit()
        print(f"Completed daily aggregation for {len(ponds_with_data)} ponds")
        
        # Send daily summaries to users
        await _send_daily_summaries(db, start_time, end_time)
        
    except Exception as e:
        print(f"Error in daily aggregation: {e}")
        db.rollback()
    finally:
        db.close()


async def _create_hourly_aggregation(
    db: Session, 
    pond_id: int, 
    start_time: datetime, 
    end_time: datetime
):
    """
    Create hourly aggregation for a specific pond
    """
    # Check if aggregation already exists
    existing = db.query(SensorDataAggregated).filter(
        and_(
            SensorDataAggregated.pond_id == pond_id,
            SensorDataAggregated.aggregation_type == 'hour',
            SensorDataAggregated.period_start == start_time
        )
    ).first()
    
    if existing:
        return  # Already aggregated
    
    # Get raw data for the hour
    raw_data = db.query(SensorData).filter(
        and_(
            SensorData.pond_id == pond_id,
            SensorData.timestamp >= start_time,
            SensorData.timestamp < end_time
        )
    ).all()
    
    if not raw_data:
        return
    
    # Calculate aggregations
    aggregation = _calculate_aggregations(raw_data)
    
    # Create aggregated record
    agg_record = SensorDataAggregated(
        pond_id=pond_id,
        period_start=start_time,
        period_end=end_time,
        aggregation_type='hour',
        data_points_count=len(raw_data),
        **aggregation
    )
    
    db.add(agg_record)


async def _create_daily_aggregation(
    db: Session, 
    pond_id: int, 
    start_time: datetime, 
    end_time: datetime
):
    """
    Create daily aggregation for a specific pond
    """
    # Check if aggregation already exists
    existing = db.query(SensorDataAggregated).filter(
        and_(
            SensorDataAggregated.pond_id == pond_id,
            SensorDataAggregated.aggregation_type == 'day',
            SensorDataAggregated.period_start == start_time
        )
    ).first()
    
    if existing:
        return
    
    # Get raw data for the day
    raw_data = db.query(SensorData).filter(
        and_(
            SensorData.pond_id == pond_id,
            SensorData.timestamp >= start_time,
            SensorData.timestamp < end_time
        )
    ).all()
    
    if not raw_data:
        return
    
    # Calculate aggregations
    aggregation = _calculate_aggregations(raw_data)
    
    # Create aggregated record
    agg_record = SensorDataAggregated(
        pond_id=pond_id,
        period_start=start_time,
        period_end=end_time,
        aggregation_type='day',
        data_points_count=len(raw_data),
        **aggregation
    )
    
    db.add(agg_record)


def _calculate_aggregations(raw_data) -> Dict[str, Any]:
    """
    Calculate statistical aggregations from raw sensor data
    """
    # Convert to DataFrame for easier calculations
    df_data = []
    for record in raw_data:
        df_data.append({
            'temperature': record.temperature,
            'ph': record.ph,
            'dissolved_oxygen': record.dissolved_oxygen,
            'turbidity': record.turbidity,
            'ammonia': record.ammonia,
            'nitrate': record.nitrate,
            'quality_score': record.quality_score,
            'is_anomaly': record.is_anomaly
        })
    
    df = pd.DataFrame(df_data)
    
    aggregation = {}
    
    # Temperature aggregations
    if 'temperature' in df.columns and df['temperature'].notna().any():
        temp_data = df['temperature'].dropna()
        aggregation.update({
            'temp_avg': float(temp_data.mean()),
            'temp_min': float(temp_data.min()),
            'temp_max': float(temp_data.max()),
            'temp_std': float(temp_data.std()) if len(temp_data) > 1 else 0
        })
    
    # pH aggregations
    if 'ph' in df.columns and df['ph'].notna().any():
        ph_data = df['ph'].dropna()
        aggregation.update({
            'ph_avg': float(ph_data.mean()),
            'ph_min': float(ph_data.min()),
            'ph_max': float(ph_data.max()),
            'ph_std': float(ph_data.std()) if len(ph_data) > 1 else 0
        })
    
    # Dissolved oxygen aggregations
    if 'dissolved_oxygen' in df.columns and df['dissolved_oxygen'].notna().any():
        do_data = df['dissolved_oxygen'].dropna()
        aggregation.update({
            'do_avg': float(do_data.mean()),
            'do_min': float(do_data.min()),
            'do_max': float(do_data.max()),
            'do_std': float(do_data.std()) if len(do_data) > 1 else 0
        })
    
    # Other parameters
    if 'turbidity' in df.columns and df['turbidity'].notna().any():
        aggregation['turbidity_avg'] = float(df['turbidity'].dropna().mean())
    
    if 'ammonia' in df.columns and df['ammonia'].notna().any():
        aggregation['ammonia_avg'] = float(df['ammonia'].dropna().mean())
    
    if 'nitrate' in df.columns and df['nitrate'].notna().any():
        aggregation['nitrate_avg'] = float(df['nitrate'].dropna().mean())
    
    # Quality and anomaly metrics
    if 'quality_score' in df.columns and df['quality_score'].notna().any():
        aggregation['quality_score_avg'] = float(df['quality_score'].dropna().mean())
    
    if 'is_anomaly' in df.columns:
        aggregation['anomaly_count'] = int(df['is_anomaly'].sum())
    
    return aggregation


async def _send_daily_summaries(db: Session, start_time: datetime, end_time: datetime):
    """
    Send daily summary emails to users who have opted in
    """
    notification_service = NotificationService()
    
    # Get users who want daily summaries
    users_with_summaries = db.query(User).filter(
        User.daily_summary_enabled == True
    ).all()
    if not users_with_summaries:
        print("No users with daily summaries enabled")
        return
    
    for (user_id,) in users_with_summaries:
        try:
            user = db.query(User).filter(User.id == user_id).first()
            if not user or not user.email:
                continue
            
            # Get user's pond data for yesterday
            user_ponds = db.query(Pond).filter(Pond.owner_id == user_id).all()
            
            summary_data = {
                'user': user,
                'date': start_time.strftime('%Y-%m-%d'),
                'ponds': []
            }
            
            for pond in user_ponds:
                # Get daily aggregation for this pond
                daily_agg = db.query(SensorDataAggregated).filter(
                    and_(
                        SensorDataAggregated.pond_id == pond.id,
                        SensorDataAggregated.aggregation_type == 'day',
                        SensorDataAggregated.period_start == start_time
                    )
                ).first()
                
                if daily_agg:
                    pond_summary = {
                        'name': pond.name,
                        'data_points': daily_agg.data_points_count,
                        'avg_temperature': daily_agg.temp_avg,
                        'avg_ph': daily_agg.ph_avg,
                        'avg_do': daily_agg.do_avg,
                        'quality_score': daily_agg.quality_score_avg,
                        'anomalies': daily_agg.anomaly_count
                    }
                    summary_data['ponds'].append(pond_summary)
            
            # Send summary if there's data
            if summary_data['ponds']:
                await notification_service.send_daily_summary(user, summary_data)
                
        except Exception as e:
            print(f"Error sending daily summary to user {user_id}: {e}")


async def cleanup_old_data():
    """
    Clean up old raw sensor data (keep aggregated data)
    Should be run weekly
    """
    db = SessionLocal()
    
    try:
        # Keep raw data for 90 days, remove older
        cutoff_date = datetime.utcnow() - timedelta(days=90)
        
        # Delete old raw sensor data
        deleted_count = db.query(SensorData).filter(
            SensorData.timestamp < cutoff_date
        ).delete()
        
        # Keep aggregated data for 2 years
        agg_cutoff_date = datetime.utcnow() - timedelta(days=730)
        
        deleted_agg_count = db.query(SensorDataAggregated).filter(
            SensorDataAggregated.period_start < agg_cutoff_date
        ).delete()
        
        db.commit()
        
        print(f"Cleaned up {deleted_count} old sensor records and {deleted_agg_count} old aggregated records")
        
    except Exception as e:
        print(f"Error in data cleanup: {e}")
        db.rollback()
    finally:
        db.close()


async def system_health_check():
    """
    Perform system health checks
    Should be run every 15 minutes
    """
    # Check for stale data
    check_for_stale_data()
    
    # Add other health checks here
    # - Database connectivity
    # - Disk space
    # - Memory usage
    # - API response times
    
    print("System health check completed")


--- FILE : app/tasks/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/utils/helpers.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : app/utils/__init__.py ---


--- EMPTY / NON READABLE FILE ---


--- FILE : nginx/nginx.conf ---


events {
    worker_connections 1024;
}

http {
    upstream api {
        server aquaculture_api:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /health {
            proxy_pass http://api/health;
        }
    }
}


